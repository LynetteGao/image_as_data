{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set-up of the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/lynette/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Basic packages\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import re\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "# Packages for data preparation\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Packages for modeling\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import regularizers\n",
    "\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import recall_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set some parameters that will be used throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_WORDS = 10000  # Parameter indicating the number of words we'll put in the dictionary\n",
    "NB_START_EPOCHS = 20  # Number of epochs we usually start to train with\n",
    "BATCH_SIZE = 512  # Size of the batches used in the mini-batch gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read in the csv with the tweets data and perform a random shuffle. It's a good practice to shuffle the data before splitting between a train and test set. We'll only keep the video decription column as input and the Relvancy column as the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'video_id', 'channel_title', 'channel_id',\n",
       "       'video_publish_date', 'video_title', 'video_description',\n",
       "       'video_category', 'video_view_count', 'video_comment_count',\n",
       "       'video_like_count', 'video_dislike_count', 'video_thumbnail',\n",
       "       'video_tags', 'collection_date', 'science.topic', 'Relevancy',\n",
       "       'attitude', 'Text/video', 'search.term', 'cld2', 'transcript',\n",
       "       'transcript_nchar', 'videoid', 'conspiracy', 'var_r', 'var_g', 'var_b',\n",
       "       'var_h', 'var_s', 'var_v', 'var_bright', 'var_bright_sd',\n",
       "       'var_contrast', 'var_colorful', 'median_r', 'median_g', 'median_b',\n",
       "       'median_h', 'median_s', 'median_v', 'median_bright', 'median_bright_sd',\n",
       "       'median_contrast', 'median_colorful', 'r_mean', 'g_mean', 'b_mean',\n",
       "       'h_mean', 's_mean', 'v_mean', 'bright_mean', 'lightning_mean',\n",
       "       'contrast_mean', 'colorful_mean', 'color_lag'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('handlabel_feature.csv')\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['var_r', 'var_g', 'var_b',\n",
    "       'var_h', 'var_s', 'var_v', 'var_bright', 'var_bright_sd',\n",
    "       'var_contrast', 'var_colorful', 'median_r', 'median_g', 'median_b',\n",
    "       'median_h', 'median_s', 'median_v', 'median_bright', 'median_bright_sd',\n",
    "       'median_contrast', 'median_colorful']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var_r</th>\n",
       "      <th>var_g</th>\n",
       "      <th>var_b</th>\n",
       "      <th>var_h</th>\n",
       "      <th>var_s</th>\n",
       "      <th>var_v</th>\n",
       "      <th>var_bright</th>\n",
       "      <th>var_bright_sd</th>\n",
       "      <th>var_contrast</th>\n",
       "      <th>var_colorful</th>\n",
       "      <th>median_r</th>\n",
       "      <th>median_g</th>\n",
       "      <th>median_b</th>\n",
       "      <th>median_h</th>\n",
       "      <th>median_s</th>\n",
       "      <th>median_v</th>\n",
       "      <th>median_bright</th>\n",
       "      <th>median_bright_sd</th>\n",
       "      <th>median_contrast</th>\n",
       "      <th>median_colorful</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>298.035544</td>\n",
       "      <td>256.526430</td>\n",
       "      <td>317.666638</td>\n",
       "      <td>176.226486</td>\n",
       "      <td>291.471955</td>\n",
       "      <td>318.839186</td>\n",
       "      <td>249.684164</td>\n",
       "      <td>58.312728</td>\n",
       "      <td>273.094250</td>\n",
       "      <td>131.323126</td>\n",
       "      <td>99.315580</td>\n",
       "      <td>85.044938</td>\n",
       "      <td>112.727762</td>\n",
       "      <td>86.092826</td>\n",
       "      <td>134.038498</td>\n",
       "      <td>134.587656</td>\n",
       "      <td>92.603380</td>\n",
       "      <td>66.266163</td>\n",
       "      <td>220.0</td>\n",
       "      <td>74.248203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14.562086</td>\n",
       "      <td>13.206167</td>\n",
       "      <td>14.473241</td>\n",
       "      <td>22.249816</td>\n",
       "      <td>3.087459</td>\n",
       "      <td>14.468497</td>\n",
       "      <td>13.461712</td>\n",
       "      <td>2.128646</td>\n",
       "      <td>6.360655</td>\n",
       "      <td>0.903765</td>\n",
       "      <td>126.550148</td>\n",
       "      <td>128.666820</td>\n",
       "      <td>130.396076</td>\n",
       "      <td>83.762660</td>\n",
       "      <td>39.437676</td>\n",
       "      <td>134.533176</td>\n",
       "      <td>128.116216</td>\n",
       "      <td>75.928568</td>\n",
       "      <td>241.0</td>\n",
       "      <td>19.761427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1397.200532</td>\n",
       "      <td>1812.288126</td>\n",
       "      <td>1125.290200</td>\n",
       "      <td>952.795220</td>\n",
       "      <td>3386.595470</td>\n",
       "      <td>942.447595</td>\n",
       "      <td>1478.249667</td>\n",
       "      <td>160.366149</td>\n",
       "      <td>1102.429909</td>\n",
       "      <td>528.665700</td>\n",
       "      <td>82.593592</td>\n",
       "      <td>44.906568</td>\n",
       "      <td>92.771928</td>\n",
       "      <td>116.763216</td>\n",
       "      <td>175.650236</td>\n",
       "      <td>106.260372</td>\n",
       "      <td>61.527896</td>\n",
       "      <td>54.770904</td>\n",
       "      <td>190.0</td>\n",
       "      <td>71.947426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3404.870303</td>\n",
       "      <td>4109.476780</td>\n",
       "      <td>3996.139113</td>\n",
       "      <td>1322.958651</td>\n",
       "      <td>1390.733279</td>\n",
       "      <td>3484.537251</td>\n",
       "      <td>3852.697268</td>\n",
       "      <td>172.840076</td>\n",
       "      <td>1404.656302</td>\n",
       "      <td>165.149776</td>\n",
       "      <td>73.374364</td>\n",
       "      <td>52.725700</td>\n",
       "      <td>56.267280</td>\n",
       "      <td>59.196960</td>\n",
       "      <td>98.607484</td>\n",
       "      <td>76.760644</td>\n",
       "      <td>60.457804</td>\n",
       "      <td>51.433120</td>\n",
       "      <td>185.0</td>\n",
       "      <td>35.788363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>800.175857</td>\n",
       "      <td>979.432379</td>\n",
       "      <td>765.202229</td>\n",
       "      <td>533.030459</td>\n",
       "      <td>899.264225</td>\n",
       "      <td>665.884765</td>\n",
       "      <td>850.382767</td>\n",
       "      <td>385.940453</td>\n",
       "      <td>2316.228370</td>\n",
       "      <td>115.950038</td>\n",
       "      <td>79.772896</td>\n",
       "      <td>73.095122</td>\n",
       "      <td>103.627240</td>\n",
       "      <td>107.586062</td>\n",
       "      <td>112.794364</td>\n",
       "      <td>110.728122</td>\n",
       "      <td>77.605316</td>\n",
       "      <td>50.424047</td>\n",
       "      <td>182.0</td>\n",
       "      <td>56.541083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>147.788425</td>\n",
       "      <td>83.104502</td>\n",
       "      <td>343.138525</td>\n",
       "      <td>287.133037</td>\n",
       "      <td>738.014925</td>\n",
       "      <td>211.022546</td>\n",
       "      <td>73.414553</td>\n",
       "      <td>233.283992</td>\n",
       "      <td>762.744039</td>\n",
       "      <td>122.025988</td>\n",
       "      <td>111.254992</td>\n",
       "      <td>113.640668</td>\n",
       "      <td>101.466524</td>\n",
       "      <td>65.478744</td>\n",
       "      <td>56.372900</td>\n",
       "      <td>122.471632</td>\n",
       "      <td>112.078180</td>\n",
       "      <td>69.131249</td>\n",
       "      <td>237.0</td>\n",
       "      <td>30.316916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>499.048934</td>\n",
       "      <td>671.771055</td>\n",
       "      <td>464.885271</td>\n",
       "      <td>768.802809</td>\n",
       "      <td>362.948260</td>\n",
       "      <td>492.587090</td>\n",
       "      <td>554.760968</td>\n",
       "      <td>93.525503</td>\n",
       "      <td>564.382108</td>\n",
       "      <td>60.941841</td>\n",
       "      <td>123.212802</td>\n",
       "      <td>119.276686</td>\n",
       "      <td>102.296032</td>\n",
       "      <td>79.687310</td>\n",
       "      <td>70.830036</td>\n",
       "      <td>129.006988</td>\n",
       "      <td>118.347988</td>\n",
       "      <td>63.955769</td>\n",
       "      <td>210.0</td>\n",
       "      <td>34.442498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>12.184924</td>\n",
       "      <td>9.988667</td>\n",
       "      <td>8.442325</td>\n",
       "      <td>7.628704</td>\n",
       "      <td>8.160141</td>\n",
       "      <td>12.292979</td>\n",
       "      <td>9.858782</td>\n",
       "      <td>1.924338</td>\n",
       "      <td>13.182123</td>\n",
       "      <td>6.304640</td>\n",
       "      <td>126.875756</td>\n",
       "      <td>101.686932</td>\n",
       "      <td>90.995444</td>\n",
       "      <td>37.053376</td>\n",
       "      <td>85.170332</td>\n",
       "      <td>128.535556</td>\n",
       "      <td>108.091288</td>\n",
       "      <td>70.702156</td>\n",
       "      <td>249.0</td>\n",
       "      <td>54.941191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>3.100336</td>\n",
       "      <td>12.856796</td>\n",
       "      <td>13.246367</td>\n",
       "      <td>0.652040</td>\n",
       "      <td>23.776415</td>\n",
       "      <td>2.342890</td>\n",
       "      <td>9.398054</td>\n",
       "      <td>2.273838</td>\n",
       "      <td>0.001471</td>\n",
       "      <td>6.499906</td>\n",
       "      <td>128.762536</td>\n",
       "      <td>80.070008</td>\n",
       "      <td>88.208350</td>\n",
       "      <td>124.785384</td>\n",
       "      <td>164.544432</td>\n",
       "      <td>141.974716</td>\n",
       "      <td>95.623774</td>\n",
       "      <td>77.683406</td>\n",
       "      <td>241.0</td>\n",
       "      <td>97.623431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>26.059390</td>\n",
       "      <td>20.160538</td>\n",
       "      <td>26.205856</td>\n",
       "      <td>37.262637</td>\n",
       "      <td>9.450165</td>\n",
       "      <td>20.737421</td>\n",
       "      <td>21.035988</td>\n",
       "      <td>16.549036</td>\n",
       "      <td>6.226976</td>\n",
       "      <td>4.321690</td>\n",
       "      <td>185.561740</td>\n",
       "      <td>178.857568</td>\n",
       "      <td>174.949488</td>\n",
       "      <td>29.728208</td>\n",
       "      <td>31.127512</td>\n",
       "      <td>188.196768</td>\n",
       "      <td>180.733376</td>\n",
       "      <td>75.648709</td>\n",
       "      <td>231.0</td>\n",
       "      <td>20.090563</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>407 rows Ã— 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           var_r        var_g        var_b        var_h        var_s  \\\n",
       "0     298.035544   256.526430   317.666638   176.226486   291.471955   \n",
       "1      14.562086    13.206167    14.473241    22.249816     3.087459   \n",
       "2    1397.200532  1812.288126  1125.290200   952.795220  3386.595470   \n",
       "3    3404.870303  4109.476780  3996.139113  1322.958651  1390.733279   \n",
       "4     800.175857   979.432379   765.202229   533.030459   899.264225   \n",
       "..           ...          ...          ...          ...          ...   \n",
       "402   147.788425    83.104502   343.138525   287.133037   738.014925   \n",
       "403   499.048934   671.771055   464.885271   768.802809   362.948260   \n",
       "404    12.184924     9.988667     8.442325     7.628704     8.160141   \n",
       "405     3.100336    12.856796    13.246367     0.652040    23.776415   \n",
       "406    26.059390    20.160538    26.205856    37.262637     9.450165   \n",
       "\n",
       "           var_v   var_bright  var_bright_sd  var_contrast  var_colorful  \\\n",
       "0     318.839186   249.684164      58.312728    273.094250    131.323126   \n",
       "1      14.468497    13.461712       2.128646      6.360655      0.903765   \n",
       "2     942.447595  1478.249667     160.366149   1102.429909    528.665700   \n",
       "3    3484.537251  3852.697268     172.840076   1404.656302    165.149776   \n",
       "4     665.884765   850.382767     385.940453   2316.228370    115.950038   \n",
       "..           ...          ...            ...           ...           ...   \n",
       "402   211.022546    73.414553     233.283992    762.744039    122.025988   \n",
       "403   492.587090   554.760968      93.525503    564.382108     60.941841   \n",
       "404    12.292979     9.858782       1.924338     13.182123      6.304640   \n",
       "405     2.342890     9.398054       2.273838      0.001471      6.499906   \n",
       "406    20.737421    21.035988      16.549036      6.226976      4.321690   \n",
       "\n",
       "       median_r    median_g    median_b    median_h    median_s    median_v  \\\n",
       "0     99.315580   85.044938  112.727762   86.092826  134.038498  134.587656   \n",
       "1    126.550148  128.666820  130.396076   83.762660   39.437676  134.533176   \n",
       "2     82.593592   44.906568   92.771928  116.763216  175.650236  106.260372   \n",
       "3     73.374364   52.725700   56.267280   59.196960   98.607484   76.760644   \n",
       "4     79.772896   73.095122  103.627240  107.586062  112.794364  110.728122   \n",
       "..          ...         ...         ...         ...         ...         ...   \n",
       "402  111.254992  113.640668  101.466524   65.478744   56.372900  122.471632   \n",
       "403  123.212802  119.276686  102.296032   79.687310   70.830036  129.006988   \n",
       "404  126.875756  101.686932   90.995444   37.053376   85.170332  128.535556   \n",
       "405  128.762536   80.070008   88.208350  124.785384  164.544432  141.974716   \n",
       "406  185.561740  178.857568  174.949488   29.728208   31.127512  188.196768   \n",
       "\n",
       "     median_bright  median_bright_sd  median_contrast  median_colorful  \n",
       "0        92.603380         66.266163            220.0        74.248203  \n",
       "1       128.116216         75.928568            241.0        19.761427  \n",
       "2        61.527896         54.770904            190.0        71.947426  \n",
       "3        60.457804         51.433120            185.0        35.788363  \n",
       "4        77.605316         50.424047            182.0        56.541083  \n",
       "..             ...               ...              ...              ...  \n",
       "402     112.078180         69.131249            237.0        30.316916  \n",
       "403     118.347988         63.955769            210.0        34.442498  \n",
       "404     108.091288         70.702156            249.0        54.941191  \n",
       "405      95.623774         77.683406            241.0        97.623431  \n",
       "406     180.733376         75.648709            231.0        20.090563  \n",
       "\n",
       "[407 rows x 20 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "df['attitude'] = le.fit_transform(df['attitude'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(407,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = df['attitude']\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we'll do is removing stopwords. These words do not have any value for predicting the sentiment.Also, we remove the http link in the texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate K-fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# inputs data samples: 407\n",
      "# targets data samples: 407\n",
      "Shape of train set: (366, 20)\n",
      "Shape of y: (366, 2)\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Shape of validation set: (41, 20)\n",
      "{'loss': [106.25879669189453, 55.21419906616211, 75.1198959350586, 32.660064697265625, 22.940919876098633, 18.97317123413086, 18.643726348876953, 90.81719970703125, 65.8553695678711, 33.33571243286133, 17.25141716003418, 24.746482849121094, 25.20262908935547, 43.89271545410156, 15.709957122802734, 25.481351852416992, 44.15961837768555, 16.661338806152344, 25.02887725830078, 41.80131530761719, 15.664270401000977, 22.582473754882812, 39.13252639770508, 13.865652084350586, 22.7144775390625, 39.5237922668457, 14.496668815612793, 22.116849899291992, 38.19081497192383, 13.874444961547852, 19.97676658630371, 34.125267028808594, 10.85598373413086, 18.866031646728516, 36.630393981933594, 12.441924095153809, 19.216773986816406, 39.38705062866211, 15.476753234863281, 17.405351638793945, 32.31757354736328, 9.807567596435547, 16.14618492126465, 35.03086471557617, 11.549427032470703, 19.89963722229004, 40.64130401611328, 17.76375389099121, 11.800134658813477, 23.093948364257812], 'accuracy': [0.7049180269241333, 0.4453551769256592, 0.7213114500045776, 0.5464481115341187, 0.5983606576919556, 0.5901639461517334, 0.6666666865348816, 0.2923497259616852, 0.7349726557731628, 0.7213114500045776, 0.5491803288459778, 0.7185792326927185, 0.44262295961380005, 0.7349726557731628, 0.6830601096153259, 0.44262295961380005, 0.7349726557731628, 0.7021858096122742, 0.43169400095939636, 0.7349726557731628, 0.7021858096122742, 0.4644808769226074, 0.7349726557731628, 0.7049180269241333, 0.4508196711540222, 0.7349726557731628, 0.7131147384643555, 0.4453551769256592, 0.7349726557731628, 0.7103825211524963, 0.4836065471172333, 0.7349726557731628, 0.7103825211524963, 0.4863387942314148, 0.7349726557731628, 0.7131147384643555, 0.45628416538238525, 0.7349726557731628, 0.7404371500015259, 0.4726775884628296, 0.7349726557731628, 0.7185792326927185, 0.49180328845977783, 0.7349726557731628, 0.7295082211494446, 0.42896175384521484, 0.7349726557731628, 0.7431694269180298, 0.5300546288490295, 0.7377049326896667], 'precision_10': [0.7049180269241333, 0.4453551769256592, 0.7213114500045776, 0.5464481115341187, 0.5983606576919556, 0.5901639461517334, 0.6666666865348816, 0.2923497259616852, 0.7349726557731628, 0.7213114500045776, 0.5491803288459778, 0.7185792326927185, 0.44262295961380005, 0.7349726557731628, 0.6830601096153259, 0.44262295961380005, 0.7349726557731628, 0.7021858096122742, 0.43169400095939636, 0.7349726557731628, 0.7021858096122742, 0.4644808769226074, 0.7349726557731628, 0.7049180269241333, 0.4508196711540222, 0.7349726557731628, 0.7131147384643555, 0.4453551769256592, 0.7349726557731628, 0.7103825211524963, 0.4836065471172333, 0.7349726557731628, 0.7103825211524963, 0.4863387942314148, 0.7349726557731628, 0.7131147384643555, 0.45628416538238525, 0.7349726557731628, 0.7404371500015259, 0.4726775884628296, 0.7349726557731628, 0.7185792326927185, 0.49180328845977783, 0.7349726557731628, 0.7295082211494446, 0.42896175384521484, 0.7349726557731628, 0.7431694269180298, 0.5300546288490295, 0.7377049326896667], 'recall_10': [0.7049180269241333, 0.4453551769256592, 0.7213114500045776, 0.5464481115341187, 0.5983606576919556, 0.5901639461517334, 0.6666666865348816, 0.2923497259616852, 0.7349726557731628, 0.7213114500045776, 0.5491803288459778, 0.7185792326927185, 0.44262295961380005, 0.7349726557731628, 0.6830601096153259, 0.44262295961380005, 0.7349726557731628, 0.7021858096122742, 0.43169400095939636, 0.7349726557731628, 0.7021858096122742, 0.4644808769226074, 0.7349726557731628, 0.7049180269241333, 0.4508196711540222, 0.7349726557731628, 0.7131147384643555, 0.4453551769256592, 0.7349726557731628, 0.7103825211524963, 0.4836065471172333, 0.7349726557731628, 0.7103825211524963, 0.4863387942314148, 0.7349726557731628, 0.7131147384643555, 0.45628416538238525, 0.7349726557731628, 0.7404371500015259, 0.4726775884628296, 0.7349726557731628, 0.7185792326927185, 0.49180328845977783, 0.7349726557731628, 0.7295082211494446, 0.42896175384521484, 0.7349726557731628, 0.7431694269180298, 0.5300546288490295, 0.7377049326896667]}\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fdb2566bf80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "> Fold 1 - Precison: 0.7183270892270051 - Recall: 0.5517241379310345%\n",
      "Shape of train set: (366, 20)\n",
      "Shape of y: (366, 2)\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Shape of validation set: (41, 20)\n",
      "{'loss': [176.16592407226562, 56.23481750488281, 119.52876281738281, 80.73194885253906, 38.2622184753418, 39.65324401855469, 63.27497482299805, 27.253047943115234, 33.140357971191406, 58.062286376953125, 24.98649024963379, 25.548372268676758, 50.69290542602539, 20.185876846313477, 27.135454177856445, 51.267791748046875, 21.811906814575195, 20.99970054626465, 42.35342788696289, 14.957822799682617, 20.15858268737793, 44.62652587890625, 16.374704360961914, 20.18803596496582, 45.86358642578125, 17.70777130126953, 22.912994384765625, 50.77219009399414, 24.033016204833984, 11.841757774353027, 21.2735595703125, 16.132991790771484, 39.611839294433594, 13.17290210723877, 18.42886734008789, 45.95512771606445, 19.389806747436523, 14.550323486328125, 36.54731369018555, 11.3294095993042, 16.152206420898438, 43.00072479248047, 16.858186721801758, 16.597442626953125, 42.279335021972656, 16.952030181884766, 14.171483039855957, 37.22576904296875, 12.52059555053711, 19.09781837463379], 'accuracy': [0.751366138458252, 0.7076502442359924, 0.39344263076782227, 0.751366138458252, 0.7322404384613037, 0.4480874240398407, 0.7540983557701111, 0.6967213153839111, 0.44262295961380005, 0.7540983557701111, 0.7213114500045776, 0.46994534134864807, 0.7540983557701111, 0.7240437269210815, 0.4344262182712555, 0.7540983557701111, 0.7404371500015259, 0.4836065471172333, 0.7540983557701111, 0.7076502442359924, 0.5, 0.7568305730819702, 0.7213114500045776, 0.4836065471172333, 0.7568305730819702, 0.7540983557701111, 0.42896175384521484, 0.751366138458252, 0.7650273442268372, 0.5765027403831482, 0.7595628499984741, 0.4890710413455963, 0.7568305730819702, 0.7431694269180298, 0.4644808769226074, 0.7540983557701111, 0.751366138458252, 0.505464494228363, 0.7568305730819702, 0.7240437269210815, 0.48087432980537415, 0.7540983557701111, 0.7595628499984741, 0.4590163826942444, 0.7540983557701111, 0.7568305730819702, 0.49453550577163696, 0.7568305730819702, 0.7540983557701111, 0.437158465385437], 'precision_11': [0.751366138458252, 0.7076502442359924, 0.39344263076782227, 0.751366138458252, 0.7322404384613037, 0.4480874240398407, 0.7540983557701111, 0.6967213153839111, 0.44262295961380005, 0.7540983557701111, 0.7213114500045776, 0.46994534134864807, 0.7540983557701111, 0.7240437269210815, 0.4344262182712555, 0.7540983557701111, 0.7404371500015259, 0.4836065471172333, 0.7540983557701111, 0.7076502442359924, 0.5, 0.7568305730819702, 0.7213114500045776, 0.4836065471172333, 0.7568305730819702, 0.7540983557701111, 0.42896175384521484, 0.751366138458252, 0.7650273442268372, 0.5765027403831482, 0.7595628499984741, 0.4890710413455963, 0.7568305730819702, 0.7431694269180298, 0.4644808769226074, 0.7540983557701111, 0.751366138458252, 0.505464494228363, 0.7568305730819702, 0.7240437269210815, 0.48087432980537415, 0.7540983557701111, 0.7595628499984741, 0.4590163826942444, 0.7540983557701111, 0.7568305730819702, 0.49453550577163696, 0.7568305730819702, 0.7540983557701111, 0.437158465385437], 'recall_11': [0.751366138458252, 0.7076502442359924, 0.39344263076782227, 0.751366138458252, 0.7322404384613037, 0.4480874240398407, 0.7540983557701111, 0.6967213153839111, 0.44262295961380005, 0.7540983557701111, 0.7213114500045776, 0.46994534134864807, 0.7540983557701111, 0.7240437269210815, 0.4344262182712555, 0.7540983557701111, 0.7404371500015259, 0.4836065471172333, 0.7540983557701111, 0.7076502442359924, 0.5, 0.7568305730819702, 0.7213114500045776, 0.4836065471172333, 0.7568305730819702, 0.7540983557701111, 0.42896175384521484, 0.751366138458252, 0.7650273442268372, 0.5765027403831482, 0.7595628499984741, 0.4890710413455963, 0.7568305730819702, 0.7431694269180298, 0.4644808769226074, 0.7540983557701111, 0.751366138458252, 0.505464494228363, 0.7568305730819702, 0.7240437269210815, 0.48087432980537415, 0.7540983557701111, 0.7595628499984741, 0.4590163826942444, 0.7540983557701111, 0.7568305730819702, 0.49453550577163696, 0.7568305730819702, 0.7540983557701111, 0.437158465385437]}\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fdb20ab7a70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Fold 2 - Precison: 0.5609756097560976 - Recall: 1.0%\n",
      "Shape of train set: (366, 20)\n",
      "Shape of y: (366, 2)\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Shape of validation set: (41, 20)\n",
      "{'loss': [305.1820373535156, 82.50167846679688, 60.016597747802734, 52.170955657958984, 48.61698532104492, 47.026981353759766, 46.22585678100586, 37.0457649230957, 52.567020416259766, 21.386293411254883, 19.18686866760254, 41.25372314453125, 63.686180114746094, 26.292051315307617, 24.822185516357422, 40.92902755737305, 13.82861328125, 12.739027976989746, 17.234302520751953, 37.69935607910156, 57.892459869384766, 25.144893646240234, 16.144285202026367, 33.89816665649414, 11.131948471069336, 10.940657615661621, 19.60194206237793, 42.485443115234375, 11.74283218383789, 21.026182174682617, 48.54228973388672, 17.806760787963867, 19.339223861694336, 39.252525329589844, 11.153905868530273, 20.584186553955078, 44.61845397949219, 16.276386260986328, 17.587947845458984, 34.09151077270508, 8.866761207580566, 12.682168006896973, 30.768762588500977, 7.904611587524414, 7.193146705627441, 11.217889785766602, 33.41987228393555, 7.053526878356934, 6.397974491119385, 11.316082000732422], 'accuracy': [0.31420764327049255, 0.5409836173057556, 0.6775956153869629, 0.5928961634635925, 0.7076502442359924, 0.5491803288459778, 0.7349726557731628, 0.5109289884567261, 0.7295082211494446, 0.5792349576950073, 0.6612021923065186, 0.437158465385437, 0.7267759442329407, 0.7322404384613037, 0.5, 0.7267759442329407, 0.6338797807693481, 0.631147563457489, 0.7213114500045776, 0.41530054807662964, 0.7267759442329407, 0.7459016442298889, 0.5355191230773926, 0.7349726557731628, 0.6393442749977112, 0.7049180269241333, 0.5300546288490295, 0.7267759442329407, 0.7049180269241333, 0.505464494228363, 0.7267759442329407, 0.7349726557731628, 0.5109289884567261, 0.7267759442329407, 0.7267759442329407, 0.4863387942314148, 0.7267759442329407, 0.7431694269180298, 0.5245901346206665, 0.7267759442329407, 0.7158470153808594, 0.5956284403800964, 0.7377049326896667, 0.688524603843689, 0.7267759442329407, 0.6092896461486816, 0.7322404384613037, 0.6967213153839111, 0.7377049326896667, 0.5819672346115112], 'precision_12': [0.31420764327049255, 0.5409836173057556, 0.6775956153869629, 0.5928961634635925, 0.7076502442359924, 0.5491803288459778, 0.7349726557731628, 0.5109289884567261, 0.7295082211494446, 0.5792349576950073, 0.6612021923065186, 0.437158465385437, 0.7267759442329407, 0.7322404384613037, 0.5, 0.7267759442329407, 0.6338797807693481, 0.631147563457489, 0.7213114500045776, 0.41530054807662964, 0.7267759442329407, 0.7459016442298889, 0.5355191230773926, 0.7349726557731628, 0.6393442749977112, 0.7049180269241333, 0.5300546288490295, 0.7267759442329407, 0.7049180269241333, 0.505464494228363, 0.7267759442329407, 0.7349726557731628, 0.5109289884567261, 0.7267759442329407, 0.7267759442329407, 0.4863387942314148, 0.7267759442329407, 0.7431694269180298, 0.5245901346206665, 0.7267759442329407, 0.7158470153808594, 0.5956284403800964, 0.7377049326896667, 0.688524603843689, 0.7267759442329407, 0.6092896461486816, 0.7322404384613037, 0.6967213153839111, 0.7377049326896667, 0.5819672346115112], 'recall_12': [0.31420764327049255, 0.5409836173057556, 0.6775956153869629, 0.5928961634635925, 0.7076502442359924, 0.5491803288459778, 0.7349726557731628, 0.5109289884567261, 0.7295082211494446, 0.5792349576950073, 0.6612021923065186, 0.437158465385437, 0.7267759442329407, 0.7322404384613037, 0.5, 0.7267759442329407, 0.6338797807693481, 0.631147563457489, 0.7213114500045776, 0.41530054807662964, 0.7267759442329407, 0.7459016442298889, 0.5355191230773926, 0.7349726557731628, 0.6393442749977112, 0.7049180269241333, 0.5300546288490295, 0.7267759442329407, 0.7049180269241333, 0.505464494228363, 0.7267759442329407, 0.7349726557731628, 0.5109289884567261, 0.7267759442329407, 0.7267759442329407, 0.4863387942314148, 0.7267759442329407, 0.7431694269180298, 0.5245901346206665, 0.7267759442329407, 0.7158470153808594, 0.5956284403800964, 0.7377049326896667, 0.688524603843689, 0.7267759442329407, 0.6092896461486816, 0.7322404384613037, 0.6967213153839111, 0.7377049326896667, 0.5819672346115112]}\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fdb20ae0d40> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "> Fold 3 - Precison: 0.8048780487804879 - Recall: 1.0%\n",
      "Shape of train set: (366, 20)\n",
      "Shape of y: (366, 2)\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "Shape of validation set: (41, 20)\n",
      "{'loss': [611.2083740234375, 252.65390014648438, 45.067901611328125, 35.983543395996094, 26.854881286621094, 22.555442810058594, 22.595396041870117, 31.95167350769043, 28.19149398803711, 51.39186477661133, 18.004684448242188, 29.64833641052246, 51.99296569824219, 20.555307388305664, 27.806638717651367, 47.44023895263672, 18.371553421020508, 26.026399612426758, 44.68680191040039, 17.295562744140625, 22.139087677001953, 40.06706619262695, 13.93382740020752, 21.20735740661621, 40.108985900878906, 13.87967300415039, 20.70697593688965, 41.94744110107422, 15.383645057678223, 22.13800621032715, 44.3986930847168, 18.57615089416504, 16.090085983276367, 30.321550369262695, 8.893139839172363, 9.921781539916992, 19.506433486938477, 18.933591842651367, 44.651180267333984, 17.257539749145508, 18.286067962646484, 41.131160736083984, 15.204833030700684, 19.030302047729492, 42.594329833984375, 17.4974365234375, 13.923225402832031, 28.965232849121094, 7.684403419494629, 12.467194557189941], 'accuracy': [0.2704918086528778, 0.284153014421463, 0.5191256999969482, 0.631147563457489, 0.5546448230743408, 0.6229507923126221, 0.5409836173057556, 0.7185792326927185, 0.4508196711540222, 0.7295082211494446, 0.6721311211585999, 0.41803279519081116, 0.7295082211494446, 0.7076502442359924, 0.42896175384521484, 0.7295082211494446, 0.7213114500045776, 0.42896175384521484, 0.7322404384613037, 0.7158470153808594, 0.4644808769226074, 0.7349726557731628, 0.7076502442359924, 0.48087432980537415, 0.7349726557731628, 0.7103825211524963, 0.48087432980537415, 0.7349726557731628, 0.7240437269210815, 0.43989071249961853, 0.7322404384613037, 0.7295082211494446, 0.5245901346206665, 0.7322404384613037, 0.6557376980781555, 0.5928961634635925, 0.7295082211494446, 0.4836065471172333, 0.7295082211494446, 0.7295082211494446, 0.4890710413455963, 0.7377049326896667, 0.7349726557731628, 0.4754098355770111, 0.7349726557731628, 0.7349726557731628, 0.5573770403862, 0.7349726557731628, 0.6748633980751038, 0.5737704634666443], 'precision_13': [0.2704918086528778, 0.284153014421463, 0.5191256999969482, 0.631147563457489, 0.5546448230743408, 0.6229507923126221, 0.5409836173057556, 0.7185792326927185, 0.4508196711540222, 0.7295082211494446, 0.6721311211585999, 0.41803279519081116, 0.7295082211494446, 0.7076502442359924, 0.42896175384521484, 0.7295082211494446, 0.7213114500045776, 0.42896175384521484, 0.7322404384613037, 0.7158470153808594, 0.4644808769226074, 0.7349726557731628, 0.7076502442359924, 0.48087432980537415, 0.7349726557731628, 0.7103825211524963, 0.48087432980537415, 0.7349726557731628, 0.7240437269210815, 0.43989071249961853, 0.7322404384613037, 0.7295082211494446, 0.5245901346206665, 0.7322404384613037, 0.6557376980781555, 0.5928961634635925, 0.7295082211494446, 0.4836065471172333, 0.7295082211494446, 0.7295082211494446, 0.4890710413455963, 0.7377049326896667, 0.7349726557731628, 0.4754098355770111, 0.7349726557731628, 0.7349726557731628, 0.5573770403862, 0.7349726557731628, 0.6748633980751038, 0.5737704634666443], 'recall_13': [0.2704918086528778, 0.284153014421463, 0.5191256999969482, 0.631147563457489, 0.5546448230743408, 0.6229507923126221, 0.5409836173057556, 0.7185792326927185, 0.4508196711540222, 0.7295082211494446, 0.6721311211585999, 0.41803279519081116, 0.7295082211494446, 0.7076502442359924, 0.42896175384521484, 0.7295082211494446, 0.7213114500045776, 0.42896175384521484, 0.7322404384613037, 0.7158470153808594, 0.4644808769226074, 0.7349726557731628, 0.7076502442359924, 0.48087432980537415, 0.7349726557731628, 0.7103825211524963, 0.48087432980537415, 0.7349726557731628, 0.7240437269210815, 0.43989071249961853, 0.7322404384613037, 0.7295082211494446, 0.5245901346206665, 0.7322404384613037, 0.6557376980781555, 0.5928961634635925, 0.7295082211494446, 0.4836065471172333, 0.7295082211494446, 0.7295082211494446, 0.4890710413455963, 0.7377049326896667, 0.7349726557731628, 0.4754098355770111, 0.7349726557731628, 0.7349726557731628, 0.5573770403862, 0.7349726557731628, 0.6748633980751038, 0.5737704634666443]}\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fdb24b56cb0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Fold 4 - Precison: 0.7443966995501221 - Recall: 0.9354838709677419%\n",
      "Shape of train set: (366, 20)\n",
      "Shape of y: (366, 2)\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "Shape of validation set: (41, 20)\n",
      "{'loss': [134.9619140625, 33.370975494384766, 58.69575119018555, 64.05646514892578, 80.25300598144531, 37.61253356933594, 21.271268844604492, 37.26090621948242, 19.285696029663086, 38.21417999267578, 16.002241134643555, 27.62439727783203, 27.830486297607422, 56.72724151611328, 22.95014190673828, 20.95936393737793, 40.05989074707031, 12.114126205444336, 12.677444458007812, 24.637693405151367, 22.894041061401367, 52.69942855834961, 20.737552642822266, 19.120758056640625, 40.9484748840332, 12.18047046661377, 18.681774139404297, 40.278812408447266, 11.53691291809082, 18.104541778564453, 42.84105682373047, 13.446864128112793, 24.240156173706055, 47.26233673095703, 18.846908569335938, 14.447746276855469, 32.11404800415039, 8.579821586608887, 8.939467430114746, 13.915921211242676, 29.487253189086914, 51.3735466003418, 23.155555725097656, 9.303893089294434, 12.643247604370117, 23.54360580444336, 46.107688903808594, 18.865604400634766, 11.241157531738281, 27.078739166259766], 'accuracy': [0.7295082211494446, 0.5846994519233704, 0.7322404384613037, 0.43169400095939636, 0.7322404384613037, 0.7295082211494446, 0.5191256999969482, 0.7349726557731628, 0.5218579173088074, 0.7377049326896667, 0.568306028842926, 0.7240437269210815, 0.46721312403678894, 0.7322404384613037, 0.7267759442329407, 0.5218579173088074, 0.7404371500015259, 0.6775956153869629, 0.6010928750038147, 0.7431694269180298, 0.4754098355770111, 0.7322404384613037, 0.7267759442329407, 0.5245901346206665, 0.7377049326896667, 0.7131147384643555, 0.5273224115371704, 0.7377049326896667, 0.7185792326927185, 0.5300546288490295, 0.7377049326896667, 0.7295082211494446, 0.4863387942314148, 0.7322404384613037, 0.7322404384613037, 0.5519125461578369, 0.7404371500015259, 0.6967213153839111, 0.6475409865379333, 0.7431694269180298, 0.4262295067310333, 0.7322404384613037, 0.748633861541748, 0.6229507923126221, 0.7404371500015259, 0.48087432980537415, 0.7349726557731628, 0.7595628499984741, 0.5846994519233704, 0.7404371500015259], 'precision_14': [0.7295082211494446, 0.5846994519233704, 0.7322404384613037, 0.43169400095939636, 0.7322404384613037, 0.7295082211494446, 0.5191256999969482, 0.7349726557731628, 0.5218579173088074, 0.7377049326896667, 0.568306028842926, 0.7240437269210815, 0.46721312403678894, 0.7322404384613037, 0.7267759442329407, 0.5218579173088074, 0.7404371500015259, 0.6775956153869629, 0.6010928750038147, 0.7431694269180298, 0.4754098355770111, 0.7322404384613037, 0.7267759442329407, 0.5245901346206665, 0.7377049326896667, 0.7131147384643555, 0.5273224115371704, 0.7377049326896667, 0.7185792326927185, 0.5300546288490295, 0.7377049326896667, 0.7295082211494446, 0.4863387942314148, 0.7322404384613037, 0.7322404384613037, 0.5519125461578369, 0.7404371500015259, 0.6967213153839111, 0.6475409865379333, 0.7431694269180298, 0.4262295067310333, 0.7322404384613037, 0.748633861541748, 0.6229507923126221, 0.7404371500015259, 0.48087432980537415, 0.7349726557731628, 0.7595628499984741, 0.5846994519233704, 0.7404371500015259], 'recall_14': [0.7295082211494446, 0.5846994519233704, 0.7322404384613037, 0.43169400095939636, 0.7322404384613037, 0.7295082211494446, 0.5191256999969482, 0.7349726557731628, 0.5218579173088074, 0.7377049326896667, 0.568306028842926, 0.7240437269210815, 0.46721312403678894, 0.7322404384613037, 0.7267759442329407, 0.5218579173088074, 0.7404371500015259, 0.6775956153869629, 0.6010928750038147, 0.7431694269180298, 0.4754098355770111, 0.7322404384613037, 0.7267759442329407, 0.5245901346206665, 0.7377049326896667, 0.7131147384643555, 0.5273224115371704, 0.7377049326896667, 0.7185792326927185, 0.5300546288490295, 0.7377049326896667, 0.7295082211494446, 0.4863387942314148, 0.7322404384613037, 0.7322404384613037, 0.5519125461578369, 0.7404371500015259, 0.6967213153839111, 0.6475409865379333, 0.7431694269180298, 0.4262295067310333, 0.7322404384613037, 0.748633861541748, 0.6229507923126221, 0.7404371500015259, 0.48087432980537415, 0.7349726557731628, 0.7595628499984741, 0.5846994519233704, 0.7404371500015259]}\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fdb233e2c20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "> Fold 5 - Precison: 0.7707841594544977 - Recall: 0.6774193548387096%\n",
      "Shape of train set: (366, 20)\n",
      "Shape of y: (366, 2)\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 6 ...\n",
      "Shape of validation set: (41, 20)\n",
      "{'loss': [167.17520141601562, 57.079689025878906, 49.43404769897461, 98.99732208251953, 41.97751235961914, 43.349430084228516, 74.48272705078125, 35.05703353881836, 24.805408477783203, 40.67414474487305, 17.326778411865234, 26.462181091308594, 34.47496032714844, 63.93619155883789, 30.5444278717041, 16.92281723022461, 32.5010871887207, 15.053051948547363, 32.648983001708984, 14.955074310302734, 36.4337158203125, 11.583080291748047, 14.643216133117676, 33.786014556884766, 62.400901794433594, 31.118349075317383, 10.795127868652344, 16.762964248657227, 30.17133903503418, 55.01838302612305, 25.93254280090332, 11.875897407531738, 25.055208206176758, 13.812824249267578, 38.34211349487305, 10.954739570617676, 21.42320442199707, 52.0533332824707, 23.735767364501953, 11.439396858215332, 28.37649917602539, 8.441365242004395, 13.191141128540039, 31.447006225585938, 54.21890640258789, 26.947071075439453, 7.756856441497803, 7.240795135498047, 7.4524455070495605, 17.428756713867188], 'accuracy': [0.7349726557731628, 0.6721311211585999, 0.4836065471172333, 0.7349726557731628, 0.7213114500045776, 0.4644808769226074, 0.7295082211494446, 0.7158470153808594, 0.562841534614563, 0.7131147384643555, 0.562841534614563, 0.7158470153808594, 0.4344262182712555, 0.7322404384613037, 0.7213114500045776, 0.562841534614563, 0.7185792326927185, 0.5737704634666443, 0.7240437269210815, 0.5710382461547852, 0.7213114500045776, 0.6366119980812073, 0.6912568211555481, 0.4234972596168518, 0.7322404384613037, 0.7267759442329407, 0.6147540807723999, 0.7240437269210815, 0.4262295067310333, 0.7295082211494446, 0.7322404384613037, 0.5901639461517334, 0.7404371500015259, 0.5136612057685852, 0.7377049326896667, 0.6803278923034668, 0.4453551769256592, 0.7295082211494446, 0.7377049326896667, 0.5546448230743408, 0.7349726557731628, 0.6366119980812073, 0.7377049326896667, 0.3852458894252777, 0.7295082211494446, 0.7431694269180298, 0.6584699749946594, 0.693989098072052, 0.6284152865409851, 0.7404371500015259], 'precision_15': [0.7349726557731628, 0.6721311211585999, 0.4836065471172333, 0.7349726557731628, 0.7213114500045776, 0.4644808769226074, 0.7295082211494446, 0.7158470153808594, 0.562841534614563, 0.7131147384643555, 0.562841534614563, 0.7158470153808594, 0.4344262182712555, 0.7322404384613037, 0.7213114500045776, 0.562841534614563, 0.7185792326927185, 0.5737704634666443, 0.7240437269210815, 0.5710382461547852, 0.7213114500045776, 0.6366119980812073, 0.6912568211555481, 0.4234972596168518, 0.7322404384613037, 0.7267759442329407, 0.6147540807723999, 0.7240437269210815, 0.4262295067310333, 0.7295082211494446, 0.7322404384613037, 0.5901639461517334, 0.7404371500015259, 0.5136612057685852, 0.7377049326896667, 0.6803278923034668, 0.4453551769256592, 0.7295082211494446, 0.7377049326896667, 0.5546448230743408, 0.7349726557731628, 0.6366119980812073, 0.7377049326896667, 0.3852458894252777, 0.7295082211494446, 0.7431694269180298, 0.6584699749946594, 0.693989098072052, 0.6284152865409851, 0.7404371500015259], 'recall_15': [0.7349726557731628, 0.6721311211585999, 0.4836065471172333, 0.7349726557731628, 0.7213114500045776, 0.4644808769226074, 0.7295082211494446, 0.7158470153808594, 0.562841534614563, 0.7131147384643555, 0.562841534614563, 0.7158470153808594, 0.4344262182712555, 0.7322404384613037, 0.7213114500045776, 0.562841534614563, 0.7185792326927185, 0.5737704634666443, 0.7240437269210815, 0.5710382461547852, 0.7213114500045776, 0.6366119980812073, 0.6912568211555481, 0.4234972596168518, 0.7322404384613037, 0.7267759442329407, 0.6147540807723999, 0.7240437269210815, 0.4262295067310333, 0.7295082211494446, 0.7322404384613037, 0.5901639461517334, 0.7404371500015259, 0.5136612057685852, 0.7377049326896667, 0.6803278923034668, 0.4453551769256592, 0.7295082211494446, 0.7377049326896667, 0.5546448230743408, 0.7349726557731628, 0.6366119980812073, 0.7377049326896667, 0.3852458894252777, 0.7295082211494446, 0.7431694269180298, 0.6584699749946594, 0.693989098072052, 0.6284152865409851, 0.7404371500015259]}\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fdb21d9d830> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Fold 6 - Precison: 0.7476871320437342 - Recall: 0.13793103448275862%\n",
      "Shape of train set: (366, 20)\n",
      "Shape of y: (366, 2)\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 7 ...\n",
      "Shape of validation set: (41, 20)\n",
      "{'loss': [36.902687072753906, 132.08523559570312, 41.160125732421875, 18.530298233032227, 22.816909790039062, 28.047216415405273, 38.706077575683594, 14.025179862976074, 20.682247161865234, 34.2388801574707, 11.67551040649414, 19.490459442138672, 36.1032829284668, 14.550166130065918, 17.459028244018555, 32.60617446899414, 12.627992630004883, 16.57488441467285, 30.557518005371094, 11.544832229614258, 14.963037490844727, 28.55360984802246, 10.116414070129395, 12.668684005737305, 26.696142196655273, 8.801594734191895, 11.756204605102539, 27.889068603515625, 9.193059921264648, 12.231974601745605, 28.47842025756836, 9.700113296508789, 14.724173545837402, 29.32633399963379, 10.869685173034668, 13.063621520996094, 27.127216339111328, 9.38273811340332, 13.151028633117676, 27.181921005249023, 9.548107147216797, 12.34886646270752, 25.40776824951172, 8.329875946044922, 9.885912895202637, 22.12932586669922, 6.221408843994141, 7.876507759094238, 19.74451446533203, 5.130527973175049], 'accuracy': [0.562841534614563, 0.2732240557670593, 0.7267759442329407, 0.5573770403862, 0.6830601096153259, 0.45628416538238525, 0.7295082211494446, 0.6202185750007629, 0.4644808769226074, 0.7322404384613037, 0.6284152865409851, 0.4617486298084259, 0.7267759442329407, 0.6748633980751038, 0.4480874240398407, 0.7267759442329407, 0.6639344096183777, 0.46994534134864807, 0.7322404384613037, 0.6666666865348816, 0.4726775884628296, 0.7322404384613037, 0.6612021923065186, 0.5081967115402222, 0.7322404384613037, 0.6530054807662964, 0.5109289884567261, 0.7295082211494446, 0.6639344096183777, 0.5027322173118591, 0.7295082211494446, 0.6830601096153259, 0.4863387942314148, 0.7295082211494446, 0.7131147384643555, 0.49180328845977783, 0.7295082211494446, 0.7158470153808594, 0.4972677528858185, 0.7295082211494446, 0.7240437269210815, 0.49180328845977783, 0.7295082211494446, 0.7267759442329407, 0.5519125461578369, 0.7349726557731628, 0.6584699749946594, 0.5792349576950073, 0.7404371500015259, 0.6366119980812073], 'precision_16': [0.562841534614563, 0.2732240557670593, 0.7267759442329407, 0.5573770403862, 0.6830601096153259, 0.45628416538238525, 0.7295082211494446, 0.6202185750007629, 0.4644808769226074, 0.7322404384613037, 0.6284152865409851, 0.4617486298084259, 0.7267759442329407, 0.6748633980751038, 0.4480874240398407, 0.7267759442329407, 0.6639344096183777, 0.46994534134864807, 0.7322404384613037, 0.6666666865348816, 0.4726775884628296, 0.7322404384613037, 0.6612021923065186, 0.5081967115402222, 0.7322404384613037, 0.6530054807662964, 0.5109289884567261, 0.7295082211494446, 0.6639344096183777, 0.5027322173118591, 0.7295082211494446, 0.6830601096153259, 0.4863387942314148, 0.7295082211494446, 0.7131147384643555, 0.49180328845977783, 0.7295082211494446, 0.7158470153808594, 0.4972677528858185, 0.7295082211494446, 0.7240437269210815, 0.49180328845977783, 0.7295082211494446, 0.7267759442329407, 0.5519125461578369, 0.7349726557731628, 0.6584699749946594, 0.5792349576950073, 0.7404371500015259, 0.6366119980812073], 'recall_16': [0.562841534614563, 0.2732240557670593, 0.7267759442329407, 0.5573770403862, 0.6830601096153259, 0.45628416538238525, 0.7295082211494446, 0.6202185750007629, 0.4644808769226074, 0.7322404384613037, 0.6284152865409851, 0.4617486298084259, 0.7267759442329407, 0.6748633980751038, 0.4480874240398407, 0.7267759442329407, 0.6639344096183777, 0.46994534134864807, 0.7322404384613037, 0.6666666865348816, 0.4726775884628296, 0.7322404384613037, 0.6612021923065186, 0.5081967115402222, 0.7322404384613037, 0.6530054807662964, 0.5109289884567261, 0.7295082211494446, 0.6639344096183777, 0.5027322173118591, 0.7295082211494446, 0.6830601096153259, 0.4863387942314148, 0.7295082211494446, 0.7131147384643555, 0.49180328845977783, 0.7295082211494446, 0.7158470153808594, 0.4972677528858185, 0.7295082211494446, 0.7240437269210815, 0.49180328845977783, 0.7295082211494446, 0.7267759442329407, 0.5519125461578369, 0.7349726557731628, 0.6584699749946594, 0.5792349576950073, 0.7404371500015259, 0.6366119980812073]}\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fdb20a21b00> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "> Fold 7 - Precison: 0.7734521575984992 - Recall: 0.625%\n",
      "Shape of train set: (367, 20)\n",
      "Shape of y: (367, 2)\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 8 ...\n",
      "Shape of validation set: (40, 20)\n",
      "{'loss': [119.14041900634766, 63.49052047729492, 32.829559326171875, 27.165306091308594, 23.263586044311523, 29.8524227142334, 44.206729888916016, 15.721181869506836, 17.5966739654541, 37.98746109008789, 11.496716499328613, 13.188897132873535, 37.4465217590332, 10.011409759521484, 24.51968765258789, 51.13631057739258, 25.108352661132812, 8.017435073852539, 10.529316902160645, 30.19339370727539, 45.3550910949707, 22.0046329498291, 6.7460832595825195, 8.045389175415039, 22.964946746826172, 42.20564651489258, 19.92498207092285, 6.14886999130249, 12.336726188659668, 20.6591854095459, 40.85667419433594, 19.55545997619629, 5.4605841636657715, 5.4253830909729, 19.062854766845703, 37.93914794921875, 16.68082046508789, 6.241661071777344, 18.625234603881836, 5.648787975311279, 17.362619400024414, 7.251370906829834, 28.745817184448242, 7.871865272521973, 18.215116500854492, 39.42162322998047, 18.476552963256836, 4.358332633972168, 4.681778430938721, 15.204718589782715], 'accuracy': [0.35694822669029236, 0.7329699993133545, 0.6239781975746155, 0.5667575001716614, 0.6948229074478149, 0.47138965129852295, 0.7220708727836609, 0.6485013365745544, 0.5367847681045532, 0.7356948256492615, 0.6621253490447998, 0.553133487701416, 0.7329699993133545, 0.6730245351791382, 0.4468664824962616, 0.7329699993133545, 0.7384195923805237, 0.6158038377761841, 0.7356948256492615, 0.41689372062683105, 0.7302452325820923, 0.7438691854476929, 0.6512261629104614, 0.7438691854476929, 0.46049046516418457, 0.7329699993133545, 0.7384195923805237, 0.6348773837089539, 0.7602179646492004, 0.43051770329475403, 0.7329699993133545, 0.752043604850769, 0.6784741282463074, 0.7356948256492615, 0.48228883743286133, 0.7329699993133545, 0.7547683715820312, 0.6021798253059387, 0.7602179646492004, 0.6348773837089539, 0.7629427909851074, 0.5585830807685852, 0.7438691854476929, 0.7574931979179382, 0.4332424998283386, 0.7302452325820923, 0.7602179646492004, 0.6975476741790771, 0.7493187785148621, 0.4904632270336151], 'precision_17': [0.35694822669029236, 0.7329699993133545, 0.6239781975746155, 0.5667575001716614, 0.6948229074478149, 0.47138965129852295, 0.7220708727836609, 0.6485013365745544, 0.5367847681045532, 0.7356948256492615, 0.6621253490447998, 0.553133487701416, 0.7329699993133545, 0.6730245351791382, 0.4468664824962616, 0.7329699993133545, 0.7384195923805237, 0.6158038377761841, 0.7356948256492615, 0.41689372062683105, 0.7302452325820923, 0.7438691854476929, 0.6512261629104614, 0.7438691854476929, 0.46049046516418457, 0.7329699993133545, 0.7384195923805237, 0.6348773837089539, 0.7602179646492004, 0.43051770329475403, 0.7329699993133545, 0.752043604850769, 0.6784741282463074, 0.7356948256492615, 0.48228883743286133, 0.7329699993133545, 0.7547683715820312, 0.6021798253059387, 0.7602179646492004, 0.6348773837089539, 0.7629427909851074, 0.5585830807685852, 0.7438691854476929, 0.7574931979179382, 0.4332424998283386, 0.7302452325820923, 0.7602179646492004, 0.6975476741790771, 0.7493187785148621, 0.4904632270336151], 'recall_17': [0.35694822669029236, 0.7329699993133545, 0.6239781975746155, 0.5667575001716614, 0.6948229074478149, 0.47138965129852295, 0.7220708727836609, 0.6485013365745544, 0.5367847681045532, 0.7356948256492615, 0.6621253490447998, 0.553133487701416, 0.7329699993133545, 0.6730245351791382, 0.4468664824962616, 0.7329699993133545, 0.7384195923805237, 0.6158038377761841, 0.7356948256492615, 0.41689372062683105, 0.7302452325820923, 0.7438691854476929, 0.6512261629104614, 0.7438691854476929, 0.46049046516418457, 0.7329699993133545, 0.7384195923805237, 0.6348773837089539, 0.7602179646492004, 0.43051770329475403, 0.7329699993133545, 0.752043604850769, 0.6784741282463074, 0.7356948256492615, 0.48228883743286133, 0.7329699993133545, 0.7547683715820312, 0.6021798253059387, 0.7602179646492004, 0.6348773837089539, 0.7629427909851074, 0.5585830807685852, 0.7438691854476929, 0.7574931979179382, 0.4332424998283386, 0.7302452325820923, 0.7602179646492004, 0.6975476741790771, 0.7493187785148621, 0.4904632270336151]}\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fdb2842bdd0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Fold 8 - Precison: 0.75 - Recall: 1.0%\n",
      "Shape of train set: (367, 20)\n",
      "Shape of y: (367, 2)\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 9 ...\n",
      "Shape of validation set: (40, 20)\n",
      "{'loss': [80.46319580078125, 83.47893524169922, 54.335304260253906, 36.75993728637695, 31.263643264770508, 30.26725959777832, 37.09127426147461, 51.229400634765625, 24.284902572631836, 26.784080505371094, 36.57115173339844, 19.66192054748535, 25.557693481445312, 25.470203399658203, 36.931739807128906, 15.494051933288574, 15.090009689331055, 19.269227981567383, 33.42930221557617, 13.478736877441406, 16.785858154296875, 26.623550415039062, 41.04734420776367, 15.69299602508545, 19.094579696655273, 30.410654067993164, 10.802501678466797, 10.186247825622559, 11.47294807434082, 18.24969482421875, 33.59756088256836, 10.54472541809082, 15.317480087280273, 27.066364288330078, 8.195663452148438, 7.992672920227051, 10.29607105255127, 20.17367935180664, 36.49372100830078, 14.260297775268555, 12.018312454223633, 21.21265411376953, 6.9260101318359375, 7.353187561035156, 12.843381881713867, 29.326833724975586, 9.025568008422852, 12.231752395629883, 24.953777313232422, 6.64749813079834], 'accuracy': [0.3405994474887848, 0.6839237213134766, 0.6076294183731079, 0.5395095348358154, 0.5040872097015381, 0.6158038377761841, 0.4495912790298462, 0.7193460464477539, 0.6185286045074463, 0.4986376166343689, 0.7220708727836609, 0.5504087209701538, 0.6866484880447388, 0.4904632270336151, 0.7329699993133545, 0.5749318599700928, 0.640326976776123, 0.5095368027687073, 0.7384195923805237, 0.583106279373169, 0.6975476741790771, 0.46049046516418457, 0.7465940117835999, 0.6893733143806458, 0.5068119764328003, 0.7438691854476929, 0.6730245351791382, 0.6376021504402161, 0.7084468603134155, 0.5095368027687073, 0.7465940117835999, 0.6893733143806458, 0.528610348701477, 0.7438691854476929, 0.667574942111969, 0.6621253490447998, 0.7193460464477539, 0.47138965129852295, 0.7438691854476929, 0.7302452325820923, 0.553133487701416, 0.7438691854476929, 0.6811988949775696, 0.7438691854476929, 0.5258855819702148, 0.7465940117835999, 0.7220708727836609, 0.531335175037384, 0.7465940117835999, 0.7193460464477539], 'precision_18': [0.3405994474887848, 0.6839237213134766, 0.6076294183731079, 0.5395095348358154, 0.5040872097015381, 0.6158038377761841, 0.4495912790298462, 0.7193460464477539, 0.6185286045074463, 0.4986376166343689, 0.7220708727836609, 0.5504087209701538, 0.6866484880447388, 0.4904632270336151, 0.7329699993133545, 0.5749318599700928, 0.640326976776123, 0.5095368027687073, 0.7384195923805237, 0.583106279373169, 0.6975476741790771, 0.46049046516418457, 0.7465940117835999, 0.6893733143806458, 0.5068119764328003, 0.7438691854476929, 0.6730245351791382, 0.6376021504402161, 0.7084468603134155, 0.5095368027687073, 0.7465940117835999, 0.6893733143806458, 0.528610348701477, 0.7438691854476929, 0.667574942111969, 0.6621253490447998, 0.7193460464477539, 0.47138965129852295, 0.7438691854476929, 0.7302452325820923, 0.553133487701416, 0.7438691854476929, 0.6811988949775696, 0.7438691854476929, 0.5258855819702148, 0.7465940117835999, 0.7220708727836609, 0.531335175037384, 0.7465940117835999, 0.7193460464477539], 'recall_18': [0.3405994474887848, 0.6839237213134766, 0.6076294183731079, 0.5395095348358154, 0.5040872097015381, 0.6158038377761841, 0.4495912790298462, 0.7193460464477539, 0.6185286045074463, 0.4986376166343689, 0.7220708727836609, 0.5504087209701538, 0.6866484880447388, 0.4904632270336151, 0.7329699993133545, 0.5749318599700928, 0.640326976776123, 0.5095368027687073, 0.7384195923805237, 0.583106279373169, 0.6975476741790771, 0.46049046516418457, 0.7465940117835999, 0.6893733143806458, 0.5068119764328003, 0.7438691854476929, 0.6730245351791382, 0.6376021504402161, 0.7084468603134155, 0.5095368027687073, 0.7465940117835999, 0.6893733143806458, 0.528610348701477, 0.7438691854476929, 0.667574942111969, 0.6621253490447998, 0.7193460464477539, 0.47138965129852295, 0.7438691854476929, 0.7302452325820923, 0.553133487701416, 0.7438691854476929, 0.6811988949775696, 0.7438691854476929, 0.5258855819702148, 0.7465940117835999, 0.7220708727836609, 0.531335175037384, 0.7465940117835999, 0.7193460464477539]}\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fdb22f3f290> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "> Fold 9 - Precison: 0.6658653846153846 - Recall: 0.4230769230769231%\n",
      "Shape of train set: (367, 20)\n",
      "Shape of y: (367, 2)\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 10 ...\n",
      "Shape of validation set: (40, 20)\n",
      "{'loss': [68.82228088378906, 204.46482849121094, 60.10907745361328, 50.05864334106445, 42.61581039428711, 46.84514617919922, 47.483341217041016, 38.24202346801758, 47.94597625732422, 28.82714080810547, 42.355690002441406, 28.64627456665039, 50.24368667602539, 18.323211669921875, 19.071840286254883, 36.30701446533203, 65.61429595947266, 24.681888580322266, 37.507110595703125, 57.763282775878906, 21.839670181274414, 29.060285568237305, 47.34934997558594, 15.881240844726562, 21.200315475463867, 40.48070526123047, 13.663872718811035, 13.508072853088379, 17.9661865234375, 46.843963623046875, 12.77205753326416, 15.508516311645508, 42.352134704589844, 11.802905082702637, 13.390856742858887, 36.25078201293945, 65.39593505859375, 28.256532669067383, 18.304243087768555, 43.46266555786133, 11.937765121459961, 21.128957748413086, 50.523719787597656, 17.465103149414062, 29.905044555664062, 51.28800964355469, 20.258644104003906, 19.694515228271484, 39.46755599975586, 11.450899124145508], 'accuracy': [0.6730245351791382, 0.2861035466194153, 0.6839237213134766, 0.5367847681045532, 0.6594005227088928, 0.4959128201007843, 0.6948229074478149, 0.46321526169776917, 0.6948229074478149, 0.46321526169776917, 0.6948229074478149, 0.4359672963619232, 0.7029972672462463, 0.5204359889030457, 0.6539509296417236, 0.41961851716041565, 0.7193460464477539, 0.6811988949775696, 0.40871936082839966, 0.7138964533805847, 0.6866484880447388, 0.46049046516418457, 0.7166212797164917, 0.6457765698432922, 0.5095368027687073, 0.7138964533805847, 0.5885558724403381, 0.6348773837089539, 0.5068119764328003, 0.7193460464477539, 0.6239781975746155, 0.5449591279029846, 0.7193460464477539, 0.5940054655075073, 0.6866484880447388, 0.43051770329475403, 0.7247956395149231, 0.7111716866493225, 0.5040872097015381, 0.7166212797164917, 0.6839237213134766, 0.4959128201007843, 0.7247956395149231, 0.7084468603134155, 0.4495912790298462, 0.7247956395149231, 0.7138964533805847, 0.5177111625671387, 0.7193460464477539, 0.6975476741790771], 'precision_19': [0.6730245351791382, 0.2861035466194153, 0.6839237213134766, 0.5367847681045532, 0.6594005227088928, 0.4959128201007843, 0.6948229074478149, 0.46321526169776917, 0.6948229074478149, 0.46321526169776917, 0.6948229074478149, 0.4359672963619232, 0.7029972672462463, 0.5204359889030457, 0.6539509296417236, 0.41961851716041565, 0.7193460464477539, 0.6811988949775696, 0.40871936082839966, 0.7138964533805847, 0.6866484880447388, 0.46049046516418457, 0.7166212797164917, 0.6457765698432922, 0.5095368027687073, 0.7138964533805847, 0.5885558724403381, 0.6348773837089539, 0.5068119764328003, 0.7193460464477539, 0.6239781975746155, 0.5449591279029846, 0.7193460464477539, 0.5940054655075073, 0.6866484880447388, 0.43051770329475403, 0.7247956395149231, 0.7111716866493225, 0.5040872097015381, 0.7166212797164917, 0.6839237213134766, 0.4959128201007843, 0.7247956395149231, 0.7084468603134155, 0.4495912790298462, 0.7247956395149231, 0.7138964533805847, 0.5177111625671387, 0.7193460464477539, 0.6975476741790771], 'recall_19': [0.6730245351791382, 0.2861035466194153, 0.6839237213134766, 0.5367847681045532, 0.6594005227088928, 0.4959128201007843, 0.6948229074478149, 0.46321526169776917, 0.6948229074478149, 0.46321526169776917, 0.6948229074478149, 0.4359672963619232, 0.7029972672462463, 0.5204359889030457, 0.6539509296417236, 0.41961851716041565, 0.7193460464477539, 0.6811988949775696, 0.40871936082839966, 0.7138964533805847, 0.6866484880447388, 0.46049046516418457, 0.7166212797164917, 0.6457765698432922, 0.5095368027687073, 0.7138964533805847, 0.5885558724403381, 0.6348773837089539, 0.5068119764328003, 0.7193460464477539, 0.6239781975746155, 0.5449591279029846, 0.7193460464477539, 0.5940054655075073, 0.6866484880447388, 0.43051770329475403, 0.7247956395149231, 0.7111716866493225, 0.5040872097015381, 0.7166212797164917, 0.6839237213134766, 0.4959128201007843, 0.7247956395149231, 0.7084468603134155, 0.4495912790298462, 0.7247956395149231, 0.7138964533805847, 0.5177111625671387, 0.7193460464477539, 0.6975476741790771]}\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fdb2213d3b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Fold 10 - Precison: 0.8356617647058824 - Recall: 0.38235294117647056%\n",
      "------------------------------------------------------------------------\n",
      "> Precison: 0.737202804573171 (+- 0.07317281890617236)\n",
      "> Recall: 0.6732988262473638\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "num_folds = 10\n",
    "# Define per-fold score containers <-- these are new\n",
    "acc_per_fold = []\n",
    "loss_per_fold = []\n",
    "\n",
    "# Define the K-fold Cross Validator\n",
    "kfold = KFold(n_splits=num_folds, shuffle=True)\n",
    "\n",
    "inputs = X\n",
    "targets = y\n",
    "\n",
    "print('# inputs data samples:', inputs.shape[0])\n",
    "print('# targets data samples:', targets.shape[0])\n",
    "\n",
    "# K-fold Cross Validation model evaluation\n",
    "fold_no = 1\n",
    "for train, test in kfold.split(inputs, targets):\n",
    "    \n",
    "#     tk = Tokenizer(num_words=NB_WORDS,\n",
    "#                filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "#                lower=True,\n",
    "#                split=\" \")\n",
    "#     tk.fit_on_texts(inputs.iloc[train])\n",
    "    \n",
    "#     X_train_seq = tk.texts_to_sequences(inputs.iloc[train])\n",
    "#     X_test_seq = tk.texts_to_sequences(inputs.iloc[test])\n",
    "    \n",
    "    \n",
    "    X_train_oh = inputs.iloc[train]\n",
    "    X_test_oh = inputs.iloc[test]\n",
    "    \n",
    "    print('Shape of train set:',X_train_oh.shape)\n",
    "    \n",
    "    \n",
    "    y_train_le = targets.iloc[train]\n",
    "    y_train_oh = to_categorical(y_train_le)\n",
    "    \n",
    "    \n",
    "    y_test_le = targets.iloc[test]\n",
    "    y_test_oh = to_categorical(y_test_le)\n",
    "\n",
    "\n",
    "    \n",
    "    print('Shape of y:',y_train_oh.shape)\n",
    "\n",
    "    # Define the model architecture\n",
    "    base_model = models.Sequential()\n",
    "    base_model.add(layers.Dense(64, activation='relu', input_shape=(20,)))\n",
    "    base_model.add(layers.Dense(64, activation='relu'))\n",
    "    base_model.add(layers.Dense(2, activation='softmax'))\n",
    "\n",
    "    # Compile the model\n",
    "    base_model.compile(optimizer='rmsprop'\n",
    "                  , loss='categorical_crossentropy'\n",
    "                  , metrics=['accuracy',tf.keras.metrics.Precision(),tf.keras.metrics.Recall()])\n",
    "    \n",
    "\n",
    "    # Generate a print\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'Training for fold {fold_no} ...')\n",
    "    print('Shape of validation set:',X_test_oh.shape)\n",
    "    \n",
    "    # Fit data to model\n",
    "    history = base_model.fit(X_train_oh\n",
    "                       ,y_train_oh\n",
    "                       , epochs=50\n",
    "                       , batch_size=BATCH_SIZE\n",
    "                       , verbose=0)\n",
    "    print(history.history)\n",
    "    \n",
    "\n",
    "    # Generate generalization metrics\n",
    "    from sklearn.metrics import classification_report\n",
    "    y_pred = base_model.predict_classes(X_test_oh)\n",
    "    \n",
    "    average_recall = recall_score(y_test_le, y_pred)\n",
    "    average_precision = average_precision_score(y_test_le, y_pred)\n",
    "    print(f'> Fold {fold_no} - Precison: {average_precision} - Recall: {average_recall}%')\n",
    "    \n",
    "    acc_per_fold.append(average_precision)\n",
    "    loss_per_fold.append(average_recall)\n",
    "\n",
    "    # Increase fold number\n",
    "    fold_no = fold_no + 1\n",
    "\n",
    "# == Provide average scores ==\n",
    "print('------------------------------------------------------------------------')\n",
    "\n",
    "print(f'> Precison: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
    "print(f'> Recall: {np.mean(loss_per_fold)}')\n",
    "print('------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier, KerasRegressor\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "\n",
    "base_model = models.Sequential()\n",
    "base_model.add(layers.Dense(128, activation='relu', input_shape=(10020,)))\n",
    "base_model.add(layers.Dense(64, activation='relu'))\n",
    "base_model.add(layers.Dense(2, activation='softmax'))\n",
    "\n",
    "X = result\n",
    "y = y\n",
    "\n",
    "my_model = KerasRegressor(build_fn=basemodel, **sk_params)    \n",
    "my_model.fit(X,y)\n",
    "\n",
    "perm = PermutationImportance(my_model, random_state=1).fit(X,y)\n",
    "eli5.show_weights(perm, feature_names = X.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_metric(history, metric_name):\n",
    "    metric = history.history[metric_name]\n",
    "    val_metric = history.history['val_' + metric_name+'_24']\n",
    "\n",
    "    e = range(1, NB_START_EPOCHS + 1)\n",
    "\n",
    "    plt.plot(e, metric, 'bo', label='Train ' + metric_name)\n",
    "    plt.plot(e, val_metric, 'b', label='Validation ' + metric_name)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.save(\"visual_mlp.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31, 10020)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_oh.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31,)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_le.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31, 10020)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_oh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shape mismatch: objects cannot be broadcast to a single shape",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-117-f54014f284ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Random Forest feature importance via permutation importance w. std. dev.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m plt.bar(range(X.shape[1]), imp_vals[indices],\n\u001b[0;32m---> 18\u001b[0;31m         yerr=std[indices])\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mbar\u001b[0;34m(x, height, width, bottom, align, data, **kwargs)\u001b[0m\n\u001b[1;32m   2455\u001b[0m     return gca().bar(\n\u001b[1;32m   2456\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbottom\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbottom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malign\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malign\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2457\u001b[0;31m         **({\"data\": data} if data is not None else {}), **kwargs)\n\u001b[0m\u001b[1;32m   2458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1808\u001b[0m                         \u001b[0;34m\"the Matplotlib list!)\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlabel_namer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1809\u001b[0m                         RuntimeWarning, stacklevel=2)\n\u001b[0;32m-> 1810\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1812\u001b[0m         inner.__doc__ = _add_data_doc(inner.__doc__,\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mbar\u001b[0;34m(self, x, height, width, bottom, align, **kwargs)\u001b[0m\n\u001b[1;32m   2249\u001b[0m         x, height, width, y, linewidth = np.broadcast_arrays(\n\u001b[1;32m   2250\u001b[0m             \u001b[0;31m# Make args iterable too.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2251\u001b[0;31m             np.atleast_1d(x), height, width, y, linewidth)\n\u001b[0m\u001b[1;32m   2252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2253\u001b[0m         \u001b[0;31m# Now that units have been converted, set the tick locations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mbroadcast_arrays\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/numpy/lib/stride_tricks.py\u001b[0m in \u001b[0;36mbroadcast_arrays\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_m\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubok\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_m\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m     \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_broadcast_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/numpy/lib/stride_tricks.py\u001b[0m in \u001b[0;36m_broadcast_shape\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;31m# use the old-iterator because np.nditer does not handle size 0 arrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[0;31m# consistently\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m     \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m     \u001b[0;31m# unfortunately, it cannot handle 32 or more arguments directly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mpos\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m31\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shape mismatch: objects cannot be broadcast to a single shape"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from mlxtend.evaluate import feature_importance_permutation\n",
    "from sklearn.metrics import f1_score\n",
    "imp_vals, imp_all = feature_importance_permutation(\n",
    "    predict_method= base_model.predict_classes, \n",
    "    X=X_test_oh.values,\n",
    "    y=y_test_le,\n",
    "    metric=f1_score,\n",
    "    num_rounds=10,\n",
    "    seed=1)\n",
    "\n",
    "\n",
    "std = np.std(imp_all, axis=1)\n",
    "indices = np.argsort(imp_vals)[::-1]\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Random Forest feature importance via permutation importance w. std. dev.\")\n",
    "plt.bar(range(X.shape[1]), imp_vals[indices],\n",
    "        yerr=std[indices])\n",
    "plt.xticks(range(X.shape[1]), indices)\n",
    "plt.xlim([-1, X.shape[1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15     0\n",
       "23     0\n",
       "25     0\n",
       "27     0\n",
       "36     1\n",
       "39     0\n",
       "47     1\n",
       "51     1\n",
       "79     1\n",
       "107    1\n",
       "128    1\n",
       "130    1\n",
       "154    0\n",
       "165    1\n",
       "178    1\n",
       "183    1\n",
       "190    1\n",
       "195    0\n",
       "210    1\n",
       "214    0\n",
       "225    1\n",
       "235    1\n",
       "252    0\n",
       "271    0\n",
       "273    1\n",
       "293    0\n",
       "329    1\n",
       "337    0\n",
       "370    1\n",
       "384    1\n",
       "392    1\n",
       "Name: attitude, dtype: int64"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_le"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Train data samples: 509\n",
      "# Test data samples: 57\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df.video_description, df.Relevancy, test_size=0.1, random_state=37)\n",
    "print('# Train data samples:', X_train.shape[0])\n",
    "print('# Test data samples:', X_test.shape[0])\n",
    "assert X_train.shape[0] == y_train.shape[0]\n",
    "assert X_test.shape[0] == y_test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting words to numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the text as input for a model, we first need to convert the tweet's words into tokens, which simply means converting the words to integers that refer to an index in a dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will only keep the most frequent words in the train set.\n",
    "\n",
    "We clean up the text by applying filters and putting the words to lowercase. Words are separated by spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted tokenizer on 509 documents\n",
      "10000 words in dictionary\n",
      "Top 5 most common words are: [('coronavirus', 624), ('news', 521), ('us', 271), ('follow', 260), ('subscribe', 255), ('channel', 255), ('virus', 239), ('watch', 230), ('5g', 215), ('dr', 215), ('covid', 214), ('twitter', 214), ('video', 211), ('not', 206), ('like', 205), ('facebook', 204), ('19', 201), ('this', 184), ('people', 183), ('playlist', 181)]\n"
     ]
    }
   ],
   "source": [
    "tk = Tokenizer(num_words=NB_WORDS,\n",
    "               filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "               lower=True,\n",
    "               split=\" \")\n",
    "tk.fit_on_texts(X_train)\n",
    "\n",
    "print('Fitted tokenizer on {} documents'.format(tk.document_count))\n",
    "print('{} words in dictionary'.format(tk.num_words))\n",
    "print('Top 5 most common words are:', collections.Counter(tk.word_counts).most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having created the dictionary we can convert the text to a list of integer indexes. This is done with the text_to_sequences method of the Tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"With theories circulating online 5G technology blame COVID-19 pandemic, experts present facts behind claims. Subscribe 7NEWS latest video Â¬Âª  Connect 7NEWS online Visit Â¬Âª  Facebook Â¬Âª  Twitter Â¬Âª  Instagram Â¬Âª  #BreakingNews #coronavirus #COVID19 #7NEWS\" is converted into [2045, 2546, 2547, 3245, 1761, 2548, 1039, 4683, 33, 124, 582, 83, 111, 818, 1, 220, 221, 115, 3246, 32, 120, 1762, 236, 486, 765, 1, 263, 660]\n"
     ]
    }
   ],
   "source": [
    "X_train_seq = tk.texts_to_sequences(X_train)\n",
    "X_test_seq = tk.texts_to_sequences(X_test)\n",
    "\n",
    "print('\"{}\" is converted into {}'.format(X_train[0], X_train_seq[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These integers should now be converted into a one-hot encoded features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_seq(seqs, nb_features = NB_WORDS):\n",
    "    ohs = np.zeros((len(seqs), nb_features))\n",
    "    for i, s in enumerate(seqs):\n",
    "        ohs[i, s] = 1.\n",
    "    return ohs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting the target classes to numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"1\" is converted into 1\n",
      "\"1\" is converted into [0. 1.]\n"
     ]
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "y_train_le = le.fit_transform(y_train)\n",
    "y_test_le = le.transform(y_test)\n",
    "y_train_oh = to_categorical(y_train_le)\n",
    "y_test_oh = to_categorical(y_test_le)\n",
    "\n",
    "print('\"{}\" is converted into {}'.format(y_train[0], y_train_le[0]))\n",
    "print('\"{}\" is converted into {}'.format(y_train_le[0], y_train_oh[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting of a validation set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of validation set: (51, 10000)\n"
     ]
    }
   ],
   "source": [
    "X_train_rest, X_valid, y_train_rest, y_valid = train_test_split(X_train_oh, y_train_oh, test_size=0.1, random_state=37)\n",
    "\n",
    "assert X_valid.shape[0] == y_valid.shape[0]\n",
    "assert X_train_rest.shape[0] == y_train_rest.shape[0]\n",
    "\n",
    "print('Shape of validation set:',X_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Implementation\n",
    "## Baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with a model with 2 densely connected layers of 64 hidden elements. The input_shape for the first layer is equal to the number of words we allowed in the dictionary and for which we created one-hot-encoded features.\n",
    "\n",
    " The softmax activation function makes sure the three probabilities sum up to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 64)                640064    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 644,354\n",
      "Trainable params: 644,354\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "base_model = models.Sequential()\n",
    "base_model.add(layers.Dense(64, activation='relu', input_shape=(NB_WORDS,)))\n",
    "base_model.add(layers.Dense(64, activation='relu'))\n",
    "base_model.add(layers.Dense(2, activation='softmax'))\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because this project is a multi-class, single-label prediction, we use categorical_crossentropy as the loss function and softmax as the final activation function. We fit the model on the remaining train data and validate on the validation set. We run for a predetermined number of epochs and will see when the model starts to overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_model(model):\n",
    "    model.compile(optimizer='rmsprop'\n",
    "                  , loss='categorical_crossentropy'\n",
    "                  , metrics=['accuracy',tf.keras.metrics.Precision(),tf.keras.metrics.Recall()])\n",
    "    \n",
    "    history = model.fit(X_train_rest\n",
    "                       , y_train_rest\n",
    "                       , epochs=NB_START_EPOCHS\n",
    "                       , batch_size=BATCH_SIZE\n",
    "                       , validation_data=(X_valid, y_valid)\n",
    "                       , verbose=0)\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.7070090174674988,\n",
       "  0.5174614787101746,\n",
       "  0.3788684904575348,\n",
       "  0.3041400909423828,\n",
       "  0.25723156332969666,\n",
       "  0.22602522373199463,\n",
       "  0.2032897025346756,\n",
       "  0.1780741959810257,\n",
       "  0.1627146452665329,\n",
       "  0.1489810198545456,\n",
       "  0.13913194835186005,\n",
       "  0.12844568490982056,\n",
       "  0.11971041560173035,\n",
       "  0.11082577705383301,\n",
       "  0.10460763424634933,\n",
       "  0.09958921372890472,\n",
       "  0.09780213981866837,\n",
       "  0.09410517662763596,\n",
       "  0.089164137840271,\n",
       "  0.08444852381944656],\n",
       " 'accuracy': [0.3995633125305176,\n",
       "  0.8253275156021118,\n",
       "  0.8668122291564941,\n",
       "  0.9017467498779297,\n",
       "  0.9366812109947205,\n",
       "  0.9344978332519531,\n",
       "  0.9410480260848999,\n",
       "  0.9454148411750793,\n",
       "  0.9563318490982056,\n",
       "  0.9541484713554382,\n",
       "  0.9563318490982056,\n",
       "  0.9563318490982056,\n",
       "  0.960698664188385,\n",
       "  0.960698664188385,\n",
       "  0.960698664188385,\n",
       "  0.960698664188385,\n",
       "  0.9628821015357971,\n",
       "  0.9628821015357971,\n",
       "  0.9628821015357971,\n",
       "  0.9628821015357971],\n",
       " 'precision': [0.39427313208580017,\n",
       "  0.8253275156021118,\n",
       "  0.8668122291564941,\n",
       "  0.9017467498779297,\n",
       "  0.9366812109947205,\n",
       "  0.9344978332519531,\n",
       "  0.9410480260848999,\n",
       "  0.9454148411750793,\n",
       "  0.9563318490982056,\n",
       "  0.9541484713554382,\n",
       "  0.9563318490982056,\n",
       "  0.9563318490982056,\n",
       "  0.960698664188385,\n",
       "  0.960698664188385,\n",
       "  0.960698664188385,\n",
       "  0.960698664188385,\n",
       "  0.9628821015357971,\n",
       "  0.9628821015357971,\n",
       "  0.9628821015357971,\n",
       "  0.9628821015357971],\n",
       " 'recall': [0.3908296823501587,\n",
       "  0.8253275156021118,\n",
       "  0.8668122291564941,\n",
       "  0.9017467498779297,\n",
       "  0.9366812109947205,\n",
       "  0.9344978332519531,\n",
       "  0.9410480260848999,\n",
       "  0.9454148411750793,\n",
       "  0.9563318490982056,\n",
       "  0.9541484713554382,\n",
       "  0.9563318490982056,\n",
       "  0.9563318490982056,\n",
       "  0.960698664188385,\n",
       "  0.960698664188385,\n",
       "  0.960698664188385,\n",
       "  0.960698664188385,\n",
       "  0.9628821015357971,\n",
       "  0.9628821015357971,\n",
       "  0.9628821015357971,\n",
       "  0.9628821015357971],\n",
       " 'val_loss': [0.5335533022880554,\n",
       "  0.43902894854545593,\n",
       "  0.3966190814971924,\n",
       "  0.36304351687431335,\n",
       "  0.36193162202835083,\n",
       "  0.3229401409626007,\n",
       "  0.33019882440567017,\n",
       "  0.3119520843029022,\n",
       "  0.3339370787143707,\n",
       "  0.302752822637558,\n",
       "  0.33394140005111694,\n",
       "  0.2985526919364929,\n",
       "  0.32714682817459106,\n",
       "  0.30503079295158386,\n",
       "  0.3339540362358093,\n",
       "  0.29546359181404114,\n",
       "  0.3473667800426483,\n",
       "  0.3011917769908905,\n",
       "  0.33582183718681335,\n",
       "  0.30534660816192627],\n",
       " 'val_accuracy': [0.9019607901573181,\n",
       "  0.9019607901573181,\n",
       "  0.9019607901573181,\n",
       "  0.9019607901573181,\n",
       "  0.9019607901573181,\n",
       "  0.8823529481887817,\n",
       "  0.9019607901573181,\n",
       "  0.9019607901573181,\n",
       "  0.9019607901573181,\n",
       "  0.9019607901573181,\n",
       "  0.9019607901573181,\n",
       "  0.9019607901573181,\n",
       "  0.8823529481887817,\n",
       "  0.9019607901573181,\n",
       "  0.9019607901573181,\n",
       "  0.9019607901573181,\n",
       "  0.9019607901573181,\n",
       "  0.9019607901573181,\n",
       "  0.9019607901573181,\n",
       "  0.9019607901573181],\n",
       " 'val_precision': [0.9019607901573181,\n",
       "  0.9019607901573181,\n",
       "  0.9019607901573181,\n",
       "  0.9019607901573181,\n",
       "  0.9019607901573181,\n",
       "  0.8823529481887817,\n",
       "  0.9019607901573181,\n",
       "  0.9019607901573181,\n",
       "  0.9019607901573181,\n",
       "  0.9019607901573181,\n",
       "  0.9019607901573181,\n",
       "  0.9019607901573181,\n",
       "  0.8823529481887817,\n",
       "  0.9019607901573181,\n",
       "  0.9019607901573181,\n",
       "  0.9019607901573181,\n",
       "  0.9019607901573181,\n",
       "  0.9019607901573181,\n",
       "  0.9019607901573181,\n",
       "  0.9019607901573181],\n",
       " 'val_recall': [0.9019607901573181,\n",
       "  0.9019607901573181,\n",
       "  0.9019607901573181,\n",
       "  0.9019607901573181,\n",
       "  0.9019607901573181,\n",
       "  0.8823529481887817,\n",
       "  0.9019607901573181,\n",
       "  0.9019607901573181,\n",
       "  0.9019607901573181,\n",
       "  0.9019607901573181,\n",
       "  0.9019607901573181,\n",
       "  0.9019607901573181,\n",
       "  0.8823529481887817,\n",
       "  0.9019607901573181,\n",
       "  0.9019607901573181,\n",
       "  0.9019607901573181,\n",
       "  0.9019607901573181,\n",
       "  0.9019607901573181,\n",
       "  0.9019607901573181,\n",
       "  0.9019607901573181]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_history = first_model(base_model)\n",
    "base_history.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the model performance, we will look at the training and validation loss and accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Validation\n",
    "\n",
    "lets check the model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, epoch_stop):\n",
    "    model.fit(X_train_oh\n",
    "              , y_train_oh\n",
    "              , epochs=epoch_stop\n",
    "              , batch_size=BATCH_SIZE\n",
    "              , verbose=0)\n",
    "    results = model.evaluate(X_test_oh, y_test_oh)\n",
    "    \n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 1ms/step - loss: 0.8903 - accuracy: 0.8947 - precision_1: 0.8947 - recall_1: 0.8947\n",
      "[0.8902625441551208, 0.8947368264198303, 0.8947368264198303, 0.8947368264198303]\n",
      "Test accuracy of baseline model: 89.47%\n"
     ]
    }
   ],
   "source": [
    "test_results = test_model(base_model,30)\n",
    "print(test_results)\n",
    "print('Test accuracy of baseline model: {0:.2f}%'.format(test_results[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 0 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0 0 1 1\n",
      " 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "y_score = base_model.predict_classes(X_test_oh)\n",
    "print(y_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.75      0.75        12\n",
      "           1       0.93      0.93      0.93        45\n",
      "\n",
      "    accuracy                           0.89        57\n",
      "   macro avg       0.84      0.84      0.84        57\n",
      "weighted avg       0.89      0.89      0.89        57\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = base_model.predict_classes(X_test_oh)\n",
    "print(y_test.ndim)\n",
    "print(classification_report(y_test_le, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9237426900584795\n",
      "0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "\n",
    "average_precision = average_precision_score(y_test_le, y_pred)\n",
    "print(average_precision)\n",
    "average_recall = recall_score(y_test_le, y_pred)\n",
    "print(average_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_arr = np.column_stack((y_test_le, y_pred)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1],\n",
       "       [1, 1],\n",
       "       [1, 1],\n",
       "       [1, 1],\n",
       "       [1, 1],\n",
       "       [0, 0],\n",
       "       [1, 1],\n",
       "       [1, 1],\n",
       "       [1, 1],\n",
       "       [1, 1],\n",
       "       [1, 1],\n",
       "       [1, 1],\n",
       "       [1, 1],\n",
       "       [1, 0],\n",
       "       [0, 0],\n",
       "       [1, 1],\n",
       "       [1, 1],\n",
       "       [1, 1],\n",
       "       [1, 0],\n",
       "       [1, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [1, 1],\n",
       "       [1, 1],\n",
       "       [1, 1],\n",
       "       [1, 1],\n",
       "       [1, 1],\n",
       "       [1, 0],\n",
       "       [1, 1],\n",
       "       [1, 1],\n",
       "       [1, 1],\n",
       "       [0, 0],\n",
       "       [1, 1],\n",
       "       [0, 0],\n",
       "       [0, 0],\n",
       "       [1, 1],\n",
       "       [1, 1],\n",
       "       [0, 1],\n",
       "       [1, 1],\n",
       "       [0, 0],\n",
       "       [1, 1],\n",
       "       [1, 1],\n",
       "       [0, 0],\n",
       "       [1, 1],\n",
       "       [1, 1],\n",
       "       [1, 1],\n",
       "       [1, 1],\n",
       "       [1, 1],\n",
       "       [1, 1],\n",
       "       [1, 1],\n",
       "       [1, 1],\n",
       "       [1, 1],\n",
       "       [1, 1],\n",
       "       [1, 1],\n",
       "       [1, 1],\n",
       "       [0, 0],\n",
       "       [0, 0]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
