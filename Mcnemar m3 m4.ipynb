{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set-up of the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/lynette/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Basic packages\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import re\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "# Packages for data preparation\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Packages for modeling\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import regularizers\n",
    "\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import recall_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "big = pd.read_excel(\"relevant_youtube_dataset.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "small = pd.read_csv('handlabel_feature.csv')\n",
    "df = pd.merge(left=small, right=big, left_on='video_id', right_on='video_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set some parameters that will be used throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(subset=['video_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>video_id</th>\n",
       "      <th>channel_title_x</th>\n",
       "      <th>channel_id_x</th>\n",
       "      <th>video_publish_date_x</th>\n",
       "      <th>video_title_x</th>\n",
       "      <th>video_description_x</th>\n",
       "      <th>video_category_x</th>\n",
       "      <th>video_view_count_x</th>\n",
       "      <th>video_comment_count_x</th>\n",
       "      <th>...</th>\n",
       "      <th>fear</th>\n",
       "      <th>joy</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>trust</th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "      <th>emotion</th>\n",
       "      <th>AVAILABILITY</th>\n",
       "      <th>Censored</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NcSUF8erpfU</td>\n",
       "      <td>The Daily Show with Trevor Noah</td>\n",
       "      <td>UCwWhs_6x42TyRM4Wstoq8HA</td>\n",
       "      <td>4/17/20 2:03</td>\n",
       "      <td>What Are the Craziest Coronavirus Conspiracy T...</td>\n",
       "      <td>From bat cuisine to 5G technology weakening im...</td>\n",
       "      <td>23</td>\n",
       "      <td>2377430</td>\n",
       "      <td>8481.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012611</td>\n",
       "      <td>0.011128</td>\n",
       "      <td>0.014095</td>\n",
       "      <td>0.005935</td>\n",
       "      <td>0.014837</td>\n",
       "      <td>0.023739</td>\n",
       "      <td>0.026706</td>\n",
       "      <td>0.137240</td>\n",
       "      <td>VideoAvailable</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>V0yb0_a-WNc</td>\n",
       "      <td>penguinz0</td>\n",
       "      <td>UCq6VFHwMzcMXbuKyG7SQYIg</td>\n",
       "      <td>4/3/20 23:30</td>\n",
       "      <td>5G Coronavirus Conspiracy Is Insane</td>\n",
       "      <td>This is the greatest nonsense of All Time</td>\n",
       "      <td>24</td>\n",
       "      <td>1310678</td>\n",
       "      <td>18635.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011355</td>\n",
       "      <td>0.003524</td>\n",
       "      <td>0.009397</td>\n",
       "      <td>0.004307</td>\n",
       "      <td>0.009397</td>\n",
       "      <td>0.022318</td>\n",
       "      <td>0.015270</td>\n",
       "      <td>0.097494</td>\n",
       "      <td>VideoAvailable</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>BkbztWS4-9I</td>\n",
       "      <td>BBC Newsnight</td>\n",
       "      <td>UC6o-wWU-v2ClFMwougmK7dA</td>\n",
       "      <td>3/5/20 14:32</td>\n",
       "      <td>Coronavirus: The conspiracy theories spreading...</td>\n",
       "      <td>From a secret plan to stop Brexit, to a virus ...</td>\n",
       "      <td>25</td>\n",
       "      <td>1046631</td>\n",
       "      <td>7652.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>0.008589</td>\n",
       "      <td>0.006748</td>\n",
       "      <td>0.006135</td>\n",
       "      <td>0.022086</td>\n",
       "      <td>0.020859</td>\n",
       "      <td>0.030675</td>\n",
       "      <td>0.142331</td>\n",
       "      <td>VideoAvailable</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>7OVT3N5_4to</td>\n",
       "      <td>TechMagnet</td>\n",
       "      <td>UCtT2VnurQKOAA0I1EKKHSPA</td>\n",
       "      <td>3/16/20 17:35</td>\n",
       "      <td>Is 5G the CAUSE of CORONAVIRUS? (COVID-19)</td>\n",
       "      <td>Does 5G cause Coronavirus ? Is the new COVID-1...</td>\n",
       "      <td>28</td>\n",
       "      <td>912978</td>\n",
       "      <td>6823.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004447</td>\n",
       "      <td>0.004708</td>\n",
       "      <td>0.004708</td>\n",
       "      <td>0.003924</td>\n",
       "      <td>0.012294</td>\n",
       "      <td>0.007063</td>\n",
       "      <td>0.017526</td>\n",
       "      <td>0.066963</td>\n",
       "      <td>VideoAvailable</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>8ocWUAwQMhY</td>\n",
       "      <td>LogicBeforeAuthority</td>\n",
       "      <td>UCTOLWfTDKvFQHSCQSIssVTQ</td>\n",
       "      <td>3/16/20 20:33</td>\n",
       "      <td>CONFIRMED!  5G Forced Installation In Schools ...</td>\n",
       "      <td>Guys, you need to get involved and do anything...</td>\n",
       "      <td>25</td>\n",
       "      <td>586496</td>\n",
       "      <td>3037.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009218</td>\n",
       "      <td>0.004768</td>\n",
       "      <td>0.006675</td>\n",
       "      <td>0.002543</td>\n",
       "      <td>0.013032</td>\n",
       "      <td>0.014304</td>\n",
       "      <td>0.018754</td>\n",
       "      <td>0.086459</td>\n",
       "      <td>VideoAvailable</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>402</td>\n",
       "      <td>bHkcnuJhDp8</td>\n",
       "      <td>The Sun</td>\n",
       "      <td>UCIzXayRP7-P0ANpq-nD-h5g</td>\n",
       "      <td>5/2/20 17:38</td>\n",
       "      <td>5G coronavirus conspiracy theorists stage anti...</td>\n",
       "      <td>5G coronavirus conspiracy theorists today stag...</td>\n",
       "      <td>25</td>\n",
       "      <td>45047</td>\n",
       "      <td>1047.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.046875</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.273438</td>\n",
       "      <td>VideoAvailable</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>403</td>\n",
       "      <td>cszlsGiD1-E</td>\n",
       "      <td>Corbett Report Extras</td>\n",
       "      <td>UCM6EbmEFrTbrQ_31bUx5h3w</td>\n",
       "      <td>3/6/20 14:24</td>\n",
       "      <td>Fact Checking the 5G/Coronavirus Hypothesis</td>\n",
       "      <td>SHOW NOTES AND MP3 AUDIO: https://www.corbettr...</td>\n",
       "      <td>28</td>\n",
       "      <td>45018</td>\n",
       "      <td>847.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007153</td>\n",
       "      <td>0.004390</td>\n",
       "      <td>0.003577</td>\n",
       "      <td>0.002926</td>\n",
       "      <td>0.008942</td>\n",
       "      <td>0.009267</td>\n",
       "      <td>0.015445</td>\n",
       "      <td>0.065843</td>\n",
       "      <td>VideoAvailable</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>404</td>\n",
       "      <td>edXRrJPdoHA</td>\n",
       "      <td>LMG Clips</td>\n",
       "      <td>UCFLFc8Lpbwt4jPtY1_Ai5yA</td>\n",
       "      <td>4/15/20 17:06</td>\n",
       "      <td>5G Doesn't Cause COVID19 That's DUMB!</td>\n",
       "      <td>Watch the full WAN Show: https://youtu.be/hXp3...</td>\n",
       "      <td>28</td>\n",
       "      <td>44337</td>\n",
       "      <td>822.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008436</td>\n",
       "      <td>0.003067</td>\n",
       "      <td>0.006902</td>\n",
       "      <td>0.003067</td>\n",
       "      <td>0.011503</td>\n",
       "      <td>0.016871</td>\n",
       "      <td>0.022239</td>\n",
       "      <td>0.089724</td>\n",
       "      <td>VideoAvailable</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>405</td>\n",
       "      <td>fS4OiNzdPAw</td>\n",
       "      <td>South China Morning Post</td>\n",
       "      <td>UC4SUWizzKc1tptprBkWjX2Q</td>\n",
       "      <td>4/28/20 7:18</td>\n",
       "      <td>How Wuhan, the 5G conspiracy, mask diplomacy a...</td>\n",
       "      <td>SCMP journalists Mimi Lau and Kinling Lo prese...</td>\n",
       "      <td>25</td>\n",
       "      <td>44138</td>\n",
       "      <td>164.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008103</td>\n",
       "      <td>0.003638</td>\n",
       "      <td>0.004961</td>\n",
       "      <td>0.003473</td>\n",
       "      <td>0.008269</td>\n",
       "      <td>0.012899</td>\n",
       "      <td>0.017529</td>\n",
       "      <td>0.073921</td>\n",
       "      <td>VideoAvailable</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>406</td>\n",
       "      <td>jMX8Gtl8_cw</td>\n",
       "      <td>paul chowdhry</td>\n",
       "      <td>UC7zrMSbsWkxFCBrsH7Va1RQ</td>\n",
       "      <td>4/8/20 20:23</td>\n",
       "      <td>5G Conspiracy Theory</td>\n",
       "      <td>It's because of the 5G innit</td>\n",
       "      <td>23</td>\n",
       "      <td>43088</td>\n",
       "      <td>367.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018433</td>\n",
       "      <td>0.009217</td>\n",
       "      <td>0.006912</td>\n",
       "      <td>0.006912</td>\n",
       "      <td>0.020737</td>\n",
       "      <td>0.018433</td>\n",
       "      <td>0.027650</td>\n",
       "      <td>0.135945</td>\n",
       "      <td>VideoAvailable</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>293 rows × 92 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0     video_id                  channel_title_x  \\\n",
       "0             0  NcSUF8erpfU  The Daily Show with Trevor Noah   \n",
       "1             1  V0yb0_a-WNc                        penguinz0   \n",
       "2             2  BkbztWS4-9I                    BBC Newsnight   \n",
       "3             4  7OVT3N5_4to                       TechMagnet   \n",
       "4             5  8ocWUAwQMhY             LogicBeforeAuthority   \n",
       "..          ...          ...                              ...   \n",
       "308         402  bHkcnuJhDp8                          The Sun   \n",
       "309         403  cszlsGiD1-E            Corbett Report Extras   \n",
       "310         404  edXRrJPdoHA                        LMG Clips   \n",
       "311         405  fS4OiNzdPAw         South China Morning Post   \n",
       "314         406  jMX8Gtl8_cw                    paul chowdhry   \n",
       "\n",
       "                 channel_id_x video_publish_date_x  \\\n",
       "0    UCwWhs_6x42TyRM4Wstoq8HA         4/17/20 2:03   \n",
       "1    UCq6VFHwMzcMXbuKyG7SQYIg         4/3/20 23:30   \n",
       "2    UC6o-wWU-v2ClFMwougmK7dA         3/5/20 14:32   \n",
       "3    UCtT2VnurQKOAA0I1EKKHSPA        3/16/20 17:35   \n",
       "4    UCTOLWfTDKvFQHSCQSIssVTQ        3/16/20 20:33   \n",
       "..                        ...                  ...   \n",
       "308  UCIzXayRP7-P0ANpq-nD-h5g         5/2/20 17:38   \n",
       "309  UCM6EbmEFrTbrQ_31bUx5h3w         3/6/20 14:24   \n",
       "310  UCFLFc8Lpbwt4jPtY1_Ai5yA        4/15/20 17:06   \n",
       "311  UC4SUWizzKc1tptprBkWjX2Q         4/28/20 7:18   \n",
       "314  UC7zrMSbsWkxFCBrsH7Va1RQ         4/8/20 20:23   \n",
       "\n",
       "                                         video_title_x  \\\n",
       "0    What Are the Craziest Coronavirus Conspiracy T...   \n",
       "1                  5G Coronavirus Conspiracy Is Insane   \n",
       "2    Coronavirus: The conspiracy theories spreading...   \n",
       "3           Is 5G the CAUSE of CORONAVIRUS? (COVID-19)   \n",
       "4    CONFIRMED!  5G Forced Installation In Schools ...   \n",
       "..                                                 ...   \n",
       "308  5G coronavirus conspiracy theorists stage anti...   \n",
       "309        Fact Checking the 5G/Coronavirus Hypothesis   \n",
       "310              5G Doesn't Cause COVID19 That's DUMB!   \n",
       "311  How Wuhan, the 5G conspiracy, mask diplomacy a...   \n",
       "314                               5G Conspiracy Theory   \n",
       "\n",
       "                                   video_description_x  video_category_x  \\\n",
       "0    From bat cuisine to 5G technology weakening im...                23   \n",
       "1            This is the greatest nonsense of All Time                24   \n",
       "2    From a secret plan to stop Brexit, to a virus ...                25   \n",
       "3    Does 5G cause Coronavirus ? Is the new COVID-1...                28   \n",
       "4    Guys, you need to get involved and do anything...                25   \n",
       "..                                                 ...               ...   \n",
       "308  5G coronavirus conspiracy theorists today stag...                25   \n",
       "309  SHOW NOTES AND MP3 AUDIO: https://www.corbettr...                28   \n",
       "310  Watch the full WAN Show: https://youtu.be/hXp3...                28   \n",
       "311  SCMP journalists Mimi Lau and Kinling Lo prese...                25   \n",
       "314                       It's because of the 5G innit                23   \n",
       "\n",
       "     video_view_count_x  video_comment_count_x  ...      fear       joy  \\\n",
       "0               2377430                 8481.0  ...  0.012611  0.011128   \n",
       "1               1310678                18635.0  ...  0.011355  0.003524   \n",
       "2               1046631                 7652.0  ...  0.014724  0.008589   \n",
       "3                912978                 6823.0  ...  0.004447  0.004708   \n",
       "4                586496                 3037.0  ...  0.009218  0.004768   \n",
       "..                  ...                    ...  ...       ...       ...   \n",
       "308               45047                 1047.0  ...  0.046875  0.007812   \n",
       "309               45018                  847.0  ...  0.007153  0.004390   \n",
       "310               44337                  822.0  ...  0.008436  0.003067   \n",
       "311               44138                  164.0  ...  0.008103  0.003638   \n",
       "314               43088                  367.0  ...  0.018433  0.009217   \n",
       "\n",
       "      sadness  surprise     trust  negative  positive   emotion  \\\n",
       "0    0.014095  0.005935  0.014837  0.023739  0.026706  0.137240   \n",
       "1    0.009397  0.004307  0.009397  0.022318  0.015270  0.097494   \n",
       "2    0.006748  0.006135  0.022086  0.020859  0.030675  0.142331   \n",
       "3    0.004708  0.003924  0.012294  0.007063  0.017526  0.066963   \n",
       "4    0.006675  0.002543  0.013032  0.014304  0.018754  0.086459   \n",
       "..        ...       ...       ...       ...       ...       ...   \n",
       "308  0.039062  0.000000  0.031250  0.062500  0.031250  0.273438   \n",
       "309  0.003577  0.002926  0.008942  0.009267  0.015445  0.065843   \n",
       "310  0.006902  0.003067  0.011503  0.016871  0.022239  0.089724   \n",
       "311  0.004961  0.003473  0.008269  0.012899  0.017529  0.073921   \n",
       "314  0.006912  0.006912  0.020737  0.018433  0.027650  0.135945   \n",
       "\n",
       "       AVAILABILITY Censored  \n",
       "0    VideoAvailable        0  \n",
       "1    VideoAvailable        0  \n",
       "2    VideoAvailable        0  \n",
       "3    VideoAvailable        0  \n",
       "4    VideoAvailable        0  \n",
       "..              ...      ...  \n",
       "308  VideoAvailable        0  \n",
       "309  VideoAvailable        0  \n",
       "310  VideoAvailable        0  \n",
       "311  VideoAvailable        0  \n",
       "314  VideoAvailable        0  \n",
       "\n",
       "[293 rows x 92 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read in the csv with the tweets data and perform a random shuffle. It's a good practice to shuffle the data before splitting between a train and test set. We'll only keep the video decription column as input and the Relvancy column as the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'video_id', 'channel_title_x', 'channel_id_x',\n",
       "       'video_publish_date_x', 'video_title_x', 'video_description_x',\n",
       "       'video_category_x', 'video_view_count_x', 'video_comment_count_x',\n",
       "       'video_like_count_x', 'video_dislike_count_x', 'video_thumbnail_x',\n",
       "       'video_tags_x', 'collection_date_x', 'science.topic_x', 'Relevancy_x',\n",
       "       'attitude', 'Text/video', 'search.term_x', 'cld2_x', 'transcript_x',\n",
       "       'transcript_nchar_x', 'videoid', 'conspiracy', 'var_r', 'var_g',\n",
       "       'var_b', 'var_h', 'var_s', 'var_v', 'var_bright', 'var_bright_sd',\n",
       "       'var_contrast', 'var_colorful', 'median_r', 'median_g', 'median_b',\n",
       "       'median_h', 'median_s', 'median_v', 'median_bright', 'median_bright_sd',\n",
       "       'median_contrast', 'median_colorful', 'r_mean', 'g_mean', 'b_mean',\n",
       "       'h_mean', 's_mean', 'v_mean', 'bright_mean', 'lightning_mean',\n",
       "       'contrast_mean', 'colorful_mean', 'color_lag', 'channel_title_y',\n",
       "       'channel_id_y', 'video_publish_date_y', 'video_title_y',\n",
       "       'video_description_y', 'video_category_y', 'video_view_count_y',\n",
       "       'video_comment_count_y', 'video_like_count_y', 'video_dislike_count_y',\n",
       "       'video_thumbnail_y', 'video_tags_y', 'collection_date_y',\n",
       "       'science.topic_y', 'Relevancy_y', 'Attitude', 'search.term_y', 'cld2_y',\n",
       "       'transcript_y', 'tran_avail', 'transcript_nchar_y',\n",
       "       'transcript_nchar.1', 'clicks', 'anger', 'anticipation', 'disgust',\n",
       "       'fear', 'joy', 'sadness', 'surprise', 'trust', 'negative', 'positive',\n",
       "       'emotion', 'AVAILABILITY', 'Censored'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df = pd.read_csv('handlabel_feature.csv')\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['var_r',\n",
       " 'var_g',\n",
       " 'var_b',\n",
       " 'var_h',\n",
       " 'var_s',\n",
       " 'var_v',\n",
       " 'var_bright',\n",
       " 'var_bright_sd',\n",
       " 'var_contrast',\n",
       " 'var_colorful',\n",
       " 'median_r',\n",
       " 'median_g',\n",
       " 'median_b',\n",
       " 'median_h',\n",
       " 'median_s',\n",
       " 'median_v',\n",
       " 'median_bright',\n",
       " 'median_bright_sd',\n",
       " 'median_contrast',\n",
       " 'median_colorful']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns.tolist()[25:45]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>anger</th>\n",
       "      <th>anticipation</th>\n",
       "      <th>disgust</th>\n",
       "      <th>fear</th>\n",
       "      <th>joy</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>trust</th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.008902</td>\n",
       "      <td>0.011869</td>\n",
       "      <td>0.007418</td>\n",
       "      <td>0.012611</td>\n",
       "      <td>0.011128</td>\n",
       "      <td>0.014095</td>\n",
       "      <td>0.005935</td>\n",
       "      <td>0.014837</td>\n",
       "      <td>0.023739</td>\n",
       "      <td>0.026706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.007831</td>\n",
       "      <td>0.007439</td>\n",
       "      <td>0.006656</td>\n",
       "      <td>0.011355</td>\n",
       "      <td>0.003524</td>\n",
       "      <td>0.009397</td>\n",
       "      <td>0.004307</td>\n",
       "      <td>0.009397</td>\n",
       "      <td>0.022318</td>\n",
       "      <td>0.015270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.011656</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>0.006135</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>0.008589</td>\n",
       "      <td>0.006748</td>\n",
       "      <td>0.006135</td>\n",
       "      <td>0.022086</td>\n",
       "      <td>0.020859</td>\n",
       "      <td>0.030675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.002616</td>\n",
       "      <td>0.007586</td>\n",
       "      <td>0.002093</td>\n",
       "      <td>0.004447</td>\n",
       "      <td>0.004708</td>\n",
       "      <td>0.004708</td>\n",
       "      <td>0.003924</td>\n",
       "      <td>0.012294</td>\n",
       "      <td>0.007063</td>\n",
       "      <td>0.017526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.005722</td>\n",
       "      <td>0.008900</td>\n",
       "      <td>0.002543</td>\n",
       "      <td>0.009218</td>\n",
       "      <td>0.004768</td>\n",
       "      <td>0.006675</td>\n",
       "      <td>0.002543</td>\n",
       "      <td>0.013032</td>\n",
       "      <td>0.014304</td>\n",
       "      <td>0.018754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.046875</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.031250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>0.004877</td>\n",
       "      <td>0.006828</td>\n",
       "      <td>0.002439</td>\n",
       "      <td>0.007153</td>\n",
       "      <td>0.004390</td>\n",
       "      <td>0.003577</td>\n",
       "      <td>0.002926</td>\n",
       "      <td>0.008942</td>\n",
       "      <td>0.009267</td>\n",
       "      <td>0.015445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>0.006135</td>\n",
       "      <td>0.007669</td>\n",
       "      <td>0.003834</td>\n",
       "      <td>0.008436</td>\n",
       "      <td>0.003067</td>\n",
       "      <td>0.006902</td>\n",
       "      <td>0.003067</td>\n",
       "      <td>0.011503</td>\n",
       "      <td>0.016871</td>\n",
       "      <td>0.022239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>0.005292</td>\n",
       "      <td>0.006615</td>\n",
       "      <td>0.003142</td>\n",
       "      <td>0.008103</td>\n",
       "      <td>0.003638</td>\n",
       "      <td>0.004961</td>\n",
       "      <td>0.003473</td>\n",
       "      <td>0.008269</td>\n",
       "      <td>0.012899</td>\n",
       "      <td>0.017529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>0.013825</td>\n",
       "      <td>0.011521</td>\n",
       "      <td>0.002304</td>\n",
       "      <td>0.018433</td>\n",
       "      <td>0.009217</td>\n",
       "      <td>0.006912</td>\n",
       "      <td>0.006912</td>\n",
       "      <td>0.020737</td>\n",
       "      <td>0.018433</td>\n",
       "      <td>0.027650</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>293 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        anger  anticipation   disgust      fear       joy   sadness  surprise  \\\n",
       "0    0.008902      0.011869  0.007418  0.012611  0.011128  0.014095  0.005935   \n",
       "1    0.007831      0.007439  0.006656  0.011355  0.003524  0.009397  0.004307   \n",
       "2    0.011656      0.014724  0.006135  0.014724  0.008589  0.006748  0.006135   \n",
       "3    0.002616      0.007586  0.002093  0.004447  0.004708  0.004708  0.003924   \n",
       "4    0.005722      0.008900  0.002543  0.009218  0.004768  0.006675  0.002543   \n",
       "..        ...           ...       ...       ...       ...       ...       ...   \n",
       "308  0.023438      0.015625  0.015625  0.046875  0.007812  0.039062  0.000000   \n",
       "309  0.004877      0.006828  0.002439  0.007153  0.004390  0.003577  0.002926   \n",
       "310  0.006135      0.007669  0.003834  0.008436  0.003067  0.006902  0.003067   \n",
       "311  0.005292      0.006615  0.003142  0.008103  0.003638  0.004961  0.003473   \n",
       "314  0.013825      0.011521  0.002304  0.018433  0.009217  0.006912  0.006912   \n",
       "\n",
       "        trust  negative  positive  \n",
       "0    0.014837  0.023739  0.026706  \n",
       "1    0.009397  0.022318  0.015270  \n",
       "2    0.022086  0.020859  0.030675  \n",
       "3    0.012294  0.007063  0.017526  \n",
       "4    0.013032  0.014304  0.018754  \n",
       "..        ...       ...       ...  \n",
       "308  0.031250  0.062500  0.031250  \n",
       "309  0.008942  0.009267  0.015445  \n",
       "310  0.011503  0.016871  0.022239  \n",
       "311  0.008269  0.012899  0.017529  \n",
       "314  0.020737  0.018433  0.027650  \n",
       "\n",
       "[293 rows x 10 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion = df[df.columns[79:89]]\n",
    "emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var_r</th>\n",
       "      <th>var_g</th>\n",
       "      <th>var_b</th>\n",
       "      <th>var_h</th>\n",
       "      <th>var_s</th>\n",
       "      <th>var_v</th>\n",
       "      <th>var_bright</th>\n",
       "      <th>var_bright_sd</th>\n",
       "      <th>var_contrast</th>\n",
       "      <th>var_colorful</th>\n",
       "      <th>median_r</th>\n",
       "      <th>median_g</th>\n",
       "      <th>median_b</th>\n",
       "      <th>median_h</th>\n",
       "      <th>median_s</th>\n",
       "      <th>median_v</th>\n",
       "      <th>median_bright</th>\n",
       "      <th>median_bright_sd</th>\n",
       "      <th>median_contrast</th>\n",
       "      <th>median_colorful</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>298.035544</td>\n",
       "      <td>256.526430</td>\n",
       "      <td>317.666638</td>\n",
       "      <td>176.226486</td>\n",
       "      <td>291.471955</td>\n",
       "      <td>318.839186</td>\n",
       "      <td>249.684164</td>\n",
       "      <td>58.312728</td>\n",
       "      <td>273.094250</td>\n",
       "      <td>131.323126</td>\n",
       "      <td>99.315580</td>\n",
       "      <td>85.044938</td>\n",
       "      <td>112.727762</td>\n",
       "      <td>86.092826</td>\n",
       "      <td>134.038498</td>\n",
       "      <td>134.587656</td>\n",
       "      <td>92.603380</td>\n",
       "      <td>66.266163</td>\n",
       "      <td>220.0</td>\n",
       "      <td>74.248203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14.562086</td>\n",
       "      <td>13.206167</td>\n",
       "      <td>14.473241</td>\n",
       "      <td>22.249816</td>\n",
       "      <td>3.087459</td>\n",
       "      <td>14.468497</td>\n",
       "      <td>13.461712</td>\n",
       "      <td>2.128646</td>\n",
       "      <td>6.360655</td>\n",
       "      <td>0.903765</td>\n",
       "      <td>126.550148</td>\n",
       "      <td>128.666820</td>\n",
       "      <td>130.396076</td>\n",
       "      <td>83.762660</td>\n",
       "      <td>39.437676</td>\n",
       "      <td>134.533176</td>\n",
       "      <td>128.116216</td>\n",
       "      <td>75.928568</td>\n",
       "      <td>241.0</td>\n",
       "      <td>19.761427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1397.200532</td>\n",
       "      <td>1812.288126</td>\n",
       "      <td>1125.290200</td>\n",
       "      <td>952.795220</td>\n",
       "      <td>3386.595470</td>\n",
       "      <td>942.447595</td>\n",
       "      <td>1478.249667</td>\n",
       "      <td>160.366149</td>\n",
       "      <td>1102.429909</td>\n",
       "      <td>528.665700</td>\n",
       "      <td>82.593592</td>\n",
       "      <td>44.906568</td>\n",
       "      <td>92.771928</td>\n",
       "      <td>116.763216</td>\n",
       "      <td>175.650236</td>\n",
       "      <td>106.260372</td>\n",
       "      <td>61.527896</td>\n",
       "      <td>54.770904</td>\n",
       "      <td>190.0</td>\n",
       "      <td>71.947426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>800.175857</td>\n",
       "      <td>979.432379</td>\n",
       "      <td>765.202229</td>\n",
       "      <td>533.030459</td>\n",
       "      <td>899.264225</td>\n",
       "      <td>665.884765</td>\n",
       "      <td>850.382767</td>\n",
       "      <td>385.940453</td>\n",
       "      <td>2316.228370</td>\n",
       "      <td>115.950038</td>\n",
       "      <td>79.772896</td>\n",
       "      <td>73.095122</td>\n",
       "      <td>103.627240</td>\n",
       "      <td>107.586062</td>\n",
       "      <td>112.794364</td>\n",
       "      <td>110.728122</td>\n",
       "      <td>77.605316</td>\n",
       "      <td>50.424047</td>\n",
       "      <td>182.0</td>\n",
       "      <td>56.541083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>611.670255</td>\n",
       "      <td>710.533599</td>\n",
       "      <td>704.285521</td>\n",
       "      <td>565.782339</td>\n",
       "      <td>267.640542</td>\n",
       "      <td>616.471287</td>\n",
       "      <td>661.184079</td>\n",
       "      <td>185.405393</td>\n",
       "      <td>48.207604</td>\n",
       "      <td>226.281947</td>\n",
       "      <td>178.297660</td>\n",
       "      <td>182.070140</td>\n",
       "      <td>183.746428</td>\n",
       "      <td>67.315794</td>\n",
       "      <td>5.285804</td>\n",
       "      <td>184.217756</td>\n",
       "      <td>181.100160</td>\n",
       "      <td>92.073127</td>\n",
       "      <td>255.0</td>\n",
       "      <td>10.850100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>147.788425</td>\n",
       "      <td>83.104502</td>\n",
       "      <td>343.138525</td>\n",
       "      <td>287.133037</td>\n",
       "      <td>738.014925</td>\n",
       "      <td>211.022546</td>\n",
       "      <td>73.414553</td>\n",
       "      <td>233.283992</td>\n",
       "      <td>762.744039</td>\n",
       "      <td>122.025988</td>\n",
       "      <td>111.254992</td>\n",
       "      <td>113.640668</td>\n",
       "      <td>101.466524</td>\n",
       "      <td>65.478744</td>\n",
       "      <td>56.372900</td>\n",
       "      <td>122.471632</td>\n",
       "      <td>112.078180</td>\n",
       "      <td>69.131249</td>\n",
       "      <td>237.0</td>\n",
       "      <td>30.316916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>499.048934</td>\n",
       "      <td>671.771055</td>\n",
       "      <td>464.885271</td>\n",
       "      <td>768.802809</td>\n",
       "      <td>362.948260</td>\n",
       "      <td>492.587090</td>\n",
       "      <td>554.760968</td>\n",
       "      <td>93.525503</td>\n",
       "      <td>564.382108</td>\n",
       "      <td>60.941841</td>\n",
       "      <td>123.212802</td>\n",
       "      <td>119.276686</td>\n",
       "      <td>102.296032</td>\n",
       "      <td>79.687310</td>\n",
       "      <td>70.830036</td>\n",
       "      <td>129.006988</td>\n",
       "      <td>118.347988</td>\n",
       "      <td>63.955769</td>\n",
       "      <td>210.0</td>\n",
       "      <td>34.442498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>12.184924</td>\n",
       "      <td>9.988667</td>\n",
       "      <td>8.442325</td>\n",
       "      <td>7.628704</td>\n",
       "      <td>8.160141</td>\n",
       "      <td>12.292979</td>\n",
       "      <td>9.858782</td>\n",
       "      <td>1.924338</td>\n",
       "      <td>13.182123</td>\n",
       "      <td>6.304640</td>\n",
       "      <td>126.875756</td>\n",
       "      <td>101.686932</td>\n",
       "      <td>90.995444</td>\n",
       "      <td>37.053376</td>\n",
       "      <td>85.170332</td>\n",
       "      <td>128.535556</td>\n",
       "      <td>108.091288</td>\n",
       "      <td>70.702156</td>\n",
       "      <td>249.0</td>\n",
       "      <td>54.941191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>3.100336</td>\n",
       "      <td>12.856796</td>\n",
       "      <td>13.246367</td>\n",
       "      <td>0.652040</td>\n",
       "      <td>23.776415</td>\n",
       "      <td>2.342890</td>\n",
       "      <td>9.398054</td>\n",
       "      <td>2.273838</td>\n",
       "      <td>0.001471</td>\n",
       "      <td>6.499906</td>\n",
       "      <td>128.762536</td>\n",
       "      <td>80.070008</td>\n",
       "      <td>88.208350</td>\n",
       "      <td>124.785384</td>\n",
       "      <td>164.544432</td>\n",
       "      <td>141.974716</td>\n",
       "      <td>95.623774</td>\n",
       "      <td>77.683406</td>\n",
       "      <td>241.0</td>\n",
       "      <td>97.623431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>26.059390</td>\n",
       "      <td>20.160538</td>\n",
       "      <td>26.205856</td>\n",
       "      <td>37.262637</td>\n",
       "      <td>9.450165</td>\n",
       "      <td>20.737421</td>\n",
       "      <td>21.035988</td>\n",
       "      <td>16.549036</td>\n",
       "      <td>6.226976</td>\n",
       "      <td>4.321690</td>\n",
       "      <td>185.561740</td>\n",
       "      <td>178.857568</td>\n",
       "      <td>174.949488</td>\n",
       "      <td>29.728208</td>\n",
       "      <td>31.127512</td>\n",
       "      <td>188.196768</td>\n",
       "      <td>180.733376</td>\n",
       "      <td>75.648709</td>\n",
       "      <td>231.0</td>\n",
       "      <td>20.090563</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>293 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           var_r        var_g        var_b       var_h        var_s  \\\n",
       "0     298.035544   256.526430   317.666638  176.226486   291.471955   \n",
       "1      14.562086    13.206167    14.473241   22.249816     3.087459   \n",
       "2    1397.200532  1812.288126  1125.290200  952.795220  3386.595470   \n",
       "3     800.175857   979.432379   765.202229  533.030459   899.264225   \n",
       "4     611.670255   710.533599   704.285521  565.782339   267.640542   \n",
       "..           ...          ...          ...         ...          ...   \n",
       "308   147.788425    83.104502   343.138525  287.133037   738.014925   \n",
       "309   499.048934   671.771055   464.885271  768.802809   362.948260   \n",
       "310    12.184924     9.988667     8.442325    7.628704     8.160141   \n",
       "311     3.100336    12.856796    13.246367    0.652040    23.776415   \n",
       "314    26.059390    20.160538    26.205856   37.262637     9.450165   \n",
       "\n",
       "          var_v   var_bright  var_bright_sd  var_contrast  var_colorful  \\\n",
       "0    318.839186   249.684164      58.312728    273.094250    131.323126   \n",
       "1     14.468497    13.461712       2.128646      6.360655      0.903765   \n",
       "2    942.447595  1478.249667     160.366149   1102.429909    528.665700   \n",
       "3    665.884765   850.382767     385.940453   2316.228370    115.950038   \n",
       "4    616.471287   661.184079     185.405393     48.207604    226.281947   \n",
       "..          ...          ...            ...           ...           ...   \n",
       "308  211.022546    73.414553     233.283992    762.744039    122.025988   \n",
       "309  492.587090   554.760968      93.525503    564.382108     60.941841   \n",
       "310   12.292979     9.858782       1.924338     13.182123      6.304640   \n",
       "311    2.342890     9.398054       2.273838      0.001471      6.499906   \n",
       "314   20.737421    21.035988      16.549036      6.226976      4.321690   \n",
       "\n",
       "       median_r    median_g    median_b    median_h    median_s    median_v  \\\n",
       "0     99.315580   85.044938  112.727762   86.092826  134.038498  134.587656   \n",
       "1    126.550148  128.666820  130.396076   83.762660   39.437676  134.533176   \n",
       "2     82.593592   44.906568   92.771928  116.763216  175.650236  106.260372   \n",
       "3     79.772896   73.095122  103.627240  107.586062  112.794364  110.728122   \n",
       "4    178.297660  182.070140  183.746428   67.315794    5.285804  184.217756   \n",
       "..          ...         ...         ...         ...         ...         ...   \n",
       "308  111.254992  113.640668  101.466524   65.478744   56.372900  122.471632   \n",
       "309  123.212802  119.276686  102.296032   79.687310   70.830036  129.006988   \n",
       "310  126.875756  101.686932   90.995444   37.053376   85.170332  128.535556   \n",
       "311  128.762536   80.070008   88.208350  124.785384  164.544432  141.974716   \n",
       "314  185.561740  178.857568  174.949488   29.728208   31.127512  188.196768   \n",
       "\n",
       "     median_bright  median_bright_sd  median_contrast  median_colorful  \n",
       "0        92.603380         66.266163            220.0        74.248203  \n",
       "1       128.116216         75.928568            241.0        19.761427  \n",
       "2        61.527896         54.770904            190.0        71.947426  \n",
       "3        77.605316         50.424047            182.0        56.541083  \n",
       "4       181.100160         92.073127            255.0        10.850100  \n",
       "..             ...               ...              ...              ...  \n",
       "308     112.078180         69.131249            237.0        30.316916  \n",
       "309     118.347988         63.955769            210.0        34.442498  \n",
       "310     108.091288         70.702156            249.0        54.941191  \n",
       "311      95.623774         77.683406            241.0        97.623431  \n",
       "314     180.733376         75.648709            231.0        20.090563  \n",
       "\n",
       "[293 rows x 20 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visual = df[df.columns[25:45]]\n",
    "visual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "df['Attitude'] = le.fit_transform(df['Attitude'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0\n",
       "1      0\n",
       "2      0\n",
       "3      0\n",
       "4      1\n",
       "      ..\n",
       "308    1\n",
       "309    0\n",
       "310    0\n",
       "311    0\n",
       "314    0\n",
       "Name: Attitude, Length: 293, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = df['Attitude']\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RandomForest - word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train_emotion,X_test_emotion,y_train_emotion,y_test_emotion = train_test_split(emotion,y,test_size = 0.2, shuffle = True, random_state = 123, stratify = y)\n",
    "X_train_visual,X_test_visual,y_train_visual,y_test_visual = train_test_split(visual,y,test_size = 0.2, shuffle = True, random_state = 123, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "202    0\n",
       "162    1\n",
       "210    1\n",
       "83     1\n",
       "279    1\n",
       "      ..\n",
       "70     1\n",
       "204    1\n",
       "250    0\n",
       "120    1\n",
       "1      0\n",
       "Name: Attitude, Length: 234, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "202    0\n",
       "162    1\n",
       "210    1\n",
       "83     1\n",
       "279    1\n",
       "      ..\n",
       "70     1\n",
       "204    1\n",
       "250    0\n",
       "120    1\n",
       "1      0\n",
       "Name: Attitude, Length: 234, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_visual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn import datasets\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators=500,\n",
    "                              random_state=1,\n",
    "                             max_features=None,max_depth=None,min_samples_split=2)\n",
    "#cv = cross_validate(forest, docvec_df, y, cv=10,scoring = ['precision','recall'])\n",
    "\n",
    "y_model1 = forest.fit(X_train_emotion,y_train_emotion).predict(X_test_emotion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
       "       1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_model1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RandomForest - Visual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn import datasets\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators=500,\n",
    "                              random_state=1,\n",
    "                             min_samples_split= 5)\n",
    "#cv = cross_validate(forest, visual_train, y, cv=10,scoring = ['precision','recall'])\n",
    "\n",
    "y_model2 = forest.fit(X_train_visual,y_train_visual).predict(X_test_visual)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_model2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mcnemar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[32  9]\n",
      " [ 4 14]]\n"
     ]
    }
   ],
   "source": [
    "from mlxtend.evaluate import mcnemar_table\n",
    "\n",
    "tb = mcnemar_table(y_target=y_test_visual, \n",
    "                   y_model1=y_model1, \n",
    "                   y_model2=y_model2)\n",
    "\n",
    "print(tb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chi-squared: 1.2307692307692308\n",
      "p-value: 0.2672574931543847\n"
     ]
    }
   ],
   "source": [
    "from mlxtend.evaluate import mcnemar\n",
    "\n",
    "chi2, p = mcnemar(ary=tb, corrected=True)\n",
    "print('chi-squared:', chi2)\n",
    "print('p-value:', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:1: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "result = pd.DataFrame(pd.np.column_stack([visual, X_oh]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.concat([visual, textual], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of            0            1            2           3            4      \\\n",
       "0     298.035544   256.526430   317.666638  176.226486   291.471955   \n",
       "1      14.562086    13.206167    14.473241   22.249816     3.087459   \n",
       "2    1397.200532  1812.288126  1125.290200  952.795220  3386.595470   \n",
       "3     800.175857   979.432379   765.202229  533.030459   899.264225   \n",
       "4     611.670255   710.533599   704.285521  565.782339   267.640542   \n",
       "..           ...          ...          ...         ...          ...   \n",
       "308   147.788425    83.104502   343.138525  287.133037   738.014925   \n",
       "309   499.048934   671.771055   464.885271  768.802809   362.948260   \n",
       "310    12.184924     9.988667     8.442325    7.628704     8.160141   \n",
       "311     3.100336    12.856796    13.246367    0.652040    23.776415   \n",
       "312    26.059390    20.160538    26.205856   37.262637     9.450165   \n",
       "\n",
       "          5            6           7            8           9      ...  10010  \\\n",
       "0    318.839186   249.684164   58.312728   273.094250  131.323126  ...    0.0   \n",
       "1     14.468497    13.461712    2.128646     6.360655    0.903765  ...    0.0   \n",
       "2    942.447595  1478.249667  160.366149  1102.429909  528.665700  ...    0.0   \n",
       "3    665.884765   850.382767  385.940453  2316.228370  115.950038  ...    0.0   \n",
       "4    616.471287   661.184079  185.405393    48.207604  226.281947  ...    0.0   \n",
       "..          ...          ...         ...          ...         ...  ...    ...   \n",
       "308  211.022546    73.414553  233.283992   762.744039  122.025988  ...    0.0   \n",
       "309  492.587090   554.760968   93.525503   564.382108   60.941841  ...    0.0   \n",
       "310   12.292979     9.858782    1.924338    13.182123    6.304640  ...    0.0   \n",
       "311    2.342890     9.398054    2.273838     0.001471    6.499906  ...    0.0   \n",
       "312   20.737421    21.035988   16.549036     6.226976    4.321690  ...    0.0   \n",
       "\n",
       "     10011  10012  10013  10014  10015  10016  10017  10018  10019  \n",
       "0      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "1      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "2      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "3      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "4      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "..     ...    ...    ...    ...    ...    ...    ...    ...    ...  \n",
       "308    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "309    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    1.0  \n",
       "310    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "311    0.0    1.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "312    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "\n",
       "[313 rows x 10020 columns]>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate K-fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# inputs data samples: 313\n",
      "# targets data samples: 313\n",
      "Shape of train set: (281, 10020)\n",
      "Shape of y: (281, 2)\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Shape of validation set: (32, 10020)\n",
      "{'loss': [14.612687110900879, 112.26792907714844, 11.04271411895752, 22.205244064331055, 6.545848369598389, 10.392186164855957, 12.305441856384277, 3.1799399852752686, 7.627873420715332, 9.307175636291504, 2.0185112953186035, 5.807286739349365, 8.04775619506836, 1.7864655256271362, 4.3010735511779785, 7.815014362335205, 1.342038631439209, 3.948058843612671, 6.801834583282471, 1.2309454679489136, 3.518735885620117, 6.582371711730957, 1.053055763244629, 3.535033702850342, 5.768925666809082, 0.5020555853843689, 0.9378923773765564, 5.056182861328125, 0.7957579493522644, 2.749514102935791, 2.596181869506836, 7.4888105392456055, 1.158350944519043, 3.417651414871216, 5.8859148025512695, 0.46127617359161377, 1.3266457319259644, 4.875954627990723, 0.7336472868919373, 0.6965383887290955, 3.714428424835205, 0.777716875076294, 1.7395023107528687, 3.419144630432129, 7.238190650939941, 1.4075311422348022, 2.541917324066162, 4.802671909332275, 0.5301170349121094, 1.877925992012024], 'accuracy': [0.6832740306854248, 0.3131672739982605, 0.3416370153427124, 0.6868327260017395, 0.7010676264762878, 0.33096083998680115, 0.6868327260017395, 0.7437722682952881, 0.4519572854042053, 0.690391480922699, 0.7580071091651917, 0.5409252643585205, 0.7117437720298767, 0.7935943007469177, 0.6120996475219727, 0.7330960631370544, 0.8149465918540955, 0.5943060517311096, 0.7330960631370544, 0.8256227970123291, 0.6441280841827393, 0.7437722682952881, 0.8327401876449585, 0.6476868391036987, 0.7473309636116028, 0.8576512336730957, 0.8042704463005066, 0.7686832547187805, 0.790035605430603, 0.7935943007469177, 0.6441280841827393, 0.7473309636116028, 0.8327401876449585, 0.608540952205658, 0.754448413848877, 0.8754448294639587, 0.77224200963974, 0.7793594598770142, 0.8612099885940552, 0.836298942565918, 0.8007117509841919, 0.8256227970123291, 0.8256227970123291, 0.6192170977592468, 0.7508896589279175, 0.8078292012214661, 0.6975088715553284, 0.7864768505096436, 0.8825622797012329, 0.7188612222671509], 'precision_32': [0.6832740306854248, 0.3131672739982605, 0.3416370153427124, 0.6868327260017395, 0.7010676264762878, 0.33096083998680115, 0.6868327260017395, 0.7437722682952881, 0.4519572854042053, 0.690391480922699, 0.7580071091651917, 0.5409252643585205, 0.7117437720298767, 0.7935943007469177, 0.6120996475219727, 0.7330960631370544, 0.8149465918540955, 0.5943060517311096, 0.7330960631370544, 0.8256227970123291, 0.6441280841827393, 0.7437722682952881, 0.8327401876449585, 0.6476868391036987, 0.7473309636116028, 0.8576512336730957, 0.8042704463005066, 0.7686832547187805, 0.790035605430603, 0.7935943007469177, 0.6441280841827393, 0.7473309636116028, 0.8327401876449585, 0.608540952205658, 0.754448413848877, 0.8754448294639587, 0.77224200963974, 0.7793594598770142, 0.8612099885940552, 0.836298942565918, 0.8007117509841919, 0.8256227970123291, 0.8256227970123291, 0.6192170977592468, 0.7508896589279175, 0.8078292012214661, 0.6975088715553284, 0.7864768505096436, 0.8825622797012329, 0.7188612222671509], 'recall_32': [0.6832740306854248, 0.3131672739982605, 0.3416370153427124, 0.6868327260017395, 0.7010676264762878, 0.33096083998680115, 0.6868327260017395, 0.7437722682952881, 0.4519572854042053, 0.690391480922699, 0.7580071091651917, 0.5409252643585205, 0.7117437720298767, 0.7935943007469177, 0.6120996475219727, 0.7330960631370544, 0.8149465918540955, 0.5943060517311096, 0.7330960631370544, 0.8256227970123291, 0.6441280841827393, 0.7437722682952881, 0.8327401876449585, 0.6476868391036987, 0.7473309636116028, 0.8576512336730957, 0.8042704463005066, 0.7686832547187805, 0.790035605430603, 0.7935943007469177, 0.6441280841827393, 0.7473309636116028, 0.8327401876449585, 0.608540952205658, 0.754448413848877, 0.8754448294639587, 0.77224200963974, 0.7793594598770142, 0.8612099885940552, 0.836298942565918, 0.8007117509841919, 0.8256227970123291, 0.8256227970123291, 0.6192170977592468, 0.7508896589279175, 0.8078292012214661, 0.6975088715553284, 0.7864768505096436, 0.8825622797012329, 0.7188612222671509]}\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f889cb33050> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "> Fold 1 - Precison: 0.679788961038961 - Recall: 0.8636363636363636%\n",
      "Shape of train set: (281, 10020)\n",
      "Shape of y: (281, 2)\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Shape of validation set: (32, 10020)\n",
      "{'loss': [12.85079288482666, 60.1048698425293, 7.313065528869629, 55.94164276123047, 13.180625915527344, 11.51456069946289, 17.902616500854492, 3.428316116333008, 12.710392951965332, 9.789180755615234, 1.2268717288970947, 1.2368799448013306, 4.666477680206299, 8.888895988464355, 1.1419105529785156, 4.443930625915527, 8.04837417602539, 1.2358250617980957, 3.1345512866973877, 5.731544494628906, 0.8254855871200562, 1.6872128248214722, 4.512801647186279, 0.6391445994377136, 1.694576621055603, 3.7633607387542725, 6.037046909332275, 1.1565278768539429, 0.6297668814659119, 0.9995376467704773, 7.465222358703613, 6.061345100402832, 1.3555586338043213, 4.015980243682861, 0.46849629282951355, 0.8147639036178589, 3.8903326988220215, 5.144746780395508, 1.567095398902893, 1.2627222537994385, 3.2979471683502197, 3.7208335399627686, 3.920991897583008, 0.6750710010528564, 0.6177618503570557, 0.9253926873207092, 2.450495719909668, 5.803940296173096, 0.8604762554168701, 0.4656669497489929], 'accuracy': [0.46975088119506836, 0.6975088715553284, 0.5907473564147949, 0.30604982376098633, 0.6975088715553284, 0.47330960631370544, 0.6975088715553284, 0.7651245594024658, 0.38790035247802734, 0.6975088715553284, 0.7473309636116028, 0.790035605430603, 0.5800711512565613, 0.7437722682952881, 0.7864768505096436, 0.5978647470474243, 0.7508896589279175, 0.8007117509841919, 0.6761565804481506, 0.7651245594024658, 0.8398576378822327, 0.6939501762390137, 0.7686832547187805, 0.836298942565918, 0.7935943007469177, 0.608540952205658, 0.7580071091651917, 0.77224200963974, 0.8256227970123291, 0.8291814923286438, 0.5231316685676575, 0.7473309636116028, 0.754448413848877, 0.7793594598770142, 0.8790035843849182, 0.854092538356781, 0.6192170977592468, 0.7793594598770142, 0.8220640420913696, 0.7437722682952881, 0.7758007049560547, 0.6868327260017395, 0.7971529960632324, 0.754448413848877, 0.8647686839103699, 0.8505337834358215, 0.6583629846572876, 0.7615658640861511, 0.8398576378822327, 0.8647686839103699], 'precision_33': [0.46975088119506836, 0.6975088715553284, 0.5907473564147949, 0.30604982376098633, 0.6975088715553284, 0.47330960631370544, 0.6975088715553284, 0.7651245594024658, 0.38790035247802734, 0.6975088715553284, 0.7473309636116028, 0.790035605430603, 0.5800711512565613, 0.7437722682952881, 0.7864768505096436, 0.5978647470474243, 0.7508896589279175, 0.8007117509841919, 0.6761565804481506, 0.7651245594024658, 0.8398576378822327, 0.6939501762390137, 0.7686832547187805, 0.836298942565918, 0.7935943007469177, 0.608540952205658, 0.7580071091651917, 0.77224200963974, 0.8256227970123291, 0.8291814923286438, 0.5231316685676575, 0.7473309636116028, 0.754448413848877, 0.7793594598770142, 0.8790035843849182, 0.854092538356781, 0.6192170977592468, 0.7793594598770142, 0.8220640420913696, 0.7437722682952881, 0.7758007049560547, 0.6868327260017395, 0.7971529960632324, 0.754448413848877, 0.8647686839103699, 0.8505337834358215, 0.6583629846572876, 0.7615658640861511, 0.8398576378822327, 0.8647686839103699], 'recall_33': [0.46975088119506836, 0.6975088715553284, 0.5907473564147949, 0.30604982376098633, 0.6975088715553284, 0.47330960631370544, 0.6975088715553284, 0.7651245594024658, 0.38790035247802734, 0.6975088715553284, 0.7473309636116028, 0.790035605430603, 0.5800711512565613, 0.7437722682952881, 0.7864768505096436, 0.5978647470474243, 0.7508896589279175, 0.8007117509841919, 0.6761565804481506, 0.7651245594024658, 0.8398576378822327, 0.6939501762390137, 0.7686832547187805, 0.836298942565918, 0.7935943007469177, 0.608540952205658, 0.7580071091651917, 0.77224200963974, 0.8256227970123291, 0.8291814923286438, 0.5231316685676575, 0.7473309636116028, 0.754448413848877, 0.7793594598770142, 0.8790035843849182, 0.854092538356781, 0.6192170977592468, 0.7793594598770142, 0.8220640420913696, 0.7437722682952881, 0.7758007049560547, 0.6868327260017395, 0.7971529960632324, 0.754448413848877, 0.8647686839103699, 0.8505337834358215, 0.6583629846572876, 0.7615658640861511, 0.8398576378822327, 0.8647686839103699]}\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f88acf294d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Fold 2 - Precison: 0.5996710526315789 - Recall: 0.9473684210526315%\n",
      "Shape of train set: (281, 10020)\n",
      "Shape of y: (281, 2)\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Shape of validation set: (32, 10020)\n",
      "{'loss': [38.87637710571289, 39.095333099365234, 10.807106971740723, 13.52180290222168, 17.20587730407715, 5.658076286315918, 6.6843671798706055, 12.40434741973877, 4.046075820922852, 5.233001708984375, 11.479544639587402, 3.801405429840088, 4.777150630950928, 9.12358570098877, 2.576449394226074, 4.159852981567383, 9.514265060424805, 2.8420162200927734, 3.612731456756592, 8.081618309020996, 1.7938988208770752, 5.306344985961914, 7.231159687042236, 1.7717946767807007, 3.519242286682129, 6.300229549407959, 1.2859834432601929, 2.8618297576904297, 6.285096168518066, 1.1270300149917603, 3.32780122756958, 5.701518535614014, 1.0145001411437988, 1.873701810836792, 5.361833572387695, 0.6023902893066406, 1.216110110282898, 4.6845879554748535, 0.42566657066345215, 0.41904494166374207, 1.7826629877090454, 3.2029948234558105, 6.397936820983887, 1.1816610097885132, 2.635164499282837, 4.900461196899414, 1.0060524940490723, 0.6486642956733704, 1.9610114097595215, 2.044621706008911], 'accuracy': [0.3167259693145752, 0.6832740306854248, 0.6797152757644653, 0.35587188601493835, 0.6832740306854248, 0.7402135133743286, 0.4911032021045685, 0.6975088715553284, 0.7188612222671509, 0.4875444769859314, 0.690391480922699, 0.7580071091651917, 0.5693950057029724, 0.7473309636116028, 0.790035605430603, 0.5480427145957947, 0.7295373678207397, 0.790035605430603, 0.5231316685676575, 0.7188612222671509, 0.7971529960632324, 0.5871886014938354, 0.7615658640861511, 0.8078292012214661, 0.6049821972846985, 0.7615658640861511, 0.836298942565918, 0.6334519386291504, 0.754448413848877, 0.8469750881195068, 0.6298932433128357, 0.7793594598770142, 0.8398576378822327, 0.7153024673461914, 0.7686832547187805, 0.8932384252548218, 0.77224200963974, 0.7935943007469177, 0.8896797299385071, 0.8825622797012329, 0.8327401876449585, 0.5943060517311096, 0.7437722682952881, 0.790035605430603, 0.6298932433128357, 0.790035605430603, 0.8683273792266846, 0.836298942565918, 0.8220640420913696, 0.7330960631370544], 'precision_34': [0.3167259693145752, 0.6832740306854248, 0.6797152757644653, 0.35587188601493835, 0.6832740306854248, 0.7402135133743286, 0.4911032021045685, 0.6975088715553284, 0.7188612222671509, 0.4875444769859314, 0.690391480922699, 0.7580071091651917, 0.5693950057029724, 0.7473309636116028, 0.790035605430603, 0.5480427145957947, 0.7295373678207397, 0.790035605430603, 0.5231316685676575, 0.7188612222671509, 0.7971529960632324, 0.5871886014938354, 0.7615658640861511, 0.8078292012214661, 0.6049821972846985, 0.7615658640861511, 0.836298942565918, 0.6334519386291504, 0.754448413848877, 0.8469750881195068, 0.6298932433128357, 0.7793594598770142, 0.8398576378822327, 0.7153024673461914, 0.7686832547187805, 0.8932384252548218, 0.77224200963974, 0.7935943007469177, 0.8896797299385071, 0.8825622797012329, 0.8327401876449585, 0.5943060517311096, 0.7437722682952881, 0.790035605430603, 0.6298932433128357, 0.790035605430603, 0.8683273792266846, 0.836298942565918, 0.8220640420913696, 0.7330960631370544], 'recall_34': [0.3167259693145752, 0.6832740306854248, 0.6797152757644653, 0.35587188601493835, 0.6832740306854248, 0.7402135133743286, 0.4911032021045685, 0.6975088715553284, 0.7188612222671509, 0.4875444769859314, 0.690391480922699, 0.7580071091651917, 0.5693950057029724, 0.7473309636116028, 0.790035605430603, 0.5480427145957947, 0.7295373678207397, 0.790035605430603, 0.5231316685676575, 0.7188612222671509, 0.7971529960632324, 0.5871886014938354, 0.7615658640861511, 0.8078292012214661, 0.6049821972846985, 0.7615658640861511, 0.836298942565918, 0.6334519386291504, 0.754448413848877, 0.8469750881195068, 0.6298932433128357, 0.7793594598770142, 0.8398576378822327, 0.7153024673461914, 0.7686832547187805, 0.8932384252548218, 0.77224200963974, 0.7935943007469177, 0.8896797299385071, 0.8825622797012329, 0.8327401876449585, 0.5943060517311096, 0.7437722682952881, 0.790035605430603, 0.6298932433128357, 0.790035605430603, 0.8683273792266846, 0.836298942565918, 0.8220640420913696, 0.7330960631370544]}\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f88b2c1f680> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "> Fold 3 - Precison: 0.71875 - Recall: 1.0%\n",
      "Shape of train set: (282, 10020)\n",
      "Shape of y: (282, 2)\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "Shape of validation set: (31, 10020)\n",
      "{'loss': [4.864107131958008, 64.80400085449219, 5.588998794555664, 21.11333656311035, 4.30918025970459, 15.554287910461426, 2.80183482170105, 16.497360229492188, 6.822903156280518, 1.2639237642288208, 3.0843658447265625, 2.9913787841796875, 6.379672050476074, 1.5726484060287476, 2.670891523361206, 4.238208293914795, 0.9487523436546326, 1.8832755088806152, 4.122702121734619, 0.8241010904312134, 2.5437161922454834, 3.9734344482421875, 0.8482087850570679, 2.0850205421447754, 3.3606295585632324, 0.552568256855011, 1.5635906457901, 3.5417985916137695, 0.6165111064910889, 1.8884121179580688, 2.986436605453491, 1.1525813341140747, 2.8766961097717285, 0.8905888795852661, 2.5062577724456787, 0.7667016386985779, 2.4468140602111816, 0.23794980347156525, 0.2765026092529297, 1.1502394676208496, 3.1427478790283203, 0.4758231043815613, 0.7497978806495667, 2.1529078483581543, 1.1538265943527222, 2.186189651489258, 1.3009912967681885, 3.6401891708374023, 0.6474858522415161, 1.5081056356430054], 'accuracy': [0.6382978558540344, 0.3510638177394867, 0.588652491569519, 0.6808510422706604, 0.44680851697921753, 0.6879432797431946, 0.741134762763977, 0.39007091522216797, 0.6879432797431946, 0.7198581695556641, 0.758865237236023, 0.5709219574928284, 0.7198581695556641, 0.783687949180603, 0.6134752035140991, 0.76241135597229, 0.7907801270484924, 0.6773049831390381, 0.7553191781044006, 0.7907801270484924, 0.6276595592498779, 0.7553191781044006, 0.8156028389930725, 0.673758864402771, 0.7730496525764465, 0.8439716100692749, 0.7056737542152405, 0.7730496525764465, 0.8439716100692749, 0.7021276354789734, 0.7765957713127136, 0.7482269406318665, 0.7695035338401794, 0.76241135597229, 0.7730496525764465, 0.8014184236526489, 0.8014184236526489, 0.9007092118263245, 0.914893627166748, 0.741134762763977, 0.7943262457847595, 0.8617021441459656, 0.826241135597229, 0.8120567202568054, 0.76241135597229, 0.7907801270484924, 0.6985815763473511, 0.783687949180603, 0.868794322013855, 0.7695035338401794], 'precision_35': [0.6382978558540344, 0.3510638177394867, 0.588652491569519, 0.6808510422706604, 0.44680851697921753, 0.6879432797431946, 0.741134762763977, 0.39007091522216797, 0.6879432797431946, 0.7198581695556641, 0.758865237236023, 0.5709219574928284, 0.7198581695556641, 0.783687949180603, 0.6134752035140991, 0.76241135597229, 0.7907801270484924, 0.6773049831390381, 0.7553191781044006, 0.7907801270484924, 0.6276595592498779, 0.7553191781044006, 0.8156028389930725, 0.673758864402771, 0.7730496525764465, 0.8439716100692749, 0.7056737542152405, 0.7730496525764465, 0.8439716100692749, 0.7021276354789734, 0.7765957713127136, 0.7482269406318665, 0.7695035338401794, 0.76241135597229, 0.7730496525764465, 0.8014184236526489, 0.8014184236526489, 0.9007092118263245, 0.914893627166748, 0.741134762763977, 0.7943262457847595, 0.8617021441459656, 0.826241135597229, 0.8120567202568054, 0.76241135597229, 0.7907801270484924, 0.6985815763473511, 0.783687949180603, 0.868794322013855, 0.7695035338401794], 'recall_35': [0.6382978558540344, 0.3510638177394867, 0.588652491569519, 0.6808510422706604, 0.44680851697921753, 0.6879432797431946, 0.741134762763977, 0.39007091522216797, 0.6879432797431946, 0.7198581695556641, 0.758865237236023, 0.5709219574928284, 0.7198581695556641, 0.783687949180603, 0.6134752035140991, 0.76241135597229, 0.7907801270484924, 0.6773049831390381, 0.7553191781044006, 0.7907801270484924, 0.6276595592498779, 0.7553191781044006, 0.8156028389930725, 0.673758864402771, 0.7730496525764465, 0.8439716100692749, 0.7056737542152405, 0.7730496525764465, 0.8439716100692749, 0.7021276354789734, 0.7765957713127136, 0.7482269406318665, 0.7695035338401794, 0.76241135597229, 0.7730496525764465, 0.8014184236526489, 0.8014184236526489, 0.9007092118263245, 0.914893627166748, 0.741134762763977, 0.7943262457847595, 0.8617021441459656, 0.826241135597229, 0.8120567202568054, 0.76241135597229, 0.7907801270484924, 0.6985815763473511, 0.783687949180603, 0.868794322013855, 0.7695035338401794]}\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f88c6f6f9e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Fold 4 - Precison: 0.7337073398784478 - Recall: 0.9565217391304348%\n",
      "Shape of train set: (282, 10020)\n",
      "Shape of y: (282, 2)\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "Shape of validation set: (31, 10020)\n",
      "{'loss': [5.301913738250732, 29.386518478393555, 66.49966430664062, 20.949359893798828, 6.635838985443115, 17.312469482421875, 3.5040087699890137, 15.619009017944336, 13.456180572509766, 3.8122315406799316, 8.13048267364502, 11.187124252319336, 3.615729570388794, 4.999382495880127, 9.755850791931152, 3.184000015258789, 3.880000591278076, 8.798354148864746, 2.706212043762207, 3.1887478828430176, 7.9768524169921875, 2.2919461727142334, 3.0652925968170166, 7.2681756019592285, 1.7199349403381348, 3.5702314376831055, 6.637542724609375, 1.7971735000610352, 3.5120561122894287, 6.425045490264893, 1.3273895978927612, 3.0316708087921143, 4.931371212005615, 0.5698677897453308, 2.201477527618408, 4.81189489364624, 0.4105856418609619, 2.204218864440918, 4.763605117797852, 0.27724573016166687, 1.3179500102996826, 3.684070348739624, 0.34832286834716797, 0.9696892499923706, 3.0400211811065674, 5.266724109649658, 0.48471230268478394, 1.9997409582138062, 4.6225810050964355, 0.26797059178352356], 'accuracy': [0.6560283899307251, 0.5496453642845154, 0.3085106313228607, 0.695035457611084, 0.4964539110660553, 0.695035457611084, 0.7446808218955994, 0.40780141949653625, 0.695035457611084, 0.7695035338401794, 0.4893617033958435, 0.73758864402771, 0.7695035338401794, 0.5567376017570496, 0.7517730593681335, 0.7943262457847595, 0.5957446694374084, 0.7517730593681335, 0.8191489577293396, 0.6241135001182556, 0.76241135597229, 0.8297872543334961, 0.6241135001182556, 0.758865237236023, 0.8404255509376526, 0.6418439745903015, 0.7695035338401794, 0.8404255509376526, 0.6347517967224121, 0.7659574747085571, 0.8439716100692749, 0.673758864402771, 0.7872340679168701, 0.890070915222168, 0.7021276354789734, 0.7907801270484924, 0.9042553305625916, 0.7269503474235535, 0.8014184236526489, 0.9184397459030151, 0.7872340679168701, 0.804964542388916, 0.9042553305625916, 0.8581560254096985, 0.6595744490623474, 0.7872340679168701, 0.9078013896942139, 0.7021276354789734, 0.7907801270484924, 0.9290780425071716], 'precision_36': [0.6560283899307251, 0.5496453642845154, 0.3085106313228607, 0.695035457611084, 0.4964539110660553, 0.695035457611084, 0.7446808218955994, 0.40780141949653625, 0.695035457611084, 0.7695035338401794, 0.4893617033958435, 0.73758864402771, 0.7695035338401794, 0.5567376017570496, 0.7517730593681335, 0.7943262457847595, 0.5957446694374084, 0.7517730593681335, 0.8191489577293396, 0.6241135001182556, 0.76241135597229, 0.8297872543334961, 0.6241135001182556, 0.758865237236023, 0.8404255509376526, 0.6418439745903015, 0.7695035338401794, 0.8404255509376526, 0.6347517967224121, 0.7659574747085571, 0.8439716100692749, 0.673758864402771, 0.7872340679168701, 0.890070915222168, 0.7021276354789734, 0.7907801270484924, 0.9042553305625916, 0.7269503474235535, 0.8014184236526489, 0.9184397459030151, 0.7872340679168701, 0.804964542388916, 0.9042553305625916, 0.8581560254096985, 0.6595744490623474, 0.7872340679168701, 0.9078013896942139, 0.7021276354789734, 0.7907801270484924, 0.9290780425071716], 'recall_36': [0.6560283899307251, 0.5496453642845154, 0.3085106313228607, 0.695035457611084, 0.4964539110660553, 0.695035457611084, 0.7446808218955994, 0.40780141949653625, 0.695035457611084, 0.7695035338401794, 0.4893617033958435, 0.73758864402771, 0.7695035338401794, 0.5567376017570496, 0.7517730593681335, 0.7943262457847595, 0.5957446694374084, 0.7517730593681335, 0.8191489577293396, 0.6241135001182556, 0.76241135597229, 0.8297872543334961, 0.6241135001182556, 0.758865237236023, 0.8404255509376526, 0.6418439745903015, 0.7695035338401794, 0.8404255509376526, 0.6347517967224121, 0.7659574747085571, 0.8439716100692749, 0.673758864402771, 0.7872340679168701, 0.890070915222168, 0.7021276354789734, 0.7907801270484924, 0.9042553305625916, 0.7269503474235535, 0.8014184236526489, 0.9184397459030151, 0.7872340679168701, 0.804964542388916, 0.9042553305625916, 0.8581560254096985, 0.6595744490623474, 0.7872340679168701, 0.9078013896942139, 0.7021276354789734, 0.7907801270484924, 0.9290780425071716]}\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f88b2cabef0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "> Fold 5 - Precison: 0.6171072843398819 - Recall: 0.6842105263157895%\n",
      "Shape of train set: (282, 10020)\n",
      "Shape of y: (282, 2)\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 6 ...\n",
      "Shape of validation set: (31, 10020)\n",
      "{'loss': [9.177539825439453, 90.14254760742188, 4.313948154449463, 11.573719024658203, 3.3259193897247314, 9.762743949890137, 3.5576813220977783, 3.6904168128967285, 6.186521053314209, 2.6690146923065186, 0.7528555393218994, 1.243870496749878, 3.4823062419891357, 4.503049373626709, 1.499423623085022, 1.3889906406402588, 3.0899147987365723, 0.812991201877594, 1.5587522983551025, 3.3498356342315674, 0.9152586460113525, 1.2607368230819702, 2.7474141120910645, 0.5761404633522034, 1.8234446048736572, 3.4911577701568604, 0.7777765393257141, 1.4505672454833984, 3.2059476375579834, 0.9764569997787476, 0.9884828329086304, 2.639887809753418, 0.5422102212905884, 0.892590343952179, 2.3258988857269287, 0.7742834091186523, 1.0209157466888428, 3.1957833766937256, 0.6793197989463806, 1.1673173904418945, 2.6341376304626465, 0.6024134755134583, 1.0804126262664795, 2.3213040828704834, 0.44571036100387573, 0.762153685092926, 1.820683240890503, 0.25392988324165344, 0.2971261143684387, 0.9578549265861511], 'accuracy': [0.6276595592498779, 0.3617021143436432, 0.5390070676803589, 0.6843971610069275, 0.41134750843048096, 0.6843971610069275, 0.7517730593681335, 0.5390070676803589, 0.741134762763977, 0.7517730593681335, 0.73758864402771, 0.7695035338401794, 0.5106382966041565, 0.7198581695556641, 0.783687949180603, 0.6453900933265686, 0.7695035338401794, 0.8333333134651184, 0.6489361524581909, 0.7695035338401794, 0.8226950168609619, 0.6843971610069275, 0.7801418304443359, 0.8368794322013855, 0.609929084777832, 0.76241135597229, 0.8333333134651184, 0.6560283899307251, 0.783687949180603, 0.8191489577293396, 0.6808510422706604, 0.7695035338401794, 0.8368794322013855, 0.7659574747085571, 0.7943262457847595, 0.8333333134651184, 0.7234042286872864, 0.783687949180603, 0.8404255509376526, 0.7092198729515076, 0.8085106611251831, 0.8510638475418091, 0.6879432797431946, 0.804964542388916, 0.8758864998817444, 0.7765957713127136, 0.8191489577293396, 0.9007092118263245, 0.8723404407501221, 0.8368794322013855], 'precision_37': [0.6276595592498779, 0.3617021143436432, 0.5390070676803589, 0.6843971610069275, 0.41134750843048096, 0.6843971610069275, 0.7517730593681335, 0.5390070676803589, 0.741134762763977, 0.7517730593681335, 0.73758864402771, 0.7695035338401794, 0.5106382966041565, 0.7198581695556641, 0.783687949180603, 0.6453900933265686, 0.7695035338401794, 0.8333333134651184, 0.6489361524581909, 0.7695035338401794, 0.8226950168609619, 0.6843971610069275, 0.7801418304443359, 0.8368794322013855, 0.609929084777832, 0.76241135597229, 0.8333333134651184, 0.6560283899307251, 0.783687949180603, 0.8191489577293396, 0.6808510422706604, 0.7695035338401794, 0.8368794322013855, 0.7659574747085571, 0.7943262457847595, 0.8333333134651184, 0.7234042286872864, 0.783687949180603, 0.8404255509376526, 0.7092198729515076, 0.8085106611251831, 0.8510638475418091, 0.6879432797431946, 0.804964542388916, 0.8758864998817444, 0.7765957713127136, 0.8191489577293396, 0.9007092118263245, 0.8723404407501221, 0.8368794322013855], 'recall_37': [0.6276595592498779, 0.3617021143436432, 0.5390070676803589, 0.6843971610069275, 0.41134750843048096, 0.6843971610069275, 0.7517730593681335, 0.5390070676803589, 0.741134762763977, 0.7517730593681335, 0.73758864402771, 0.7695035338401794, 0.5106382966041565, 0.7198581695556641, 0.783687949180603, 0.6453900933265686, 0.7695035338401794, 0.8333333134651184, 0.6489361524581909, 0.7695035338401794, 0.8226950168609619, 0.6843971610069275, 0.7801418304443359, 0.8368794322013855, 0.609929084777832, 0.76241135597229, 0.8333333134651184, 0.6560283899307251, 0.783687949180603, 0.8191489577293396, 0.6808510422706604, 0.7695035338401794, 0.8368794322013855, 0.7659574747085571, 0.7943262457847595, 0.8333333134651184, 0.7234042286872864, 0.783687949180603, 0.8404255509376526, 0.7092198729515076, 0.8085106611251831, 0.8510638475418091, 0.6879432797431946, 0.804964542388916, 0.8758864998817444, 0.7765957713127136, 0.8191489577293396, 0.9007092118263245, 0.8723404407501221, 0.8368794322013855]}\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f88acb2c440> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Fold 6 - Precison: 0.7875366568914957 - Recall: 0.4090909090909091%\n",
      "Shape of train set: (282, 10020)\n",
      "Shape of y: (282, 2)\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 7 ...\n",
      "Shape of validation set: (31, 10020)\n",
      "{'loss': [22.24107551574707, 131.91973876953125, 4.652421474456787, 11.526435852050781, 27.715890884399414, 6.828553199768066, 18.615102767944336, 17.28653335571289, 5.859803199768066, 5.395294189453125, 11.655725479125977, 3.8463871479034424, 4.993592739105225, 10.190401077270508, 3.6003053188323975, 3.1780638694763184, 8.579179763793945, 2.635603666305542, 3.4198601245880127, 7.922551155090332, 2.3801212310791016, 2.834646463394165, 6.857501983642578, 1.7628735303878784, 3.338780164718628, 6.916040420532227, 1.798554539680481, 2.640835762023926, 6.231626033782959, 1.434790015220642, 2.6826720237731934, 5.972437858581543, 1.4894193410873413, 2.3481814861297607, 5.104970455169678, 1.0159809589385986, 2.499634265899658, 5.4971923828125, 0.9668499827384949, 3.4064207077026367, 6.120072841644287, 1.1618105173110962, 1.8163127899169922, 4.385706424713135, 0.5305056571960449, 2.750011444091797, 5.077458381652832, 0.9627787470817566, 2.2813570499420166, 4.06866455078125], 'accuracy': [0.6631205677986145, 0.3191489279270172, 0.5815602540969849, 0.4893617033958435, 0.6808510422706604, 0.609929084777832, 0.43617022037506104, 0.6808510422706604, 0.7517730593681335, 0.5141844153404236, 0.716312050819397, 0.76241135597229, 0.5177304744720459, 0.7340425252914429, 0.7765957713127136, 0.5815602540969849, 0.7482269406318665, 0.7943262457847595, 0.588652491569519, 0.7482269406318665, 0.8120567202568054, 0.6205673813819885, 0.7659574747085571, 0.8333333134651184, 0.6241135001182556, 0.7695035338401794, 0.8333333134651184, 0.6241135001182556, 0.7695035338401794, 0.847517728805542, 0.6276595592498779, 0.7730496525764465, 0.8439716100692749, 0.6631205677986145, 0.7801418304443359, 0.8546099066734314, 0.652482271194458, 0.7730496525764465, 0.8404255509376526, 0.6418439745903015, 0.7517730593681335, 0.8723404407501221, 0.6914893388748169, 0.783687949180603, 0.8865247964859009, 0.6560283899307251, 0.7801418304443359, 0.8546099066734314, 0.7304964661598206, 0.8085106611251831], 'precision_38': [0.6631205677986145, 0.3191489279270172, 0.5815602540969849, 0.4893617033958435, 0.6808510422706604, 0.609929084777832, 0.43617022037506104, 0.6808510422706604, 0.7517730593681335, 0.5141844153404236, 0.716312050819397, 0.76241135597229, 0.5177304744720459, 0.7340425252914429, 0.7765957713127136, 0.5815602540969849, 0.7482269406318665, 0.7943262457847595, 0.588652491569519, 0.7482269406318665, 0.8120567202568054, 0.6205673813819885, 0.7659574747085571, 0.8333333134651184, 0.6241135001182556, 0.7695035338401794, 0.8333333134651184, 0.6241135001182556, 0.7695035338401794, 0.847517728805542, 0.6276595592498779, 0.7730496525764465, 0.8439716100692749, 0.6631205677986145, 0.7801418304443359, 0.8546099066734314, 0.652482271194458, 0.7730496525764465, 0.8404255509376526, 0.6418439745903015, 0.7517730593681335, 0.8723404407501221, 0.6914893388748169, 0.783687949180603, 0.8865247964859009, 0.6560283899307251, 0.7801418304443359, 0.8546099066734314, 0.7304964661598206, 0.8085106611251831], 'recall_38': [0.6631205677986145, 0.3191489279270172, 0.5815602540969849, 0.4893617033958435, 0.6808510422706604, 0.609929084777832, 0.43617022037506104, 0.6808510422706604, 0.7517730593681335, 0.5141844153404236, 0.716312050819397, 0.76241135597229, 0.5177304744720459, 0.7340425252914429, 0.7765957713127136, 0.5815602540969849, 0.7482269406318665, 0.7943262457847595, 0.588652491569519, 0.7482269406318665, 0.8120567202568054, 0.6205673813819885, 0.7659574747085571, 0.8333333134651184, 0.6241135001182556, 0.7695035338401794, 0.8333333134651184, 0.6241135001182556, 0.7695035338401794, 0.847517728805542, 0.6276595592498779, 0.7730496525764465, 0.8439716100692749, 0.6631205677986145, 0.7801418304443359, 0.8546099066734314, 0.652482271194458, 0.7730496525764465, 0.8404255509376526, 0.6418439745903015, 0.7517730593681335, 0.8723404407501221, 0.6914893388748169, 0.783687949180603, 0.8865247964859009, 0.6560283899307251, 0.7801418304443359, 0.8546099066734314, 0.7304964661598206, 0.8085106611251831]}\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f88ad38fb00> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "> Fold 7 - Precison: 0.7578952459254243 - Recall: 0.9565217391304348%\n",
      "Shape of train set: (282, 10020)\n",
      "Shape of y: (282, 2)\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 8 ...\n",
      "Shape of validation set: (31, 10020)\n",
      "{'loss': [17.178993225097656, 53.363304138183594, 22.38347816467285, 7.860077857971191, 18.601268768310547, 17.766311645507812, 8.692391395568848, 2.435920238494873, 8.014320373535156, 1.5901269912719727, 7.063347339630127, 10.80043888092041, 4.359669208526611, 2.883441686630249, 6.878572940826416, 1.4069846868515015, 3.6723196506500244, 8.779068946838379, 2.3028950691223145, 4.280395984649658, 7.217123031616211, 2.3222386837005615, 2.6367528438568115, 5.216519355773926, 1.0613425970077515, 2.6704204082489014, 5.5314741134643555, 1.4370871782302856, 2.292494297027588, 4.598260402679443, 0.7863770127296448, 1.9138257503509521, 3.9621949195861816, 0.4776645004749298, 1.26643967628479, 3.560000419616699, 0.31329989433288574, 0.249910369515419, 1.0796318054199219, 3.221281051635742, 5.802637577056885, 1.5563467741012573, 1.4005484580993652, 3.1209511756896973, 0.33542600274086, 0.2670876383781433, 0.5070453882217407, 1.7740213871002197, 5.115293025970459, 0.6673920154571533], 'accuracy': [0.3333333432674408, 0.6843971610069275, 0.6843971610069275, 0.5780141949653625, 0.3794326186180115, 0.6843971610069275, 0.7304964661598206, 0.6134752035140991, 0.7446808218955994, 0.6808510422706604, 0.5106382966041565, 0.7198581695556641, 0.758865237236023, 0.6170212626457214, 0.73758864402771, 0.76241135597229, 0.585106372833252, 0.7234042286872864, 0.7730496525764465, 0.5602836608886719, 0.7304964661598206, 0.7907801270484924, 0.6347517967224121, 0.76241135597229, 0.8226950168609619, 0.6418439745903015, 0.76241135597229, 0.8191489577293396, 0.6489361524581909, 0.7730496525764465, 0.8546099066734314, 0.6914893388748169, 0.7801418304443359, 0.8865247964859009, 0.7659574747085571, 0.783687949180603, 0.911347508430481, 0.9007092118263245, 0.8333333134651184, 0.563829779624939, 0.741134762763977, 0.8014184236526489, 0.741134762763977, 0.8014184236526489, 0.911347508430481, 0.9042553305625916, 0.8936170339584351, 0.7198581695556641, 0.7730496525764465, 0.8723404407501221], 'precision_39': [0.3333333432674408, 0.6843971610069275, 0.6843971610069275, 0.5780141949653625, 0.3794326186180115, 0.6843971610069275, 0.7304964661598206, 0.6134752035140991, 0.7446808218955994, 0.6808510422706604, 0.5106382966041565, 0.7198581695556641, 0.758865237236023, 0.6170212626457214, 0.73758864402771, 0.76241135597229, 0.585106372833252, 0.7234042286872864, 0.7730496525764465, 0.5602836608886719, 0.7304964661598206, 0.7907801270484924, 0.6347517967224121, 0.76241135597229, 0.8226950168609619, 0.6418439745903015, 0.76241135597229, 0.8191489577293396, 0.6489361524581909, 0.7730496525764465, 0.8546099066734314, 0.6914893388748169, 0.7801418304443359, 0.8865247964859009, 0.7659574747085571, 0.783687949180603, 0.911347508430481, 0.9007092118263245, 0.8333333134651184, 0.563829779624939, 0.741134762763977, 0.8014184236526489, 0.741134762763977, 0.8014184236526489, 0.911347508430481, 0.9042553305625916, 0.8936170339584351, 0.7198581695556641, 0.7730496525764465, 0.8723404407501221], 'recall_39': [0.3333333432674408, 0.6843971610069275, 0.6843971610069275, 0.5780141949653625, 0.3794326186180115, 0.6843971610069275, 0.7304964661598206, 0.6134752035140991, 0.7446808218955994, 0.6808510422706604, 0.5106382966041565, 0.7198581695556641, 0.758865237236023, 0.6170212626457214, 0.73758864402771, 0.76241135597229, 0.585106372833252, 0.7234042286872864, 0.7730496525764465, 0.5602836608886719, 0.7304964661598206, 0.7907801270484924, 0.6347517967224121, 0.76241135597229, 0.8226950168609619, 0.6418439745903015, 0.76241135597229, 0.8191489577293396, 0.6489361524581909, 0.7730496525764465, 0.8546099066734314, 0.6914893388748169, 0.7801418304443359, 0.8865247964859009, 0.7659574747085571, 0.783687949180603, 0.911347508430481, 0.9007092118263245, 0.8333333134651184, 0.563829779624939, 0.741134762763977, 0.8014184236526489, 0.741134762763977, 0.8014184236526489, 0.911347508430481, 0.9042553305625916, 0.8936170339584351, 0.7198581695556641, 0.7730496525764465, 0.8723404407501221]}\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f887c6274d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Fold 8 - Precison: 0.7622800586510264 - Recall: 0.3181818181818182%\n",
      "Shape of train set: (282, 10020)\n",
      "Shape of y: (282, 2)\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 9 ...\n",
      "Shape of validation set: (31, 10020)\n",
      "{'loss': [22.503007888793945, 117.48268127441406, 5.481495380401611, 30.58106803894043, 3.653019905090332, 18.48642349243164, 13.722098350524902, 2.2851946353912354, 9.671390533447266, 11.073265075683594, 2.553772449493408, 7.373686790466309, 8.97211742401123, 2.2478179931640625, 5.3828125, 7.551213264465332, 1.6559593677520752, 4.232172012329102, 6.939353942871094, 1.388141393661499, 4.116809844970703, 6.613546371459961, 1.2446094751358032, 3.5375850200653076, 6.681276798248291, 1.2686142921447754, 2.603816032409668, 5.619369029998779, 0.7248944640159607, 1.2940436601638794, 3.927027940750122, 0.6960406303405762, 2.432223320007324, 2.9065537452697754, 6.407992839813232, 0.9499929547309875, 3.763368844985962, 5.803196907043457, 1.2217406034469604, 2.713165283203125, 4.81079626083374, 0.7706507444381714, 1.4193506240844727, 3.5593035221099854, 0.559281587600708, 1.264766812324524, 2.3065459728240967, 4.400817394256592, 0.47760170698165894, 0.8218995332717896], 'accuracy': [0.6808510422706604, 0.3191489279270172, 0.4609929025173187, 0.6808510422706604, 0.6382978558540344, 0.3758865296840668, 0.6808510422706604, 0.695035457611084, 0.478723406791687, 0.695035457611084, 0.7695035338401794, 0.5460993051528931, 0.7234042286872864, 0.7695035338401794, 0.585106372833252, 0.73758864402771, 0.7907801270484924, 0.631205677986145, 0.7553191781044006, 0.804964542388916, 0.6241135001182556, 0.76241135597229, 0.8120567202568054, 0.6595744490623474, 0.76241135597229, 0.8226950168609619, 0.6843971610069275, 0.7765957713127136, 0.8510638475418091, 0.7801418304443359, 0.7907801270484924, 0.8510638475418091, 0.8191489577293396, 0.6418439745903015, 0.7482269406318665, 0.8156028389930725, 0.631205677986145, 0.783687949180603, 0.8510638475418091, 0.6666666865348816, 0.7872340679168701, 0.8758864998817444, 0.76241135597229, 0.8014184236526489, 0.8546099066734314, 0.8617021441459656, 0.695035457611084, 0.7872340679168701, 0.9042553305625916, 0.8156028389930725], 'precision_40': [0.6808510422706604, 0.3191489279270172, 0.4609929025173187, 0.6808510422706604, 0.6382978558540344, 0.3758865296840668, 0.6808510422706604, 0.695035457611084, 0.478723406791687, 0.695035457611084, 0.7695035338401794, 0.5460993051528931, 0.7234042286872864, 0.7695035338401794, 0.585106372833252, 0.73758864402771, 0.7907801270484924, 0.631205677986145, 0.7553191781044006, 0.804964542388916, 0.6241135001182556, 0.76241135597229, 0.8120567202568054, 0.6595744490623474, 0.76241135597229, 0.8226950168609619, 0.6843971610069275, 0.7765957713127136, 0.8510638475418091, 0.7801418304443359, 0.7907801270484924, 0.8510638475418091, 0.8191489577293396, 0.6418439745903015, 0.7482269406318665, 0.8156028389930725, 0.631205677986145, 0.783687949180603, 0.8510638475418091, 0.6666666865348816, 0.7872340679168701, 0.8758864998817444, 0.76241135597229, 0.8014184236526489, 0.8546099066734314, 0.8617021441459656, 0.695035457611084, 0.7872340679168701, 0.9042553305625916, 0.8156028389930725], 'recall_40': [0.6808510422706604, 0.3191489279270172, 0.4609929025173187, 0.6808510422706604, 0.6382978558540344, 0.3758865296840668, 0.6808510422706604, 0.695035457611084, 0.478723406791687, 0.695035457611084, 0.7695035338401794, 0.5460993051528931, 0.7234042286872864, 0.7695035338401794, 0.585106372833252, 0.73758864402771, 0.7907801270484924, 0.631205677986145, 0.7553191781044006, 0.804964542388916, 0.6241135001182556, 0.76241135597229, 0.8120567202568054, 0.6595744490623474, 0.76241135597229, 0.8226950168609619, 0.6843971610069275, 0.7765957713127136, 0.8510638475418091, 0.7801418304443359, 0.7907801270484924, 0.8510638475418091, 0.8191489577293396, 0.6418439745903015, 0.7482269406318665, 0.8156028389930725, 0.631205677986145, 0.783687949180603, 0.8510638475418091, 0.6666666865348816, 0.7872340679168701, 0.8758864998817444, 0.76241135597229, 0.8014184236526489, 0.8546099066734314, 0.8617021441459656, 0.695035457611084, 0.7872340679168701, 0.9042553305625916, 0.8156028389930725]}\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f88873cd9e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "> Fold 9 - Precison: 0.7256855443246119 - Recall: 0.9130434782608695%\n",
      "Shape of train set: (282, 10020)\n",
      "Shape of y: (282, 2)\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 10 ...\n",
      "Shape of validation set: (31, 10020)\n",
      "{'loss': [24.309913635253906, 196.88929748535156, 5.8822479248046875, 17.597639083862305, 12.265581130981445, 25.322694778442383, 10.84796142578125, 8.059218406677246, 19.93463134765625, 10.332364082336426, 2.3293087482452393, 4.155832290649414, 12.60787296295166, 3.9978978633880615, 7.772399425506592, 14.777726173400879, 6.427188396453857, 2.5190634727478027, 7.769146919250488, 1.263776421546936, 2.445807933807373, 10.024689674377441, 13.08029842376709, 4.999500274658203, 2.7374119758605957, 7.583362102508545, 1.069534182548523, 4.294061183929443, 11.19492244720459, 3.930692672729492, 3.148618459701538, 8.532306671142578, 2.0474660396575928, 4.9690117835998535, 10.982182502746582, 4.172586917877197, 1.6776001453399658, 5.502190113067627, 0.6401140689849854, 1.0555633306503296, 4.997995853424072, 10.026390075683594, 3.183279037475586, 2.492591142654419, 6.951760768890381, 1.1351242065429688, 4.868071556091309, 9.181330680847168, 3.055206775665283, 1.405832052230835], 'accuracy': [0.6879432797431946, 0.304964542388916, 0.563829779624939, 0.695035457611084, 0.3191489279270172, 0.695035457611084, 0.7340425252914429, 0.40780141949653625, 0.695035457611084, 0.758865237236023, 0.7340425252914429, 0.6170212626457214, 0.7269503474235535, 0.783687949180603, 0.4751773178577423, 0.7021276354789734, 0.7872340679168701, 0.631205677986145, 0.7553191781044006, 0.7553191781044006, 0.8120567202568054, 0.4716311991214752, 0.716312050819397, 0.7943262457847595, 0.7198581695556641, 0.7765957713127136, 0.847517728805542, 0.6063829660415649, 0.7553191781044006, 0.8191489577293396, 0.6843971610069275, 0.7765957713127136, 0.847517728805542, 0.5531914830207825, 0.7482269406318665, 0.8226950168609619, 0.7907801270484924, 0.8191489577293396, 0.8581560254096985, 0.8794326186180115, 0.588652491569519, 0.76241135597229, 0.8368794322013855, 0.716312050819397, 0.7872340679168701, 0.8723404407501221, 0.5921986103057861, 0.7695035338401794, 0.8404255509376526, 0.8226950168609619], 'precision_41': [0.6879432797431946, 0.304964542388916, 0.563829779624939, 0.695035457611084, 0.3191489279270172, 0.695035457611084, 0.7340425252914429, 0.40780141949653625, 0.695035457611084, 0.758865237236023, 0.7340425252914429, 0.6170212626457214, 0.7269503474235535, 0.783687949180603, 0.4751773178577423, 0.7021276354789734, 0.7872340679168701, 0.631205677986145, 0.7553191781044006, 0.7553191781044006, 0.8120567202568054, 0.4716311991214752, 0.716312050819397, 0.7943262457847595, 0.7198581695556641, 0.7765957713127136, 0.847517728805542, 0.6063829660415649, 0.7553191781044006, 0.8191489577293396, 0.6843971610069275, 0.7765957713127136, 0.847517728805542, 0.5531914830207825, 0.7482269406318665, 0.8226950168609619, 0.7907801270484924, 0.8191489577293396, 0.8581560254096985, 0.8794326186180115, 0.588652491569519, 0.76241135597229, 0.8368794322013855, 0.716312050819397, 0.7872340679168701, 0.8723404407501221, 0.5921986103057861, 0.7695035338401794, 0.8404255509376526, 0.8226950168609619], 'recall_41': [0.6879432797431946, 0.304964542388916, 0.563829779624939, 0.695035457611084, 0.3191489279270172, 0.695035457611084, 0.7340425252914429, 0.40780141949653625, 0.695035457611084, 0.758865237236023, 0.7340425252914429, 0.6170212626457214, 0.7269503474235535, 0.783687949180603, 0.4751773178577423, 0.7021276354789734, 0.7872340679168701, 0.631205677986145, 0.7553191781044006, 0.7553191781044006, 0.8120567202568054, 0.4716311991214752, 0.716312050819397, 0.7943262457847595, 0.7198581695556641, 0.7765957713127136, 0.847517728805542, 0.6063829660415649, 0.7553191781044006, 0.8191489577293396, 0.6843971610069275, 0.7765957713127136, 0.847517728805542, 0.5531914830207825, 0.7482269406318665, 0.8226950168609619, 0.7907801270484924, 0.8191489577293396, 0.8581560254096985, 0.8794326186180115, 0.588652491569519, 0.76241135597229, 0.8368794322013855, 0.716312050819397, 0.7872340679168701, 0.8723404407501221, 0.5921986103057861, 0.7695035338401794, 0.8404255509376526, 0.8226950168609619]}\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f88da653cb0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Fold 10 - Precison: 0.6412806209071066 - Recall: 0.9473684210526315%\n",
      "------------------------------------------------------------------------\n",
      "> Precison: 0.7023702764588535 (+- 0.06146396945902786)\n",
      "> Recall: 0.7995943415851883\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "num_folds = 10\n",
    "# Define per-fold score containers <-- these are new\n",
    "acc_per_fold = []\n",
    "loss_per_fold = []\n",
    "\n",
    "# Define the K-fold Cross Validator\n",
    "kfold = KFold(n_splits=num_folds, shuffle=True)\n",
    "\n",
    "inputs = result\n",
    "targets = y\n",
    "\n",
    "print('# inputs data samples:', inputs.shape[0])\n",
    "print('# targets data samples:', targets.shape[0])\n",
    "\n",
    "# K-fold Cross Validation model evaluation\n",
    "fold_no = 1\n",
    "for train, test in kfold.split(inputs, targets):\n",
    "    \n",
    "#     tk = Tokenizer(num_words=NB_WORDS,\n",
    "#                filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "#                lower=True,\n",
    "#                split=\" \")\n",
    "#     tk.fit_on_texts(inputs.iloc[train])\n",
    "    \n",
    "#     X_train_seq = tk.texts_to_sequences(inputs.iloc[train])\n",
    "#     X_test_seq = tk.texts_to_sequences(inputs.iloc[test])\n",
    "    \n",
    "    \n",
    "    X_train_oh = inputs.iloc[train]\n",
    "    X_test_oh = inputs.iloc[test]\n",
    "    \n",
    "    print('Shape of train set:',X_train_oh.shape)\n",
    "    \n",
    "    \n",
    "    y_train_le = targets.iloc[train]\n",
    "    y_train_oh = to_categorical(y_train_le)\n",
    "    \n",
    "    \n",
    "    y_test_le = targets.iloc[test]\n",
    "    y_test_oh = to_categorical(y_test_le)\n",
    "\n",
    "\n",
    "    \n",
    "    print('Shape of y:',y_train_oh.shape)\n",
    "\n",
    "    # Define the model architecture\n",
    "    base_model = models.Sequential()\n",
    "    base_model.add(layers.Dense(128, activation='relu', input_shape=(10020,)))\n",
    "    base_model.add(layers.Dense(64, activation='relu'))\n",
    "    base_model.add(layers.Dense(2, activation='softmax'))\n",
    "\n",
    "    # Compile the model\n",
    "    base_model.compile(optimizer='rmsprop'\n",
    "                  , loss='categorical_crossentropy'\n",
    "                  , metrics=['accuracy',tf.keras.metrics.Precision(),tf.keras.metrics.Recall()])\n",
    "    \n",
    "\n",
    "    # Generate a print\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'Training for fold {fold_no} ...')\n",
    "    print('Shape of validation set:',X_test_oh.shape)\n",
    "    \n",
    "    # Fit data to model\n",
    "    history = base_model.fit(X_train_oh\n",
    "                       ,y_train_oh\n",
    "                       , epochs=50\n",
    "                       , batch_size=BATCH_SIZE\n",
    "                       , verbose=0)\n",
    "    print(history.history)\n",
    "    \n",
    "\n",
    "    # Generate generalization metrics\n",
    "    from sklearn.metrics import classification_report\n",
    "    y_pred = base_model.predict_classes(X_test_oh)\n",
    "    \n",
    "    average_recall = recall_score(y_test_le, y_pred)\n",
    "    average_precision = average_precision_score(y_test_le, y_pred)\n",
    "    print(f'> Fold {fold_no} - Precison: {average_precision} - Recall: {average_recall}%')\n",
    "    \n",
    "    acc_per_fold.append(average_precision)\n",
    "    loss_per_fold.append(average_recall)\n",
    "\n",
    "    # Increase fold number\n",
    "    fold_no = fold_no + 1\n",
    "\n",
    "# == Provide average scores ==\n",
    "print('------------------------------------------------------------------------')\n",
    "\n",
    "print(f'> Precison: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
    "print(f'> Recall: {np.mean(loss_per_fold)}')\n",
    "print('------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier, KerasRegressor\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "\n",
    "base_model = models.Sequential()\n",
    "base_model.add(layers.Dense(128, activation='relu', input_shape=(10020,)))\n",
    "base_model.add(layers.Dense(64, activation='relu'))\n",
    "base_model.add(layers.Dense(2, activation='softmax'))\n",
    "\n",
    "X = result\n",
    "y = y\n",
    "\n",
    "my_model = KerasRegressor(build_fn=basemodel, **sk_params)    \n",
    "my_model.fit(X,y)\n",
    "\n",
    "perm = PermutationImportance(my_model, random_state=1).fit(X,y)\n",
    "eli5.show_weights(perm, feature_names = X.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_metric(history, metric_name):\n",
    "    metric = history.history[metric_name]\n",
    "    val_metric = history.history['val_' + metric_name+'_24']\n",
    "\n",
    "    e = range(1, NB_START_EPOCHS + 1)\n",
    "\n",
    "    plt.plot(e, metric, 'bo', label='Train ' + metric_name)\n",
    "    plt.plot(e, val_metric, 'b', label='Validation ' + metric_name)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.save(\"visual_mlp.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31, 10020)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_oh.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31,)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_le.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31, 10020)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_oh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shape mismatch: objects cannot be broadcast to a single shape",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-117-f54014f284ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Random Forest feature importance via permutation importance w. std. dev.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m plt.bar(range(X.shape[1]), imp_vals[indices],\n\u001b[0;32m---> 18\u001b[0;31m         yerr=std[indices])\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mbar\u001b[0;34m(x, height, width, bottom, align, data, **kwargs)\u001b[0m\n\u001b[1;32m   2455\u001b[0m     return gca().bar(\n\u001b[1;32m   2456\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbottom\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbottom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malign\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malign\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2457\u001b[0;31m         **({\"data\": data} if data is not None else {}), **kwargs)\n\u001b[0m\u001b[1;32m   2458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1808\u001b[0m                         \u001b[0;34m\"the Matplotlib list!)\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlabel_namer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1809\u001b[0m                         RuntimeWarning, stacklevel=2)\n\u001b[0;32m-> 1810\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1812\u001b[0m         inner.__doc__ = _add_data_doc(inner.__doc__,\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mbar\u001b[0;34m(self, x, height, width, bottom, align, **kwargs)\u001b[0m\n\u001b[1;32m   2249\u001b[0m         x, height, width, y, linewidth = np.broadcast_arrays(\n\u001b[1;32m   2250\u001b[0m             \u001b[0;31m# Make args iterable too.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2251\u001b[0;31m             np.atleast_1d(x), height, width, y, linewidth)\n\u001b[0m\u001b[1;32m   2252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2253\u001b[0m         \u001b[0;31m# Now that units have been converted, set the tick locations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mbroadcast_arrays\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/numpy/lib/stride_tricks.py\u001b[0m in \u001b[0;36mbroadcast_arrays\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_m\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubok\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_m\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m     \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_broadcast_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/numpy/lib/stride_tricks.py\u001b[0m in \u001b[0;36m_broadcast_shape\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;31m# use the old-iterator because np.nditer does not handle size 0 arrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[0;31m# consistently\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m     \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m     \u001b[0;31m# unfortunately, it cannot handle 32 or more arguments directly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mpos\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m31\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shape mismatch: objects cannot be broadcast to a single shape"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from mlxtend.evaluate import feature_importance_permutation\n",
    "from sklearn.metrics import f1_score\n",
    "imp_vals, imp_all = feature_importance_permutation(\n",
    "    predict_method= base_model.predict_classes, \n",
    "    X=X_test_oh.values,\n",
    "    y=y_test_le,\n",
    "    metric=f1_score,\n",
    "    num_rounds=10,\n",
    "    seed=1)\n",
    "\n",
    "\n",
    "std = np.std(imp_all, axis=1)\n",
    "indices = np.argsort(imp_vals)[::-1]\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Random Forest feature importance via permutation importance w. std. dev.\")\n",
    "plt.bar(range(X.shape[1]), imp_vals[indices],\n",
    "        yerr=std[indices])\n",
    "plt.xticks(range(X.shape[1]), indices)\n",
    "plt.xlim([-1, X.shape[1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15     0\n",
       "23     0\n",
       "25     0\n",
       "27     0\n",
       "36     1\n",
       "39     0\n",
       "47     1\n",
       "51     1\n",
       "79     1\n",
       "107    1\n",
       "128    1\n",
       "130    1\n",
       "154    0\n",
       "165    1\n",
       "178    1\n",
       "183    1\n",
       "190    1\n",
       "195    0\n",
       "210    1\n",
       "214    0\n",
       "225    1\n",
       "235    1\n",
       "252    0\n",
       "271    0\n",
       "273    1\n",
       "293    0\n",
       "329    1\n",
       "337    0\n",
       "370    1\n",
       "384    1\n",
       "392    1\n",
       "Name: attitude, dtype: int64"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_le"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Train data samples: 509\n",
      "# Test data samples: 57\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df.video_description, df.Relevancy, test_size=0.1, random_state=37)\n",
    "print('# Train data samples:', X_train.shape[0])\n",
    "print('# Test data samples:', X_test.shape[0])\n",
    "assert X_train.shape[0] == y_train.shape[0]\n",
    "assert X_test.shape[0] == y_test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting words to numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the text as input for a model, we first need to convert the tweet's words into tokens, which simply means converting the words to integers that refer to an index in a dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will only keep the most frequent words in the train set.\n",
    "\n",
    "We clean up the text by applying filters and putting the words to lowercase. Words are separated by spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted tokenizer on 509 documents\n",
      "10000 words in dictionary\n",
      "Top 5 most common words are: [('coronavirus', 624), ('news', 521), ('us', 271), ('follow', 260), ('subscribe', 255), ('channel', 255), ('virus', 239), ('watch', 230), ('5g', 215), ('dr', 215), ('covid', 214), ('twitter', 214), ('video', 211), ('not', 206), ('like', 205), ('facebook', 204), ('19', 201), ('this', 184), ('people', 183), ('playlist', 181)]\n"
     ]
    }
   ],
   "source": [
    "tk = Tokenizer(num_words=NB_WORDS,\n",
    "               filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "               lower=True,\n",
    "               split=\" \")\n",
    "tk.fit_on_texts(X_train)\n",
    "\n",
    "print('Fitted tokenizer on {} documents'.format(tk.document_count))\n",
    "print('{} words in dictionary'.format(tk.num_words))\n",
    "print('Top 5 most common words are:', collections.Counter(tk.word_counts).most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having created the dictionary we can convert the text to a list of integer indexes. This is done with the text_to_sequences method of the Tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"With theories circulating online 5G technology blame COVID-19 pandemic, experts present facts behind claims. Subscribe 7NEWS latest video ¬ª  Connect 7NEWS online Visit ¬ª  Facebook ¬ª  Twitter ¬ª  Instagram ¬ª  #BreakingNews #coronavirus #COVID19 #7NEWS\" is converted into [2045, 2546, 2547, 3245, 1761, 2548, 1039, 4683, 33, 124, 582, 83, 111, 818, 1, 220, 221, 115, 3246, 32, 120, 1762, 236, 486, 765, 1, 263, 660]\n"
     ]
    }
   ],
   "source": [
    "X_train_seq = tk.texts_to_sequences(X_train)\n",
    "X_test_seq = tk.texts_to_sequences(X_test)\n",
    "\n",
    "print('\"{}\" is converted into {}'.format(X_train[0], X_train_seq[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These integers should now be converted into a one-hot encoded features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_seq(seqs, nb_features = NB_WORDS):\n",
    "    ohs = np.zeros((len(seqs), nb_features))\n",
    "    for i, s in enumerate(seqs):\n",
    "        ohs[i, s] = 1.\n",
    "    return ohs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting the target classes to numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"1\" is converted into 1\n",
      "\"1\" is converted into [0. 1.]\n"
     ]
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "y_train_le = le.fit_transform(y_train)\n",
    "y_test_le = le.transform(y_test)\n",
    "y_train_oh = to_categorical(y_train_le)\n",
    "y_test_oh = to_categorical(y_test_le)\n",
    "\n",
    "print('\"{}\" is converted into {}'.format(y_train[0], y_train_le[0]))\n",
    "print('\"{}\" is converted into {}'.format(y_train_le[0], y_train_oh[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting of a validation set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of validation set: (51, 10000)\n"
     ]
    }
   ],
   "source": [
    "X_train_rest, X_valid, y_train_rest, y_valid = train_test_split(X_train_oh, y_train_oh, test_size=0.1, random_state=37)\n",
    "\n",
    "assert X_valid.shape[0] == y_valid.shape[0]\n",
    "assert X_train_rest.shape[0] == y_train_rest.shape[0]\n",
    "\n",
    "print('Shape of validation set:',X_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Implementation\n",
    "## Baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with a model with 2 densely connected layers of 64 hidden elements. The input_shape for the first layer is equal to the number of words we allowed in the dictionary and for which we created one-hot-encoded features.\n",
    "\n",
    " The softmax activation function makes sure the three probabilities sum up to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 64)                640064    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 644,354\n",
      "Trainable params: 644,354\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "base_model = models.Sequential()\n",
    "base_model.add(layers.Dense(64, activation='relu', input_shape=(NB_WORDS,)))\n",
    "base_model.add(layers.Dense(64, activation='relu'))\n",
    "base_model.add(layers.Dense(2, activation='softmax'))\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because this project is a multi-class, single-label prediction, we use categorical_crossentropy as the loss function and softmax as the final activation function. We fit the model on the remaining train data and validate on the validation set. We run for a predetermined number of epochs and will see when the model starts to overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_model(model):\n",
    "    model.compile(optimizer='rmsprop'\n",
    "                  , loss='categorical_crossentropy'\n",
    "                  , metrics=['accuracy',tf.keras.metrics.Precision(),tf.keras.metrics.Recall()])\n",
    "    \n",
    "    history = model.fit(X_train_rest\n",
    "                       , y_train_rest\n",
    "                       , epochs=NB_START_EPOCHS\n",
    "                       , batch_size=BATCH_SIZE\n",
    "                       , validation_data=(X_valid, y_valid)\n",
    "                       , verbose=0)\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.7070090174674988,\n",
       "  0.5174614787101746,\n",
       "  0.3788684904575348,\n",
       "  0.3041400909423828,\n",
       "  0.25723156332969666,\n",
       "  0.22602522373199463,\n",
       "  0.2032897025346756,\n",
       "  0.1780741959810257,\n",
       "  0.1627146452665329,\n",
       "  0.1489810198545456,\n",
       "  0.13913194835186005,\n",
       "  0.12844568490982056,\n",
       "  0.11971041560173035,\n",
       "  0.11082577705383301,\n",
       "  0.10460763424634933,\n",
       "  0.09958921372890472,\n",
       "  0.09780213981866837,\n",
       "  0.09410517662763596,\n",
       "  0.089164137840271,\n",
       "  0.08444852381944656],\n",
       " 'accuracy': [0.3995633125305176,\n",
       "  0.8253275156021118,\n",
       "  0.8668122291564941,\n",
       "  0.9017467498779297,\n",
       "  0.9366812109947205,\n",
       "  0.9344978332519531,\n",
       "  0.9410480260848999,\n",
       "  0.9454148411750793,\n",
       "  0.9563318490982056,\n",
       "  0.9541484713554382,\n",
       "  0.9563318490982056,\n",
       "  0.9563318490982056,\n",
       "  0.960698664188385,\n",
       "  0.960698664188385,\n",
       "  0.960698664188385,\n",
       "  0.960698664188385,\n",
       "  0.9628821015357971,\n",
       "  0.9628821015357971,\n",
       "  0.9628821015357971,\n",
       "  0.9628821015357971],\n",
       " 'precision': [0.39427313208580017,\n",
       "  0.8253275156021118,\n",
       "  0.8668122291564941,\n",
       "  0.9017467498779297,\n",
       "  0.9366812109947205,\n",
       "  0.9344978332519531,\n",
       "  0.9410480260848999,\n",
       "  0.9454148411750793,\n",
       "  0.9563318490982056,\n",
       "  0.9541484713554382,\n",
       "  0.9563318490982056,\n",
       "  0.9563318490982056,\n",
       "  0.960698664188385,\n",
       "  0.960698664188385,\n",
       "  0.960698664188385,\n",
       "  0.960698664188385,\n",
       "  0.9628821015357971,\n",
       "  0.9628821015357971,\n",
       "  0.9628821015357971,\n",
       "  0.9628821015357971],\n",
       " 'recall': [0.3908296823501587,\n",
       "  0.8253275156021118,\n",
       "  0.8668122291564941,\n",
       "  0.9017467498779297,\n",
       "  0.9366812109947205,\n",
       "  0.9344978332519531,\n",
       "  0.9410480260848999,\n",
       "  0.9454148411750793,\n",
       "  0.9563318490982056,\n",
       "  0.9541484713554382,\n",
       "  0.9563318490982056,\n",
       "  0.9563318490982056,\n",
       "  0.960698664188385,\n",
       "  0.960698664188385,\n",
       "  0.960698664188385,\n",
       "  0.960698664188385,\n",
       "  0.9628821015357971,\n",
       "  0.9628821015357971,\n",
       "  0.9628821015357971,\n",
       "  0.9628821015357971],\n",
       " 'val_loss': [0.5335533022880554,\n",
       "  0.43902894854545593,\n",
       "  0.3966190814971924,\n",
       "  0.36304351687431335,\n",
       "  0.36193162202835083,\n",
       "  0.3229401409626007,\n",
       "  0.33019882440567017,\n",
       "  0.3119520843029022,\n",
       "  0.3339370787143707,\n",
       "  0.302752822637558,\n",
       "  0.33394140005111694,\n",
       "  0.2985526919364929,\n",
       "  0.32714682817459106,\n",
       "  0.30503079295158386,\n",
       "  0.3339540362358093,\n",
       "  0.29546359181404114,\n",
       "  0.3473667800426483,\n",
       "  0.3011917769908905,\n",
       "  0.33582183718681335,\n",
       "  0.30534660816192627],\n",
       " 'val_accuracy': [0.9019607901573181,\n",
       "  0.9019607901573181,\n",
       "  0.9019607901573181,\n",
       "  0.9019607901573181,\n",
       "  0.9019607901573181,\n",
       "  0.8823529481887817,\n",
       "  0.9019607901573181,\n",
       "  0.9019607901573181,\n",
       "  0.9019607901573181,\n",
       "  0.9019607901573181,\n",
       "  0.9019607901573181,\n",
       "  0.9019607901573181,\n",
       "  0.8823529481887817,\n",
       "  0.9019607901573181,\n",
       "  0.9019607901573181,\n",
       "  0.9019607901573181,\n",
       "  0.9019607901573181,\n",
       "  0.9019607901573181,\n",
       "  0.9019607901573181,\n",
       "  0.9019607901573181],\n",
       " 'val_precision': [0.9019607901573181,\n",
       "  0.9019607901573181,\n",
       "  0.9019607901573181,\n",
       "  0.9019607901573181,\n",
       "  0.9019607901573181,\n",
       "  0.8823529481887817,\n",
       "  0.9019607901573181,\n",
       "  0.9019607901573181,\n",
       "  0.9019607901573181,\n",
       "  0.9019607901573181,\n",
       "  0.9019607901573181,\n",
       "  0.9019607901573181,\n",
       "  0.8823529481887817,\n",
       "  0.9019607901573181,\n",
       "  0.9019607901573181,\n",
       "  0.9019607901573181,\n",
       "  0.9019607901573181,\n",
       "  0.9019607901573181,\n",
       "  0.9019607901573181,\n",
       "  0.9019607901573181],\n",
       " 'val_recall': [0.9019607901573181,\n",
       "  0.9019607901573181,\n",
       "  0.9019607901573181,\n",
       "  0.9019607901573181,\n",
       "  0.9019607901573181,\n",
       "  0.8823529481887817,\n",
       "  0.9019607901573181,\n",
       "  0.9019607901573181,\n",
       "  0.9019607901573181,\n",
       "  0.9019607901573181,\n",
       "  0.9019607901573181,\n",
       "  0.9019607901573181,\n",
       "  0.8823529481887817,\n",
       "  0.9019607901573181,\n",
       "  0.9019607901573181,\n",
       "  0.9019607901573181,\n",
       "  0.9019607901573181,\n",
       "  0.9019607901573181,\n",
       "  0.9019607901573181,\n",
       "  0.9019607901573181]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_history = first_model(base_model)\n",
    "base_history.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the model performance, we will look at the training and validation loss and accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Validation\n",
    "\n",
    "lets check the model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, epoch_stop):\n",
    "    model.fit(X_train_oh\n",
    "              , y_train_oh\n",
    "              , epochs=epoch_stop\n",
    "              , batch_size=BATCH_SIZE\n",
    "              , verbose=0)\n",
    "    results = model.evaluate(X_test_oh, y_test_oh)\n",
    "    \n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 1ms/step - loss: 0.8903 - accuracy: 0.8947 - precision_1: 0.8947 - recall_1: 0.8947\n",
      "[0.8902625441551208, 0.8947368264198303, 0.8947368264198303, 0.8947368264198303]\n",
      "Test accuracy of baseline model: 89.47%\n"
     ]
    }
   ],
   "source": [
    "test_results = test_model(base_model,30)\n",
    "print(test_results)\n",
    "print('Test accuracy of baseline model: {0:.2f}%'.format(test_results[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 0 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0 0 1 1\n",
      " 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "y_score = base_model.predict_classes(X_test_oh)\n",
    "print(y_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.75      0.75        12\n",
      "           1       0.93      0.93      0.93        45\n",
      "\n",
      "    accuracy                           0.89        57\n",
      "   macro avg       0.84      0.84      0.84        57\n",
      "weighted avg       0.89      0.89      0.89        57\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = base_model.predict_classes(X_test_oh)\n",
    "print(y_test.ndim)\n",
    "print(classification_report(y_test_le, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9237426900584795\n",
      "0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "\n",
    "average_precision = average_precision_score(y_test_le, y_pred)\n",
    "print(average_precision)\n",
    "average_recall = recall_score(y_test_le, y_pred)\n",
    "print(average_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_arr = np.column_stack((y_test_le, y_pred)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1],\n",
       "       [1, 1],\n",
       "       [1, 1],\n",
       "       [1, 1],\n",
       "       [1, 1],\n",
       "       [0, 0],\n",
       "       [1, 1],\n",
       "       [1, 1],\n",
       "       [1, 1],\n",
       "       [1, 1],\n",
       "       [1, 1],\n",
       "       [1, 1],\n",
       "       [1, 1],\n",
       "       [1, 0],\n",
       "       [0, 0],\n",
       "       [1, 1],\n",
       "       [1, 1],\n",
       "       [1, 1],\n",
       "       [1, 0],\n",
       "       [1, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [1, 1],\n",
       "       [1, 1],\n",
       "       [1, 1],\n",
       "       [1, 1],\n",
       "       [1, 1],\n",
       "       [1, 0],\n",
       "       [1, 1],\n",
       "       [1, 1],\n",
       "       [1, 1],\n",
       "       [0, 0],\n",
       "       [1, 1],\n",
       "       [0, 0],\n",
       "       [0, 0],\n",
       "       [1, 1],\n",
       "       [1, 1],\n",
       "       [0, 1],\n",
       "       [1, 1],\n",
       "       [0, 0],\n",
       "       [1, 1],\n",
       "       [1, 1],\n",
       "       [0, 0],\n",
       "       [1, 1],\n",
       "       [1, 1],\n",
       "       [1, 1],\n",
       "       [1, 1],\n",
       "       [1, 1],\n",
       "       [1, 1],\n",
       "       [1, 1],\n",
       "       [1, 1],\n",
       "       [1, 1],\n",
       "       [1, 1],\n",
       "       [1, 1],\n",
       "       [1, 1],\n",
       "       [0, 0],\n",
       "       [0, 0]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
