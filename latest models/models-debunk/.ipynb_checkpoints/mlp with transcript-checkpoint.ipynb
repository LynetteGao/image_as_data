{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set-up of the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/lynette/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Basic packages\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import re\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "# Packages for data preparation\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Packages for modeling\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import regularizers\n",
    "\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set some parameters that will be used throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_WORDS = 10000  # Parameter indicating the number of words we'll put in the dictionary\n",
    "NB_START_EPOCHS = 30  # Number of epochs we usually start to train with\n",
    "BATCH_SIZE = 512  # Size of the batches used in the mini-batch gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords =\t{\n",
    "  \"5g coronavirus\": [\"5g\",\"corona\",\"coronavirus\"],\n",
    "  \"qanon coronavirus\": [\"qanon\",\"awakening\",\"corona\",\"coronavirus\"],\n",
    "  \"dean koontz darkness\": [\"dean\",\"darkness\",\"predict\",\"corona\",\"coronavirus\"],\n",
    "    \"judy mikovits\": [\"judy\",\"pandemic\",\"fauci\",\"corona\",\"coronavirus\"],\n",
    "    \"wuhan lab\": [\"wuhan\",\"lab\",\"laboratory\",\"corona\",\"coronavirus\"],\n",
    "    \"bioweapon coronavirus\" : [\"bioweapon\",\"weapon\",\"biological\",\"corona\",\"coronavirus\"],\n",
    "    \"bill gates coronavirus\": [\"bill\",\"gates\",\"vaccine\", \"vaccination\",\n",
    "                              \"chip\",\"plan\",\"corona\",\"coronavirus\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read in the csv with the tweets data and perform a random shuffle. It's a good practice to shuffle the data before splitting between a train and test set. We'll only keep the video decription column as input and the Relvancy column as the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'video_id', 'channel_title', 'channel_id',\n",
       "       'video_publish_date', 'video_title', 'video_description',\n",
       "       'video_category', 'video_view_count', 'video_comment_count',\n",
       "       'video_like_count', 'video_dislike_count', 'video_thumbnail',\n",
       "       'video_tags', 'collection_date', 'science.topic', 'Relevancy',\n",
       "       'attitude', 'Text/video', 'search.term', 'cld2', 'transcript',\n",
       "       'transcript_nchar', 'videoid', 'conspiracy', 'var_r', 'var_g', 'var_b',\n",
       "       'var_h', 'var_s', 'var_v', 'var_bright', 'var_bright_sd',\n",
       "       'var_contrast', 'var_colorful', 'median_r', 'median_g', 'median_b',\n",
       "       'median_h', 'median_s', 'median_v', 'median_bright', 'median_bright_sd',\n",
       "       'median_contrast', 'median_colorful', 'r_mean', 'g_mean', 'b_mean',\n",
       "       'h_mean', 's_mean', 'v_mean', 'bright_mean', 'lightning_mean',\n",
       "       'contrast_mean', 'colorful_mean', 'color_lag'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('handlabel_feature.csv')\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['transcript']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[(X[\"transcript\"] != 'CaptionUnavailable') & (X[\"transcript\"] != 'VideoUnavailable')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(313, 1)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "df['attitude'] = le.fit_transform(df['attitude'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0\n",
       "1      0\n",
       "2      0\n",
       "4      0\n",
       "5      1\n",
       "      ..\n",
       "402    1\n",
       "403    0\n",
       "404    0\n",
       "405    0\n",
       "406    0\n",
       "Name: attitude, Length: 313, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = df['attitude'][(df[\"transcript\"] != 'CaptionUnavailable') & (df[\"transcript\"] != 'VideoUnavailable')]\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we'll do is removing stopwords. These words do not have any value for predicting the sentiment.Also, we remove the http link in the texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transcript    coronavirus completely changing way o... know ...\n",
       "dtype: object"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_stopwords(input_text):\n",
    "    stopwords_list = stopwords.words('english')\n",
    "    stopwords_list.append('The')\n",
    "    # Some words which might indicate a certain sentiment are kept via a whitelist\n",
    "    whitelist = [\"n't\", \"not\", \"no\"]\n",
    "    words = str(input_text).split() \n",
    "    clean_words = [word for word in words if (word not in stopwords_list or word in whitelist) and len(word) > 1] \n",
    "    return \" \".join(clean_words) \n",
    "    \n",
    "def remove_mentions(input_text):\n",
    "    return re.sub(r'@\\w+', '', input_text)\n",
    "\n",
    "\n",
    "       \n",
    "df.transcript = df.transcript.apply(remove_stopwords).apply(remove_mentions).apply(remove_http)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extract key words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one dozen fires targeting 5g towers Netherlands country alone Cyprus Belgium Island among home two conspiracy theorists believe 5g may connection covert 19 UK reported around 50 fires targeting towers 5g equipment media reports broadband engineers harassed job sworn yo dad people taken rumours England's national medical director say nonsense worst kind fake news I'm absolutely outraged absolutely discussed people would taking action infrastructure need respond health emergency idea behind theory 5g radio waves harmful health experts slammed idea Australian cybersecurity expert called myth said wrong destructive w-h-o maintained 5g no consequences Public Health European authorities labeled disinformation one clear there's no Geographic correlation deployment 5g outbreak virus uphill battle rumors spread social media Facebook announced March would take posts peddling false claims conspiracy theories led anti 5g groups Australia UK shut YouTube also pledged clamp disinformation stopped videos appearing websites fight false theories set continue fear confusion taking extra toll world already midst crisis nicole ttyn cg TN\n",
      "127\n",
      "-----------\n",
      " 5g towers Netherlands country alone Cyprus Belgium Island among home two conspiracy theorists believe 5g may connection covert 19 UK5g may connection covert 19 UK reported around 50 fires targeting towers 5g equipment media reports broadband engineers harassed jobBelgium Island among home two conspiracy theorists believe 5g may connection covert 19 UK reported around 50 fires targeting towers5g equipment media reports broadband engineers harassed job sworn yo dad people taken rumours England's national medical director say nonsensekind fake news I'm absolutely outraged absolutely discussed people would taking action infrastructure need respond health emergency idea behind theory5g radio waves harmful health experts slammed idea Australian cybersecurity expert called myth said wrong destructive w-h-o maintained 5g nobehind theory 5g radio waves harmful health experts slammed idea Australian cybersecurity expert called myth said wrong destructive w-h-o maintained5g no consequences Public Health European authorities labeled disinformation one clear there's no Geographic correlation deployment 5g outbreak virus uphillwrong destructive w-h-o maintained 5g no consequences Public Health European authorities labeled disinformation one clear there's no Geographic correlation deployment5g outbreak virus uphill battle rumors spread social media Facebook announced March would take posts peddling false claims conspiracy theoriesvirus uphill battle rumors spread social media Facebook announced March would take posts peddling false claims conspiracy theories led anti5g groups Australia UK shut YouTube also pledged clamp disinformation stopped videos appearing websites fight false theories set continue fear\n"
     ]
    }
   ],
   "source": [
    "paragraph = df.loc[549].transcript\n",
    "print(paragraph)\n",
    "words = ['5g']\n",
    "result = \"\"\n",
    "for word in words:\n",
    "    result += ' '\n",
    "    wordlist = paragraph.split(\" \")\n",
    "    indices = [i for i, x in enumerate(wordlist) if x == word]\n",
    "#     index = wordlist.index(word)\n",
    "    print(index)\n",
    "    for index in indices:\n",
    "        first_part = wordlist[index-20:index]\n",
    "        second_part = wordlist[index:index+20]\n",
    "        result += \" \".join(first_part)\n",
    "        result += \" \".join(second_part)\n",
    "#     print(\"%s %s\" % (result.join(\" \".join(first_part)), \" \".join(second_part)))\n",
    "print('-----------')    \n",
    "print(result)\n",
    "\n",
    "\n",
    "def extract_keywords(input_text, words):\n",
    "    result = \"\"\n",
    "    for word in words:\n",
    "        wordlist = input_text.split(\" \")\n",
    "        indices = [i for i, x in enumerate(wordlist) if x.lower() == word]\n",
    "        if len(indices) == 0: continue\n",
    "        result += ' '\n",
    "        for index in indices:\n",
    "            first_part = wordlist[index-25:index]\n",
    "            second_part = wordlist[index:index+25]\n",
    "            result += \" \".join(first_part)\n",
    "            result += \" \".join(second_part) \n",
    "    if result == \"\": return input_text\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18     LESTER HOLT AND \"NBC NIGHTLY NEWS\" WILL HAVE F...\n",
      "547    last week I've become dangerous man digital wo...\n",
      "435    hey guys welcome show today we're gonna discus...\n",
      "560    government work us work yes they're serve us p...\n",
      "1      what's guys Stephen welcome back another video...\n",
      "                             ...                        \n",
      "562    um people believe 5g like like cell phones sta...\n",
      "340    5g 5g Network that's taking people completely ...\n",
      "551    fresh wave panic next-gen 5g technology appear...\n",
      "535    true caused 5g towers yes no please Lea people...\n",
      "2      5g inhaling steam fake vaccines latest dodgy c...\n",
      "Name: transcript, Length: 86, dtype: object\n",
      "18      IS CRACKING >> Victor: TWITTER IS CRACKING DO...\n",
      "547     lot people testing positive something call go...\n",
      "435     5g corona virus we're gonna talking pros cons...\n",
      "560    government work us work yes they're serve us p...\n",
      "1       5g weapon system maybe read news people reall...\n",
      "                             ...                        \n",
      "562     5g like like cell phones started kovat people...\n",
      "340     5g 5g Network that's taking people completely...\n",
      "551     5g technology appeared recent weeks linking t...\n",
      "535     5g towers yes no please Lea people no Pete th...\n",
      "2       5g inhaling steam fake vaccines latest dodgy ...\n",
      "Name: transcript, Length: 86, dtype: object\n",
      "50     what's YouTube Cody I'm Michelle show please l...\n",
      "71     right go video want able discuss videos we're ...\n",
      "64     hey what's everyone thanks joining today I've ...\n",
      "525    welcome back special video video I'm going tal...\n",
      "425    Shawn Morgan I'm author queue FAQ little mini ...\n",
      "466    welcome channel folks yesterday talked possibl...\n",
      "478    good evening Patriots pragmatic cue non broadc...\n",
      "334    corona virus outbreak totally ripe conspiracy ...\n",
      "65     right there's lot talk mark beast might good r...\n",
      "67     hi welcome listening x22 report name Dave epis...\n",
      "40     welcome back true seeker something important n...\n",
      "74     another great episode life pocket I'm hunter C...\n",
      "434    right let's talk something that's not architec...\n",
      "70     men's pleasure always ladies gentlemen thank a...\n",
      "39     hey everyone I'm I'm Shawn oh we've got bit ey...\n",
      "361    must politics aside stop partisanship unified ...\n",
      "72     I'm Shawn Morgan I'm asking hard questions alt...\n",
      "500    welcome effortless English show world's number...\n",
      "515    organ I'm creator queueing faq.com get free eb...\n",
      "62     anything hour want check Chrissy type things l...\n",
      "61     there's global reset works 66 priests arrested...\n",
      "42     actually questions mainstream media asking cor...\n",
      "58     fake news real people definitely real guys kno...\n",
      "335    last time talked Robert really fascinating thi...\n",
      "59     welcome Patriots around world I'm gene kale gu...\n",
      "280    welcome channel folks first want give massive ...\n",
      "68     I'm video vice media Vice actually done quite ...\n",
      "423    Q's postings maybe several talks ten days dark...\n",
      "54     conspiracy theorists suggested kovat 19 either...\n",
      "47     Unknown: So found link video medical doctor sk...\n",
      "73     good morning want give quick update think goin...\n",
      "57     thank joining livestream I'm actually probably...\n",
      "376    hey folks wanted follow-up video today done vi...\n",
      "49     believe Wendy's mass arrests happen martial la...\n",
      "46     order understand number four briefly explain s...\n",
      "497    guys seen inevitable underscore et Twitter thr...\n",
      "473    Sean wrote Qun FAQ check site free get free e-...\n",
      "44     he's everyone's Alchemist hear today want talk...\n",
      "76     cannabis combat yeah building lecture versus c...\n",
      "53     okay finally I've figured live stream yes basi...\n",
      "66     we've learned food we've learned things time w...\n",
      "386    welcome channel folks there's lot coming latel...\n",
      "52     hey everybody we're gonna start Trump's famous...\n",
      "443    fundamental duty ever question authority cruci...\n",
      "75     okay got piece news mouth bees BBC mainstream ...\n",
      "367    Shawn cue nonfat calm made mini e-book called ...\n",
      "63     hello everyone Joe 14 today I'm gonna talk wha...\n",
      "277    right there's lot talk mark beast might good r...\n",
      "Name: transcript, dtype: object\n",
      "50      HN exposing shit like somebody wrote internet...\n",
      "71      account videos must go viral must share make ...\n",
      "64      even know worth started think I'm like even s...\n",
      "525     different things things going things heard th...\n",
      "425     free  anon FAQ calm I'm watching looking thre...\n",
      "466     others come forward caught yet much Tom Hanks...\n",
      "478     read actually Wikipedia said January 16th act...\n",
      "334     fighting Satanic deep stay includes Hillary C...\n",
      "65      new wealth new world light Luciferian system ...\n",
      "67      News says bar disappointed partisan attacks l...\n",
      "40      event 201 October 18 2019 day leaving 74 days...\n",
      "74      largely eventuate let's start virus know guys...\n",
      "434     go whoever came thing adrenochrome true not e...\n",
      "70      example certainly less cryptic right god bles...\n",
      "39      bit eye-opener today well eye-opener say know...\n",
      "361     mercury goes fast he's gonna loop right middl...\n",
      "72      shown 2010 huge study can't think journal sit...\n",
      "500     serious problems going fine nothing guys may ...\n",
      "515     know independent journalist turn career like ...\n",
      "62     anything hour want check Chrissy type things l...\n",
      "61     there's global reset works 66 priests arrested...\n",
      "42      community chat not like places chance people ...\n",
      "58      look people like Madonna Ellen DeGeneres Tom ...\n",
      "335     corona virus needs full investigation origins...\n",
      "59      drop social media dummy account talk people s...\n",
      "280     Donald Trump taking care Deep States cleaning...\n",
      "68     I'm video vice media Vice actually done quite ...\n",
      "423     corona virus March 17 2020 there's queuing re...\n",
      "54     conspiracy theorists suggested kovat 19 either...\n",
      "47      people offering gold bounty anyone give proof...\n",
      "73      coronavirus thing know who's gonna watch vide...\n",
      "57      really negative responses people people reall...\n",
      "376     might find interesting I've said look things ...\n",
      "49      coronavirus president United States many doct...\n",
      "46      altered long period time human cells human gu...\n",
      "497     white hands going use evil that's point right...\n",
      "473     dictatorship really know what's going inside ...\n",
      "44      Awakening 2020 David Wilcock proposing happen...\n",
      "76      network deep state what's fuck 800,000 childr...\n",
      "53      operation way actually open source intelligen...\n",
      "66      guys lock DC possibly go really start arresti...\n",
      "386     see symbol party know Democratic Party replac...\n",
      "52     hey everybody we're gonna start Trump's famous...\n",
      "443     information like uncle give false information...\n",
      "75      bees BBC mainstream media fake news reporting...\n",
      "367     one David Spade okay David Spade video note d...\n",
      "63      corona virus quarantine lockdown know what's ...\n",
      "277     new wealth new world light Luciferian system ...\n",
      "Name: transcript, dtype: object\n",
      "161    okay I've got couple books want everybody awar...\n",
      "170    understand zombie said go back 20 months aroun...\n",
      "160    man computer suddenly turned keyboard started ...\n",
      "165    click video real quick want y'all see right kn...\n",
      "168                      thank watching forget subscribe\n",
      "171    yep that's two babies two babies Roga cricket ...\n",
      "156    author Dean Koons predict coronavirus 1981 nov...\n",
      "173    hello everyone watch output today I'm going ta...\n",
      "166                                                     \n",
      "321    hi guys welcome another video konami pack move...\n",
      "287    1981 dean koontz book eyes darkness includes b...\n",
      "281    welcome channel friends want thank dropping qu...\n",
      "460    hello friends back another interesting fun vid...\n",
      "310    hey everybody Geneva uh genius cause hot show ...\n",
      "175    hey how's going y'all entertainment headquarte...\n",
      "155    hello everybody hope fun home house moment tod...\n",
      "444    hey everyone that's not even welcome child oh ...\n",
      "496    Coronavirus predicted novelist 'The Eyes Darkn...\n",
      "179    hello guys welcome back small tech guru Tube c...\n",
      "180    hello guys good morning today would like expla...\n",
      "157    subscribe channel clicking subscribe button cl...\n",
      "279    hello everybody welcome back another video vid...\n",
      "395    hey what's guys make sure become champion join...\n",
      "176    welcome channel friends going quickly clear co...\n",
      "Name: transcript, dtype: object\n",
      "161     quickly arrived says here's later come back d...\n",
      "170    understand zombie said go back 20 months aroun...\n",
      "160    man computer suddenly turned keyboard started ...\n",
      "165     like reported certain news sites insane right...\n",
      "168                      thank watching forget subscribe\n",
      "171     vanish quickly arrived attack 10 years later ...\n",
      "156     Dean Koons predict coronavirus 1981 novel eye...\n",
      "173     people able take uni virus turn incident one ...\n",
      "166                                                     \n",
      "321     normal dimension monotonic author initially c...\n",
      "287     1981dean koontz book eyes darkness includes b...\n",
      "281     friends want thank dropping quick book review...\n",
      "460     attacking lungs bronchial tubes resisting kno...\n",
      "310     problem stop keep putting news scaring everyb...\n",
      "175     Dean Kunitz funny last name like coon Dean Ku...\n",
      "155     Dean Koontz came righty recently kind predict...\n",
      "444     hey everyone that's not even welcome child oh...\n",
      "496     Dean Koontz. It mentions biological weapon si...\n",
      "179     blocker away respira no.1 400 feet Atari Stoc...\n",
      "180     little bit opinion apologize i'm wrong i'm al...\n",
      "157     also billahi min ash-shaytani r-rajim bismill...\n",
      "279     Dean Koontz's eyes darkness back corona virus...\n",
      "395     theory think like conspiracy theory video fou...\n",
      "176     Dean Koons and/or end-of-days Sylvia Browne a...\n",
      "Name: transcript, dtype: object\n",
      "202    everything powers destroy life correct arreste...\n",
      "186    right we're gonna talk plan Demick we're gonna...\n",
      "216    online past days probably seen plan Demick upc...\n",
      "189    sound like piece family brother resist I'm I'm...\n",
      "451    heartland America Gateway West good morning go...\n",
      "                             ...                        \n",
      "199    pandemic documentary hidden agenda fact check ...\n",
      "193    Wuhan capital Hubei Province largest city cent...\n",
      "218    today special guest show brilliant researcher ...\n",
      "215    clear anti-vaccine position doctor apart lack ...\n",
      "410    welcome scientifically informed insider look m...\n",
      "Name: transcript, Length: 67, dtype: object\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202     gag order released write book called plague c...\n",
      "186     bad also kovat 19 bad Italy lot dogs also oce...\n",
      "216     coronavirus giant hoax whether agree film not...\n",
      "189     Judy Mike gagged basically gag order US gover...\n",
      "451     welcome back coast coast Georgia Dory we're g...\n",
      "                             ...                        \n",
      "199     tension recently documentary going viral soci...\n",
      "193     species including envelope gp41 HIV IVs gp41 ...\n",
      "218     viruses 40 years former aide scientist conduc...\n",
      "215     video lasts 30 minutes finished see moment ag...\n",
      "410     nothing wrong love sunshine think I'll stick ...\n",
      "Name: transcript, Length: 67, dtype: object\n",
      "302    Wuhan seafood market January 1st shut many ear...\n",
      "240    we're looking exactly came came happened separ...\n",
      "409    bombshell report today we're going reading new...\n",
      "441    eight medical staff diagnosed ccp virus one Ha...\n",
      "449                                                     \n",
      "237    right let's go story um Obama Barack Obama rig...\n",
      "255    corona virus pandemic begin Chinese laughs u.s...\n",
      "247    WHATEVER WORD? >> WHEN IT COMES TO RETALIATORY...\n",
      "263    Chinese government officials told Syst citizen...\n",
      "238    30th October 2019 Frederic news Post reported ...\n",
      "495    Hey, welcome back everyone. We're broadcasting...\n",
      "265    going make video timeline corona virus outbrea...\n",
      "380    US intelligence saying week coronavirus likely...\n",
      "453    sources telling Fox News today United States g...\n",
      "250    far theory official charge coronavirus came la...\n",
      "297    MARIA: WELCOME BACK BREAKING MARIA: WELCOME BA...\n",
      "256    we're getting laboratory nuclear daksa testing...\n",
      "498    Joe Rogan experience seen shit points fact cam...\n",
      "501    evidence becoming clearer clearer pointing ide...\n",
      "248    mr. president could said Sunday would strong r...\n",
      "239    welcome topic news I'm Tatiana darzee China em...\n",
      "330    cover story tonight doctor Wuhan probably not ...\n",
      "462    girag experience read really disturbing thing ...\n",
      "231    widely shared social media post claims Japanes...\n",
      "492    welcome back everyone I'm filming apartment Qu...\n",
      "350    CORONAVIRUS TASK FORCE BRIEFING AT THE WHITE H...\n",
      "252    EQUIPMENT FROM ALL AROUND THE WORLD. WORLD. TU...\n",
      "253    welcome state calamity show today we're going ...\n",
      "426    watch CNN watch fox watch CBS case there's lot...\n",
      "261    rebel news exclusive Justin Trudeau's governme...\n",
      "232    seems like they've interesting developments qu...\n",
      "440    there's little doubt corona virus pandemic beg...\n",
      "233    wanted research together colleagues identify o...\n",
      "301    one two things happened I'd either know compet...\n",
      "236    >>> INTELLIGENCE AGENCIES TODAY >>> INTELLIGEN...\n",
      "254    I've told Chinese government's high security v...\n",
      "245    well producer said he'd like say big smile wel...\n",
      "347    we're gonna see happens we're going see happen...\n",
      "342    Chinese regime claiming no new virus cases epi...\n",
      "433    well issue virus come becoming increasingly po...\n",
      "351    THAT APPEAR TO HAVE BEEN REMOVED FROM THE INTE...\n",
      "258    Easter weekend arsonist set fire dozens 5g cel...\n",
      "244    hey welcome back everyone we're broadcasting a...\n",
      "228    there's little doubt corona virus pandemic beg...\n",
      "264    shocking news origins corona virus 22 million ...\n",
      "407    China's foreign ministry Thursday addressed ac...\n",
      "242    mr. president US intelligence saying week coro...\n",
      "243    Secretary State Mike Pompeo mentioned linking ...\n",
      "257    there's battle President Donald Trump dr. pouc...\n",
      "290    here's something else that's making news US ar...\n",
      "355    lockdown measures place Chinese cities comes w...\n",
      "226    think I'm beginning understand dr. faux Qi use...\n",
      "229    we're gonna see happens we're going see happen...\n",
      "260    turn accusations China covered extent virus ou...\n",
      "273    thank earlier today jay Inslee said tweets enc...\n",
      "457    think I'm beginning understand dr. faux Qi use...\n",
      "Name: transcript, dtype: object\n",
      "302     Wuhan seafood market January 1st shut many ea...\n",
      "240     Wuhan Institute biology origin virus yes yes ...\n",
      "409     newsweek think helps talk openly dr fauci rem...\n",
      "441     exclusive recording obtained NTD reveals poor...\n",
      "449                                                     \n",
      "237     Wuhan want peddle conspiracies know we'll get...\n",
      "255     Chappell corona virus begin Chinese lab ongoi...\n",
      "247     PRESIDENT SAID YESTERDAY THAT HE HAS HIGH DEG...\n",
      "263     Wuhan good scapegoat escape bat case may gove...\n",
      "238     broke China Europe America shown chart first ...\n",
      "495     origin virus not written off. I'll go informa...\n",
      "265     started Wu Han lab rumor suddenly become fron...\n",
      "380     Wuhan there's also another report NIH Obama a...\n",
      "453     Wuhan lack safety protocols intern infected l...\n",
      "250     Wuhan US government saying record we've cover...\n",
      "297     THE END OF THE PROCESS. MARIA: SO THEY DOWNPL...\n",
      "256     Wuhan constructed 12 days stood running month...\n",
      "498     came lab well know what's funny talking can't...\n",
      "501     not quite enough let's go alright feels lot b...\n",
      "248    mr. president could said Sunday would strong r...\n",
      "239     Wuhan Institute virology secretly deleted pho...\n",
      "330     Wuhan probably not heard story hidden mighty ...\n",
      "462     sensationalistic publication right they're pu...\n",
      "231     Wuhan laboratory. You may recall Wuhan outbre...\n",
      "492     Committee also announced launch investigation...\n",
      "350     THE INTELLIGENCE WAS INCONCLUSIVE BUT THERE I...\n",
      "252     TUCKER? >> Tucker: SEEMS LIKELY. >> Tucker: S...\n",
      "253     Wuhan China mentioned previous episode time a...\n",
      "426     soon realized much story told today corona vi...\n",
      "261     Wuhan China virus lab suspected source deadly...\n",
      "232     Wuhan coronavirus come covered 19 coronavirus...\n",
      "440     corona virus pandemic began China also man-ma...\n",
      "233     bat coronavirus that's closely related us app...\n",
      "301     I've given farmers targeted China we're looki...\n",
      "236     CRITICS WHO SUGGEST THAT TRUMP‚ÄôS DECISION T...\n",
      "254     Wuhan cities China many odds virus outbreak h...\n",
      "245     hear minute not words words undone jet nones ...\n",
      "347    we're gonna see happens we're going see happen...\n",
      "342     responds one prominent officials Czech Republ...\n",
      "433     see clips starting question Fox News correspo...\n",
      "351     OF THE CORONAVIRUS PANDEMIC. PANDEMIC. AS THE...\n",
      "258     number two mentioned idea virus set loose wor...\n",
      "244     starting leaked documents china we're gonna g...\n",
      "228     corona virus pandemic began China also man-ma...\n",
      "264     life yeah medical science front want turn mom...\n",
      "407     Wuhan disease first emerged saying World Heal...\n",
      "242     Wuhan there's also another report NIH Obama a...\n",
      "243     well right well seems like we're heading anot...\n",
      "257     wait true according lot media America's presi...\n",
      "290     Wuhan working Chinese funded Chinese also get...\n",
      "355     implemented Xuan City last week European tick...\n",
      "226     whole world waiting hear that's Donald Trump'...\n",
      "229    we're gonna see happens we're going see happen...\n",
      "260     Wuhan dr. Anthony Falchi addressing issue ABC...\n",
      "273     people employed we're gonna see whether happe...\n",
      "457     whole world waiting hear that's Donald Trump'...\n",
      "Name: transcript, dtype: object\n",
      "489    CORONAVIRUS TASK FORCE BRIEFING AT THE WHITE H...\n",
      "132    hi brothers sisters want bring update want say...\n",
      "142    hello good afternoon I'm Jason isatis outside ...\n",
      "308    Joe Rogan experience myth number two well say ...\n",
      "477    hi name Nick I'm PhD student molecular medicin...\n",
      "141    hey everyone Derrick Bros conscious resistance...\n",
      "390    okay May 2020 wanted give Paul Cottrell forced...\n",
      "378    engineer coverage pandemic deep origin Cogan 1...\n",
      "137    hi Mazz Mike today I've got latest message God...\n",
      "123    extensive research scientists US elsewhere det...\n",
      "120    guys got Netflix movie made 2018 movie okay go...\n",
      "134    I'm pretty positive bio weapon lay point Robbi...\n",
      "148    yesterday President Trump said no one even hea...\n",
      "292    proof coronavirus biological weapon small vict...\n",
      "149    hello Jason isatis outside box show amazing gu...\n",
      "486    think found smoking gun uh recent scientific s...\n",
      "366    welcome episode John Henry Weston show special...\n",
      "416    Wuhan seafood market January 1st shut many ear...\n",
      "415    hello I'm Jana nod2 South China Morning Post t...\n",
      "272    one thing administration must focus right fixi...\n",
      "151    Tsar's Cove go vid 19 coronavirus race war wea...\n",
      "293    comes China not system comes China comes China...\n",
      "493    virus biological weapon mean peddle conspiracy...\n",
      "313    important updates kovat 19 edge wonder many CE...\n",
      "119    get reports Chinese Communist Party telling 94...\n",
      "145    video I've purposely avoided coronavirus tobin...\n",
      "526    coronavirus man-made bio weapon myth busted no...\n",
      "135    good morning we're today announce three separa...\n",
      "133    watch CNN watch fox watched CBS case there's l...\n",
      "283    good morning we're today announce three separa...\n",
      "136    adam always thinks i'm going start singing mus...\n",
      "396    okay March 11 2020 wanted wrap-up motivations ...\n",
      "127    think offensive biological warfare weapon orig...\n",
      "450    coronavirus yellow chicken gudiwada mu dou go ...\n",
      "458    actually really hard run trials drugs might ac...\n",
      "276    novel coronavirus biological weapon mean peddl...\n",
      "327    world battles covert 19 several theories exist...\n",
      "420    there's lot still know virus way stop kill Chi...\n",
      "358    friends Sam Chaney reporting weaponize news co...\n",
      "131    happened Alex Wuhan scientists took North Caro...\n",
      "117    many times told even FBI admit white supremaci...\n",
      "154    hi everyone Sonia interesting update today u.s...\n",
      "363    countries big big trouble they're not reportin...\n",
      "130    planned long time horrifying says DOJ could ev...\n",
      "333    hey possible coronavirus we're dealing actuall...\n",
      "143    good morning we're today announce three separa...\n",
      "354    major role hi everyone i'm zhang slow beyond m...\n",
      "153    breaking news State Department cables warning ...\n",
      "125    let us recap incredible day played Dow biggest...\n",
      "116    hi brothers sisters pastor Tim may hear grandb...\n",
      "150    yeah wheels cracking everybody welcome back an...\n",
      "124    coronavirus deep state bioweapon attack China ...\n",
      "266    I'm gonna lay right engineered man-made 11 tha...\n",
      "Name: transcript, dtype: object\n",
      "489     IS GROWING BELIEF THAT THE COVID-19 GROWING B...\n",
      "132     video faith plus nothing equals salvation ete...\n",
      "142     policy bio warfare former boys early board me...\n",
      "308     give little bit background aboard career back...\n",
      "477     likes receptor bind often however interaction...\n",
      "141     Lee Michael Jones writer editor culture wars ...\n",
      "390     really channel since day one bio weapons coro...\n",
      "378     said early there's no whay virus came us stri...\n",
      "137     said know think main bi either stood wanted u...\n",
      "123     corona virus discovered China December Chines...\n",
      "120    guys got Netflix movie made 2018 movie okay go...\n",
      "134     weapon lay point Robbie probably meme coming ...\n",
      "148    yesterday President Trump said no one even hea...\n",
      "292     weapon small victories Hong Kong outrage Chin...\n",
      "149     men's testicles I'm walking wrong bloody hand...\n",
      "486     giving gain-of-function activity final piece ...\n",
      "366     oh gosh absolutely false course kind thing th...\n",
      "416     Chinese no evidence course they're gonna tell...\n",
      "415     calling foreign virus Secretary State Mike Po...\n",
      "272     citizens we're behind United Kingdom behind F...\n",
      "151     weapon seen videos like couples discuss Chine...\n",
      "293    comes China not system comes China comes China...\n",
      "493     weapon mean peddle conspiracy theories think ...\n",
      "313     many residents believe CCP written Wuhan loss...\n",
      "119     US Army Navy Air Force participating military...\n",
      "145     oh know different strain know different strai...\n",
      "526     weapon myth busted novel coronavirus pandemic...\n",
      "135     Chinese national working scientific researche...\n",
      "133     corona virus actually published paper researc...\n",
      "283     Chinese national working scientific researche...\n",
      "136     phone television name going chinese dominatio...\n",
      "396     whether natural bio weapon ability find wheth...\n",
      "127     weapon originally developed University North ...\n",
      "450     received every matter know abide tickler conn...\n",
      "458     world falling apart god damn get oh gonna gre...\n",
      "276     weapon mean peddle conspiracy theories think ...\n",
      "327     spread spread leak Wuhan lab experiment gone ...\n",
      "420     preparedness forth principal deputy secretary...\n",
      "358     TV town president CMAC loaded internet docume...\n",
      "131     weapon took technology behind well developed ...\n",
      "117     corona virus pandemic going right know white ...\n",
      "154     Wuhan China Secretary State Mike Pompeo said ...\n",
      "363     that's know report facts we're country that's...\n",
      "130     corona virus weapon they're right know they'r...\n",
      "333     weapon created laboratory I've got get answer...\n",
      "143     Chinese national working scientific researche...\n",
      "354     hospitals filled not know I'm not ground seen...\n",
      "153     could virus come mean there's real people dyi...\n",
      "125     not warfare per se leveraging capability act ...\n",
      "116     causing they've using long time may not know ...\n",
      "150     family friends co-workers anywhere everywhere...\n",
      "124     bioweapon attack China planned 2005 interview...\n",
      "266     what's really going roadmap understand new da...\n",
      "Name: transcript, dtype: object\n",
      "362    world going different place children never nev...\n",
      "481    impact huge epidemic like flu epidemic would p...\n",
      "112    pouchy dr. Burks Bill Gates World Health Organ...\n",
      "106    dangers coronavirus two million Americans 17-0...\n",
      "442    failure prepare could allow next epidemic dram...\n",
      "421    want move next topic get along yeah doc told t...\n",
      "298    urgent invention world right vaccine prevents ...\n",
      "85     well one things that's seems polarizing issue ...\n",
      "487    bill gates right said could following advice j...\n",
      "83     trade-off mentioned obviously economic pain th...\n",
      "78     gyro experience weirdest motherfuckers I've ev...\n",
      "295    Our next guest one richest generous men world....\n",
      "110    peace peace family right live streams powerful...\n",
      "81     goes without saying coronavirus huge global is...\n",
      "357    speaky shall judged law Liberty hello Adam law...\n",
      "446    hey yo what's going here's tube know I'm host ...\n",
      "480    President United States would stop impending g...\n",
      "472    welcome everyone I'm Spiro thanks tuning curre...\n",
      "102    there's deposit must oh yeah Monday business r...\n",
      "320    ‚ô™ ‚ô™ ‚ô™ ‚ô™ >> Chris: WANT TO PLAY >> Chri...\n",
      "286    >>> NOW TO OUR SPECIAL REPORT >>> NOW TO OUR S...\n",
      "389    bill gates welcome daily social distancing sho...\n",
      "101    yo disgust studios YouTube channel please forg...\n",
      "89     today greatest risk global catastrophe look li...\n",
      "Name: transcript, dtype: object\n",
      "362     depopulate relatives living longer scare peop...\n",
      "481     ready epidemics different sizes there's lot r...\n",
      "112     Bill Gates World Health Organization kovat ni...\n",
      "106     busy lock houses rooms like what's going behi...\n",
      "442     antibiotics work bacteria not viruses exposed...\n",
      "421     us mark that's going people's bodies people f...\n",
      "298     else there's no risk causing disease one fina...\n",
      "85      bill gates vaccinations read article believe ...\n",
      "487     bill gates right said could following advice ...\n",
      "83      normalcy returns we've largely vaccinated ent...\n",
      "78      guy terms like could rattle fat parents homes...\n",
      "295     Bill Gates. Hi, Bill. Hi. First all, thank th...\n",
      "110     bill gates admits dr farci foreign government...\n",
      "81      recently video myth surrounding corona virus ...\n",
      "357     Bill Gates use microchip implants fight coron...\n",
      "446     ID 2020 website obvious guns ho creating glob...\n",
      "480     biological weapons theater ongoing 2018 creat...\n",
      "472     right corona virus supposed believe they're g...\n",
      "102     sites debunking claims linked coronavirus con...\n",
      "320     HIGHLY INFECTIOUS VIRUS BE HIGHLY INFECTIOUS ...\n",
      "286     PUBLIC STAGE FIVE YEARS AGO, WARNING ABOUT PA...\n",
      "389     bill gates welcome daily social distancing sh...\n",
      "101     record comes helping vaccinations video goods...\n",
      "89      get vaccines kids wall go back 1990 9% childr...\n",
      "Name: transcript, dtype: object\n"
     ]
    }
   ],
   "source": [
    "for key in keywords:\n",
    "    print(df.loc[df.keyword == key,'transcript'])    \n",
    "    df.loc[df.keyword == key,'transcript'] = df.loc[df.keyword == key,'transcript'].apply(extract_keywords,words = keywords[key])\n",
    "    print(df.loc[df.keyword == key,'transcript'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Dean Koons predict coronavirus 1981 novel eyes darkness say yes pages novel resurfaced since gone viral read book believe eerie prediction accuracy virus created biowarfarelaunched conspiracy theories coronavirus covet 19 somehow escaped Wuan Institute virology though no evidence prove currently believed Wuan virus started animals somehow made jump humansDean Koontz wrote following passage book called stuff Wu 400 developed DNA labs outside city Wuan 400th viable strain man-made microorganisms created research center onecreature carry like syphilis mu 400 can't survive outside living human body longer minute means permanently contaminate objects entire places way anthrax virulent microorganisms thoughDean Koontz writing fictional virus many people think similarities startling near-perfect prediction others pointed name whoa 400 could 20 times 20 Dean Koontz one goinganthrax virulent microorganisms though Dean Koontz writing fictional virus many people think similarities startling near-perfect prediction others pointed name whoa 400 could 20 times 20Dean Koontz one going viral possibly predicting corona virus many people including Kim Kardashian revisiting passage Sylvia Browns book end days predictions prophecies end worldbook end days predictions prophecies end world book Sylvia Browne references ammonia like illness impacts globe 2020 following coronavirus conspiracies seemingly psychic predictions Sylvia BrowneDean Koontz think Dean Koontz's description woman 400 think sounds like corona viruspredictions prophecies end world book Sylvia Browne references ammonia like illness impacts globe 2020 following coronavirus conspiracies seemingly psychic predictions Sylvia Browne Dean Koontz thinkDean Koontz's description woman 400 think sounds like corona virus darkness say yes pages novel resurfaced since gone viral read book believe eerie prediction accuracy virus created biowarfare named Wu 400 city launched book speaks predict coronavirus 1981 novel eyes darkness say yes pages novel resurfaced since gone viral read book believe eerie prediction accuracy virus created biowarfare named Wu fictional virus many people think similarities startling near-perfect prediction others pointed name whoa 400 could 20 times 20 Dean Koontz one going viral possibly predictingcorona virus many people including Kim Kardashian revisiting passage Sylvia Browns book end days predictions prophecies end world book Sylvia Browne references ammonia like illnessammonia like illness impacts globe 2020 following coronavirus conspiracies seemingly psychic predictions Sylvia Browne Dean Koontz think Dean Koontz's description woman 400 think sounds likecorona virus coronavirus 1981 novel eyes darkness say yes pages novel resurfaced since gone viral read book believe eerie prediction accuracy virus created biowarfare named Wu 400launched book speaks whoa 400 weapon easily transmitted human human dies inanimate objects making easier way kill without fear environmental contamination even launched conspiracy theoriescoronavirus covet 19 somehow escaped Wuan Institute virology though no evidence prove currently believed Wuan virus started animals somehow made jump humans Dean Koontz wroteincluding Kim Kardashian revisiting passage Sylvia Browns book end days predictions prophecies end world book Sylvia Browne references ammonia like illness impacts globe 2020 followingcoronavirus conspiracies seemingly psychic predictions Sylvia Browne Dean Koontz think Dean Koontz's description woman 400 think sounds like corona virus\""
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[156,'transcript']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def eval_metric(history, metric_name):\n",
    "    metric = history.history[metric_name]\n",
    "\n",
    "    e = range(1, 50 + 1)\n",
    "\n",
    "    plt.plot(e, metric, 'bo', label='Train ' + metric_name)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "eval_metric(history, 'loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_seq(seqs, nb_features = NB_WORDS):\n",
    "    ohs = np.zeros((len(seqs), nb_features))\n",
    "    for i, s in enumerate(seqs):\n",
    "        ohs[i, s] = 1.\n",
    "    return ohs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>with coronavirus completely changing our way o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>you all know I'm a big fan of conspiracy theor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>how do you handle an epidemic in the age of fa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>what's up guys Stephen here and welcome back t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>hi guys this is Daniel Alexander cannon here o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>so the government work for us we don't work fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>but even if 5g has nothing to do with this cor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>um some people believe that 5g like like for c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>this is a podcast from the South China Morning...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>we're all catching the wires because of the fi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>313 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            transcript\n",
       "0    with coronavirus completely changing our way o...\n",
       "1    you all know I'm a big fan of conspiracy theor...\n",
       "2    how do you handle an epidemic in the age of fa...\n",
       "4    what's up guys Stephen here and welcome back t...\n",
       "5    hi guys this is Daniel Alexander cannon here o...\n",
       "..                                                 ...\n",
       "402  so the government work for us we don't work fo...\n",
       "403  but even if 5g has nothing to do with this cor...\n",
       "404  um some people believe that 5g like like for c...\n",
       "405  this is a podcast from the South China Morning...\n",
       "406  we're all catching the wires because of the fi...\n",
       "\n",
       "[313 rows x 1 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0\n",
       "1      0\n",
       "2      0\n",
       "4      0\n",
       "5      1\n",
       "      ..\n",
       "402    1\n",
       "403    0\n",
       "404    0\n",
       "405    0\n",
       "406    0\n",
       "Name: attitude, Length: 313, dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "repeated: 0\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Shape of trainiig set: (281, 10000)\n",
      "WARNING:tensorflow:From <ipython-input-45-45fa7875586f>:74: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "> Fold 1 - Precison: 0.7061901913875598 - Recall: 0.8421052631578947%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Shape of trainiig set: (281, 10000)\n",
      "> Fold 2 - Precison: 0.8333333333333334 - Recall: 1.0%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Shape of trainiig set: (281, 10000)\n",
      "> Fold 3 - Precison: 0.723343685300207 - Recall: 0.8095238095238095%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "Shape of trainiig set: (282, 10000)\n",
      "> Fold 4 - Precison: 0.75 - Recall: 1.0%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "Shape of trainiig set: (282, 10000)\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f84a0bbf8c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "> Fold 5 - Precison: 0.8114519177998658 - Recall: 0.8260869565217391%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 6 ...\n",
      "Shape of trainiig set: (282, 10000)\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f84a0da8b90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "> Fold 6 - Precison: 0.8571428571428571 - Recall: 1.0%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 7 ...\n",
      "Shape of trainiig set: (282, 10000)\n",
      "WARNING:tensorflow:7 out of the last 7 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f84a1a69f80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "> Fold 7 - Precison: 0.8170406732117812 - Recall: 0.95%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 8 ...\n",
      "Shape of trainiig set: (282, 10000)\n",
      "WARNING:tensorflow:8 out of the last 8 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f84a0e0d9e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "> Fold 8 - Precison: 0.8695652173913043 - Recall: 1.0%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 9 ...\n",
      "Shape of trainiig set: (282, 10000)\n",
      "WARNING:tensorflow:9 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f84b6e72680> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "> Fold 9 - Precison: 0.7746823069403714 - Recall: 0.9545454545454546%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 10 ...\n",
      "Shape of trainiig set: (282, 10000)\n",
      "WARNING:tensorflow:10 out of the last 10 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f84a0c6f8c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "> Fold 10 - Precison: 0.898164522226965 - Recall: 0.9130434782608695%\n",
      "------------------------------------------------------------------------\n",
      "> Precison: 0.8040914704734246 (+- 0.06076478926473974)\n",
      "> Recall: 0.9295304962009767\n",
      "------------------------------------------------------------------------\n",
      "repeated: 1\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Shape of trainiig set: (281, 10000)\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f84a0789b90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "> Fold 1 - Precison: 0.7618006993006993 - Recall: 0.9090909090909091%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Shape of trainiig set: (281, 10000)\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f84a0897f80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "> Fold 2 - Precison: 0.8613636363636364 - Recall: 0.8181818181818182%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Shape of trainiig set: (281, 10000)\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f84904d4dd0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "> Fold 3 - Precison: 0.7108928571428571 - Recall: 0.8571428571428571%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "Shape of trainiig set: (282, 10000)\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f84a0ecc710> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "> Fold 4 - Precison: 0.8317741935483871 - Recall: 0.875%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "Shape of trainiig set: (282, 10000)\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f84b2be27a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "> Fold 5 - Precison: 0.8359447004608294 - Recall: 0.9%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 6 ...\n",
      "Shape of trainiig set: (282, 10000)\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f84b26ad560> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "> Fold 6 - Precison: 0.8 - Recall: 1.0%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 7 ...\n",
      "Shape of trainiig set: (282, 10000)\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f84a09a0830> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "> Fold 7 - Precison: 0.8518518518518519 - Recall: 1.0%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 8 ...\n",
      "Shape of trainiig set: (282, 10000)\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f84a04e17a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "> Fold 8 - Precison: 0.8170406732117812 - Recall: 0.95%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 9 ...\n",
      "Shape of trainiig set: (282, 10000)\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f84b2a6c290> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "> Fold 9 - Precison: 0.739516129032258 - Recall: 0.9%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 10 ...\n",
      "Shape of trainiig set: (282, 10000)\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f84b2a6def0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Fold 10 - Precison: 0.8116461482520388 - Recall: 0.9565217391304348%\n",
      "------------------------------------------------------------------------\n",
      "> Precison: 0.802183088916434 (+- 0.047108022612792885)\n",
      "> Recall: 0.916593732354602\n",
      "------------------------------------------------------------------------\n",
      "repeated: 2\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Shape of trainiig set: (281, 10000)\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f84b2c6e8c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "> Fold 1 - Precison: 0.6962719298245614 - Recall: 0.8947368421052632%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Shape of trainiig set: (281, 10000)\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f84b2dfeb90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "> Fold 2 - Precison: 0.57375 - Recall: 0.8%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Shape of trainiig set: (281, 10000)\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f84b200cb00> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "> Fold 3 - Precison: 0.9 - Recall: 1.0%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "Shape of trainiig set: (282, 10000)\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f84a4effdd0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "> Fold 4 - Precison: 0.8783622828784119 - Recall: 0.92%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "Shape of trainiig set: (282, 10000)\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f84b200cb00> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "> Fold 5 - Precison: 0.8527126099706746 - Recall: 0.95%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 6 ...\n",
      "Shape of trainiig set: (282, 10000)\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f84b1941cb0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "> Fold 6 - Precison: 0.64 - Recall: 1.0%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 7 ...\n",
      "Shape of trainiig set: (282, 10000)\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f84b2c368c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "> Fold 7 - Precison: 0.8885630498533724 - Recall: 0.8%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 8 ...\n",
      "Shape of trainiig set: (282, 10000)\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f84b3914b90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "> Fold 8 - Precison: 0.8424709932423817 - Recall: 0.8260869565217391%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 9 ...\n",
      "Shape of trainiig set: (282, 10000)\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f84a1509f80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Fold 9 - Precison: 0.8571428571428571 - Recall: 1.0%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 10 ...\n",
      "Shape of trainiig set: (282, 10000)\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f84b1fea320> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "> Fold 10 - Precison: 0.8604154144126094 - Recall: 0.9523809523809523%\n",
      "------------------------------------------------------------------------\n",
      "> Precison: 0.7989689137324868 (+- 0.11091426318047132)\n",
      "> Recall: 0.9143204751007955\n",
      "------------------------------------------------------------------------\n",
      "repeated: 3\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Shape of trainiig set: (281, 10000)\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f84b6e724d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "> Fold 1 - Precison: 0.8330681818181819 - Recall: 0.9545454545454546%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Shape of trainiig set: (281, 10000)\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f84a1f36830> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "> Fold 2 - Precison: 0.8257327586206896 - Recall: 0.96%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Shape of trainiig set: (281, 10000)\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f84b3928d40> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "> Fold 3 - Precison: 0.9475 - Recall: 0.76%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "Shape of trainiig set: (282, 10000)\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f84a0ba6440> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "> Fold 4 - Precison: 0.8912357057518348 - Recall: 0.9259259259259259%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "Shape of trainiig set: (282, 10000)\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f84a11417a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "> Fold 5 - Precison: 0.8 - Recall: 1.0%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 6 ...\n",
      "Shape of trainiig set: (282, 10000)\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f84914f4950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "> Fold 6 - Precison: 0.7843413978494622 - Recall: 0.95%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 7 ...\n",
      "Shape of trainiig set: (282, 10000)\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f84b2362ef0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "> Fold 7 - Precison: 0.7669802867383513 - Recall: 0.9583333333333334%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 8 ...\n",
      "Shape of trainiig set: (282, 10000)\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f84a1f153b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Fold 8 - Precison: 0.9037995664924136 - Recall: 0.9545454545454546%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 9 ...\n",
      "Shape of trainiig set: (282, 10000)\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f84a0f18710> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "> Fold 9 - Precison: 0.615254015933133 - Recall: 0.9230769230769231%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 10 ...\n",
      "Shape of trainiig set: (282, 10000)\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f84a0a44c20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "> Fold 10 - Precison: 0.7035853390592229 - Recall: 0.8235294117647058%\n",
      "------------------------------------------------------------------------\n",
      "> Precison: 0.807149725226329 (+- 0.09319946704422173)\n",
      "> Recall: 0.9209956503191797\n",
      "------------------------------------------------------------------------\n",
      "repeated: 4\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Shape of trainiig set: (281, 10000)\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f84a41c9dd0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "> Fold 1 - Precison: 0.8897664835164836 - Recall: 0.9615384615384616%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Shape of trainiig set: (281, 10000)\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8490548290> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "> Fold 2 - Precison: 0.7367173721340388 - Recall: 0.9523809523809523%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Shape of trainiig set: (281, 10000)\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f84b200c320> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "> Fold 3 - Precison: 0.6145833333333334 - Recall: 0.8333333333333334%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "Shape of trainiig set: (282, 10000)\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f84b26e7f80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "> Fold 4 - Precison: 0.7385560675883257 - Recall: 0.8%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "Shape of trainiig set: (282, 10000)\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f84a10673b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "> Fold 5 - Precison: 0.8486160892074871 - Recall: 0.9583333333333334%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 6 ...\n",
      "Shape of trainiig set: (282, 10000)\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f84b2d43560> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "> Fold 6 - Precison: 0.9461827956989247 - Recall: 0.92%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 7 ...\n",
      "Shape of trainiig set: (282, 10000)\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f84a058ba70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Fold 7 - Precison: 0.7620560443141088 - Recall: 0.9444444444444444%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 8 ...\n",
      "Shape of trainiig set: (282, 10000)\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f84a1f2b5f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "> Fold 8 - Precison: 0.7843413978494622 - Recall: 0.95%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 9 ...\n",
      "Shape of trainiig set: (282, 10000)\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f84a0640cb0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "> Fold 9 - Precison: 0.8220918866080157 - Recall: 0.9090909090909091%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 10 ...\n",
      "Shape of trainiig set: (282, 10000)\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f84a0c3d3b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "> Fold 10 - Precison: 0.7807859703020994 - Recall: 0.9047619047619048%\n",
      "------------------------------------------------------------------------\n",
      "> Precison: 0.792369744055228 (+- 0.08712651805071552)\n",
      "> Recall: 0.9133883338883338\n",
      "------------------------------------------------------------------------\n",
      "repeated: 5\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Shape of trainiig set: (281, 10000)\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f84b2c2c3b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "> Fold 1 - Precison: 0.8518518518518519 - Recall: 1.0%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Shape of trainiig set: (281, 10000)\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f84a0f198c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "> Fold 2 - Precison: 0.8571428571428571 - Recall: 1.0%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Shape of trainiig set: (281, 10000)\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f84a11a0c20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "> Fold 3 - Precison: 0.8889462809917354 - Recall: 0.9090909090909091%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "Shape of trainiig set: (282, 10000)\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f84b2a6d5f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "> Fold 4 - Precison: 0.5749327956989247 - Recall: 0.875%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "Shape of trainiig set: (282, 10000)\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f84a3eb13b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "> Fold 5 - Precison: 0.8459014104175395 - Recall: 0.9047619047619048%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 6 ...\n",
      "Shape of trainiig set: (282, 10000)\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f84a118d710> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Fold 6 - Precison: 0.712516129032258 - Recall: 0.9%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 7 ...\n",
      "Shape of trainiig set: (282, 10000)\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f84a3e9ec20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "> Fold 7 - Precison: 0.8623991935483871 - Recall: 0.875%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 8 ...\n",
      "Shape of trainiig set: (282, 10000)\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f84a117c320> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "> Fold 8 - Precison: 0.7777777777777778 - Recall: 1.0%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 9 ...\n",
      "Shape of trainiig set: (282, 10000)\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f84a06ad3b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "> Fold 9 - Precison: 0.7377254366501679 - Recall: 0.9523809523809523%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 10 ...\n",
      "Shape of trainiig set: (282, 10000)\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f84b2a6d440> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "> Fold 10 - Precison: 0.9583333333333334 - Recall: 1.0%\n",
      "------------------------------------------------------------------------\n",
      "> Precison: 0.8067527066444834 (+- 0.10351813218246991)\n",
      "> Recall: 0.9416233766233766\n",
      "------------------------------------------------------------------------\n",
      "repeated: 6\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Shape of trainiig set: (281, 10000)\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f84a0c3d3b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "> Fold 1 - Precison: 0.8305999999999999 - Recall: 0.84%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Shape of trainiig set: (281, 10000)\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f84b2d43710> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "> Fold 2 - Precison: 0.8928571428571429 - Recall: 1.0%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Shape of trainiig set: (281, 10000)\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f84a4085c20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "> Fold 3 - Precison: 0.8294565217391303 - Recall: 0.9130434782608695%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "Shape of trainiig set: (282, 10000)\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f84a1fd2dd0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "> Fold 4 - Precison: 0.8250424448217317 - Recall: 0.8947368421052632%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "Shape of trainiig set: (282, 10000)\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f84b3982830> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Fold 5 - Precison: 0.8114519177998658 - Recall: 0.8260869565217391%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 6 ...\n",
      "Shape of trainiig set: (282, 10000)\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f84a3d2fdd0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "> Fold 6 - Precison: 0.7521351766513058 - Recall: 0.9047619047619048%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 7 ...\n",
      "Shape of trainiig set: (282, 10000)\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f84b2d59f80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "> Fold 7 - Precison: 0.7736768288181886 - Recall: 0.9473684210526315%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 8 ...\n",
      "Shape of trainiig set: (282, 10000)\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f84911cd3b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "> Fold 8 - Precison: 0.8148148148148148 - Recall: 1.0%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 9 ...\n",
      "Shape of trainiig set: (282, 10000)\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8491a1c560> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "> Fold 9 - Precison: 0.7264888337468982 - Recall: 0.95%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 10 ...\n",
      "Shape of trainiig set: (282, 10000)\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f84907c3a70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "> Fold 10 - Precison: 0.633405017921147 - Recall: 0.8888888888888888%\n",
      "------------------------------------------------------------------------\n",
      "> Precison: 0.7889928699170224 (+- 0.06806938078231548)\n",
      "> Recall: 0.9164886491591296\n",
      "------------------------------------------------------------------------\n",
      "repeated: 7\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Shape of trainiig set: (281, 10000)\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f84913ccb00> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "> Fold 1 - Precison: 0.8074196597353497 - Recall: 0.8260869565217391%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Shape of trainiig set: (281, 10000)\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f84b2995dd0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "> Fold 2 - Precison: 0.8729891304347827 - Recall: 0.9565217391304348%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Shape of trainiig set: (281, 10000)\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f84b26adcb0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "> Fold 3 - Precison: 0.9356380837359098 - Recall: 0.8148148148148148%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "Shape of trainiig set: (282, 10000)\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f84b2c413b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Fold 4 - Precison: 0.783017765310893 - Recall: 0.8260869565217391%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "Shape of trainiig set: (282, 10000)\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f84a0abd710> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "> Fold 5 - Precison: 0.7037037037037037 - Recall: 1.0%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 6 ...\n",
      "Shape of trainiig set: (282, 10000)\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f84a0d8cc20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "> Fold 6 - Precison: 0.6571087216248507 - Recall: 0.8888888888888888%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 7 ...\n",
      "Shape of trainiig set: (282, 10000)\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f84b1941830> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "> Fold 7 - Precison: 0.6923076923076923 - Recall: 1.0%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 8 ...\n",
      "Shape of trainiig set: (282, 10000)\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f84b2be2050> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "> Fold 8 - Precison: 0.7143633276740238 - Recall: 0.9473684210526315%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 9 ...\n",
      "Shape of trainiig set: (282, 10000)\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f84a09b93b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "> Fold 9 - Precison: 0.9413277232351567 - Recall: 0.9166666666666666%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 10 ...\n",
      "Shape of trainiig set: (282, 10000)\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8490b92710> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "> Fold 10 - Precison: 0.875 - Recall: 1.0%\n",
      "------------------------------------------------------------------------\n",
      "> Precison: 0.7982875807762362 (+- 0.09912614136789719)\n",
      "> Recall: 0.9176434443596915\n",
      "------------------------------------------------------------------------\n",
      "repeated: 8\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Shape of trainiig set: (281, 10000)\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8490638c20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "> Fold 1 - Precison: 0.6428571428571429 - Recall: 1.0%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Shape of trainiig set: (281, 10000)\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f84a6e26dd0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "> Fold 2 - Precison: 0.82875 - Recall: 0.875%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Shape of trainiig set: (281, 10000)\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8480a905f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Fold 3 - Precison: 0.8601624668435013 - Recall: 0.9615384615384616%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "Shape of trainiig set: (282, 10000)\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f84b6e725f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "> Fold 4 - Precison: 0.7620560443141088 - Recall: 0.9444444444444444%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "Shape of trainiig set: (282, 10000)\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f84b26a2f80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "> Fold 5 - Precison: 0.7143633276740238 - Recall: 0.9473684210526315%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 6 ...\n",
      "Shape of trainiig set: (282, 10000)\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f84a3ec53b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "> Fold 6 - Precison: 0.8800144747725392 - Recall: 0.9583333333333334%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 7 ...\n",
      "Shape of trainiig set: (282, 10000)\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f84a11a18c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "> Fold 7 - Precison: 0.9165871349494179 - Recall: 0.9230769230769231%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 8 ...\n",
      "Shape of trainiig set: (282, 10000)\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f84a0d57dd0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "> Fold 8 - Precison: 0.739516129032258 - Recall: 0.9%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 9 ...\n",
      "Shape of trainiig set: (282, 10000)\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8480f8e320> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "> Fold 9 - Precison: 0.6881285098602586 - Recall: 0.9473684210526315%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 10 ...\n",
      "Shape of trainiig set: (282, 10000)\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8490ce1440> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "> Fold 10 - Precison: 0.8119281373138316 - Recall: 0.9047619047619048%\n",
      "------------------------------------------------------------------------\n",
      "> Precison: 0.7844363367617082 (+- 0.0847359164556985)\n",
      "> Recall: 0.9361891909260331\n",
      "------------------------------------------------------------------------\n",
      "repeated: 9\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Shape of trainiig set: (281, 10000)\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f84b202dcb0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "> Fold 1 - Precison: 0.9942129629629629 - Recall: 0.9629629629629629%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Shape of trainiig set: (281, 10000)\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f84a07083b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Fold 2 - Precison: 0.7833333333333332 - Recall: 0.95%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Shape of trainiig set: (281, 10000)\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f84a3f13710> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "> Fold 3 - Precison: 0.7787698412698413 - Recall: 0.9047619047619048%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "Shape of trainiig set: (282, 10000)\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8490640c20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "> Fold 4 - Precison: 0.7692307692307693 - Recall: 1.0%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "Shape of trainiig set: (282, 10000)\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f84a1184290> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "> Fold 5 - Precison: 0.7838108595471849 - Recall: 0.9565217391304348%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 6 ...\n",
      "Shape of trainiig set: (282, 10000)\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f84b2995050> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "> Fold 6 - Precison: 0.9138148667601683 - Recall: 0.8260869565217391%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 7 ...\n",
      "Shape of trainiig set: (282, 10000)\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f84b24913b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "> Fold 7 - Precison: 0.7042266106692878 - Recall: 0.7368421052631579%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 8 ...\n",
      "Shape of trainiig set: (282, 10000)\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f84a09b1710> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "> Fold 8 - Precison: 0.5899922693091573 - Recall: 0.9411764705882353%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 9 ...\n",
      "Shape of trainiig set: (282, 10000)\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f84b2451c20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "> Fold 9 - Precison: 0.8416226130111123 - Recall: 0.9565217391304348%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 10 ...\n",
      "Shape of trainiig set: (282, 10000)\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8492152320> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "> Fold 10 - Precison: 0.8340762463343109 - Recall: 0.9545454545454546%\n",
      "------------------------------------------------------------------------\n",
      "> Precison: 0.7993090372428128 (+- 0.10429500341018533)\n",
      "> Recall: 0.9189419332904325\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "precision = []\n",
    "recall = []\n",
    "\n",
    "for i in range(10):\n",
    "    print('repeated:',i)\n",
    "    num_folds = 10\n",
    "    # Define per-fold score containers <-- these are new\n",
    "    acc_per_fold = []\n",
    "    loss_per_fold = []\n",
    "\n",
    "    # Define the K-fold Cross Validator\n",
    "    kfold = KFold(n_splits=num_folds, shuffle=True)\n",
    "\n",
    "    inputs = X.transcript\n",
    "    targets = y\n",
    "\n",
    "#     print('# inputs data samples:', inputs.shape[0])\n",
    "#     print('# targets data samples:', targets.shape[0])\n",
    "\n",
    "    # K-fold Cross Validation model evaluation\n",
    "    fold_no = 1\n",
    "    for train, test in kfold.split(inputs, targets):\n",
    "\n",
    "        tk = Tokenizer(num_words=NB_WORDS,\n",
    "                   filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "                   lower=True,\n",
    "                   split=\" \")\n",
    "        tk.fit_on_texts(inputs.iloc[train])\n",
    "\n",
    "        X_train_seq = tk.texts_to_sequences(inputs.iloc[train])\n",
    "        X_test_seq = tk.texts_to_sequences(inputs.iloc[test])\n",
    "\n",
    "\n",
    "        X_train_oh = one_hot_seq(X_train_seq)\n",
    "        X_test_oh = one_hot_seq(X_test_seq)\n",
    "\n",
    "\n",
    "        le = LabelEncoder()\n",
    "        y_train_le = le.fit_transform(targets.iloc[train])\n",
    "        y_test_le = le.transform(targets.iloc[test])\n",
    "        y_train_oh = to_categorical(y_train_le)\n",
    "        y_test_oh = to_categorical(y_test_le)\n",
    "\n",
    "        # Define the model architecture\n",
    "        base_model = models.Sequential()\n",
    "        base_model.add(layers.Dense(64, activation='relu', input_shape=(NB_WORDS,)))\n",
    "        base_model.add(layers.Dense(64, activation='relu'))\n",
    "        base_model.add(layers.Dense(2, activation='softmax'))\n",
    "\n",
    "        # Compile the model\n",
    "        base_model.compile(optimizer='rmsprop'\n",
    "                      , loss='categorical_crossentropy'\n",
    "                      , metrics=['accuracy',tf.keras.metrics.Precision(),tf.keras.metrics.Recall()])\n",
    "\n",
    "\n",
    "        # Generate a print\n",
    "        print('------------------------------------------------------------------------')\n",
    "        print(f'Training for fold {fold_no} ...')\n",
    "        print('Shape of trainiig set:',X_train_oh.shape)\n",
    "\n",
    "        # Fit data to model\n",
    "        history = base_model.fit(X_train_oh\n",
    "                           ,y_train_oh\n",
    "                           , epochs=50\n",
    "                           , batch_size=BATCH_SIZE\n",
    "                           , verbose=0)\n",
    "#         print(history.history)\n",
    "\n",
    "\n",
    "        # Generate generalization metrics\n",
    "        from sklearn.metrics import classification_report\n",
    "        y_pred = base_model.predict_classes(X_test_oh)\n",
    "\n",
    "        average_recall = recall_score(y_test_le, y_pred)\n",
    "        average_precision = average_precision_score(y_test_le, y_pred)\n",
    "        print(f'> Fold {fold_no} - Precison: {average_precision} - Recall: {average_recall}%')\n",
    "\n",
    "        acc_per_fold.append(average_precision)\n",
    "        loss_per_fold.append(average_recall)\n",
    "\n",
    "        # Increase fold number\n",
    "        fold_no = fold_no + 1\n",
    "\n",
    "    # == Provide average scores ==\n",
    "    print('------------------------------------------------------------------------')\n",
    "\n",
    "    print(f'> Precison: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
    "    precision.append(np.mean(acc_per_fold))\n",
    "    print(f'> Recall: {np.mean(loss_per_fold)}')\n",
    "    print('------------------------------------------------------------------------')\n",
    "    recall.append(np.mean(loss_per_fold))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.save(\"mlp with transcript.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8040914704734246,\n",
       " 0.802183088916434,\n",
       " 0.7989689137324868,\n",
       " 0.807149725226329,\n",
       " 0.792369744055228,\n",
       " 0.8067527066444834,\n",
       " 0.7889928699170224,\n",
       " 0.7982875807762362,\n",
       " 0.7844363367617082,\n",
       " 0.7993090372428128]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.007168137077769212"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(precision)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7982541473746163"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7928489964966213, 0.8036592982526113)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy.stats as st\n",
    "\n",
    "st.t.interval(0.95, len(precision)-1, loc=np.mean(precision), scale=st.sem(precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9295304962009767,\n",
       " 0.916593732354602,\n",
       " 0.9143204751007955,\n",
       " 0.9209956503191797,\n",
       " 0.9133883338883338,\n",
       " 0.9416233766233766,\n",
       " 0.9164886491591296,\n",
       " 0.9176434443596915,\n",
       " 0.9361891909260331,\n",
       " 0.9189419332904325]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.009284022072101519"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(recall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9225715282222551"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9155708892124385, 0.9295721672320717)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st.t.interval(0.95, len(recall)-1, loc=np.mean(recall), scale=st.sem(recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting words to numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the text as input for a model, we first need to convert the tweet's words into tokens, which simply means converting the words to integers that refer to an index in a dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will only keep the most frequent words in the train set.\n",
    "\n",
    "We clean up the text by applying filters and putting the words to lowercase. Words are separated by spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>with coronavirus completely changing our way o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>you all know I'm a big fan of conspiracy theor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>how do you handle an epidemic in the age of fa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>what's up guys Stephen here and welcome back t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>hi guys this is Daniel Alexander cannon here o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>so the government work for us we don't work fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>but even if 5g has nothing to do with this cor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>um some people believe that 5g like like for c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>this is a podcast from the South China Morning...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>we're all catching the wires because of the fi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>313 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            transcript\n",
       "0    with coronavirus completely changing our way o...\n",
       "1    you all know I'm a big fan of conspiracy theor...\n",
       "2    how do you handle an epidemic in the age of fa...\n",
       "4    what's up guys Stephen here and welcome back t...\n",
       "5    hi guys this is Daniel Alexander cannon here o...\n",
       "..                                                 ...\n",
       "402  so the government work for us we don't work fo...\n",
       "403  but even if 5g has nothing to do with this cor...\n",
       "404  um some people believe that 5g like like for c...\n",
       "405  this is a podcast from the South China Morning...\n",
       "406  we're all catching the wires because of the fi...\n",
       "\n",
       "[313 rows x 1 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted tokenizer on 313 documents\n",
      "10000 words in dictionary\n",
      "Top 5 most common words are: [('the', 35424), ('and', 23437), ('to', 18362), ('of', 16015), ('that', 15541), ('a', 15287), ('you', 12990), ('in', 11896), ('is', 10700), ('i', 9946), ('it', 9288), ('this', 9187), ('so', 6718), ('we', 5951), ('was', 5438), ('they', 5410), ('on', 5344), ('know', 5306), ('have', 4832), ('for', 4828)]\n"
     ]
    }
   ],
   "source": [
    "tk = Tokenizer(num_words=NB_WORDS,\n",
    "               filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "               lower=True,\n",
    "               split=\" \")\n",
    "tk.fit_on_texts(X.transcript)\n",
    "\n",
    "print('Fitted tokenizer on {} documents'.format(tk.document_count))\n",
    "print('{} words in dictionary'.format(tk.num_words))\n",
    "print('Top 5 most common words are:', collections.Counter(tk.word_counts).most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Train data samples: 323\n",
      "# Test data samples: 36\n",
      "Fitted tokenizer on 323 documents\n",
      "20000 words in dictionary\n",
      "Top 5 most common words are: [('know', 5545), ('not', 4258), ('people', 3994), ('like', 3747), (\"that's\", 2892), ('virus', 2685), ('one', 2371), (\"i'm\", 2309), ('right', 2305), ('get', 2107), ('no', 1910), ('think', 1846), ('going', 1823), (\"they're\", 1674), ('said', 1651), ('really', 1566), ('us', 1561), ('would', 1543), ('see', 1539), (\"there's\", 1513)]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df.transcript, df.attitude, test_size=0.1, random_state=37)\n",
    "print('# Train data samples:', X_train.shape[0])\n",
    "print('# Test data samples:', X_test.shape[0])\n",
    "assert X_train.shape[0] == y_train.shape[0]\n",
    "assert X_test.shape[0] == y_test.shape[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having created the dictionary we can convert the text to a list of integer indexes. This is done with the text_to_sequences method of the Tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_seq = tk.texts_to_sequences(X_train)\n",
    "X_test_seq = tk.texts_to_sequences(X_test)\n",
    "\n",
    "print('\"{}\" is converted into {}'.format(X_train[292], X_train_seq[292]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These integers should now be converted into a one-hot encoded features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"[30, 13, 97, 246, 413, 128, 128, 1465, 4737, 1393, 2, 2711, 413, 133, 128, 1538, 7520, 93, 144, 1775, 7521, 83, 1595, 1596, 121, 4738, 4739, 773, 831, 1693, 3, 29, 375, 213, 2606, 123, 81, 6024, 119, 351, 1020, 1334, 1089, 1089, 1241, 422, 1539, 1562, 476, 3560, 105, 7522, 58, 292, 21, 26, 4738, 30, 255, 21, 1811, 6, 5, 313, 269, 54, 28, 670, 87, 3247, 1394, 1597, 50, 3, 23, 21, 26, 2154, 13938, 525, 255, 1152, 2107, 311, 4430, 258, 234, 3388, 80, 10, 861, 444, 702, 5099, 80, 755, 376, 13939, 258, 234, 4740, 80, 386, 114, 9, 10472, 2607, 362, 1895, 6668, 153, 14, 10473, 19, 222, 13, 128, 15, 8, 488, 78, 15, 8, 488, 8697, 908, 77, 10474, 135, 2608, 25, 6669, 841, 2, 331, 440, 114, 8, 920, 10475, 43, 129, 2, 497, 219, 13, 1854, 32, 369, 11, 6025, 402, 103, 1, 54, 2, 18, 501, 2609, 501, 25, 378, 29, 96, 102, 144, 467, 210, 1855, 6670, 832, 564, 166, 1069, 243, 11, 50, 70, 3026, 2436, 39, 68, 3921, 2812, 1, 8698, 1598, 38, 21, 2522, 7523, 20, 28, 987, 3561, 1563, 10476, 394, 3562, 956, 30, 6671, 13940, 1563, 7524, 3131, 493, 7525, 44, 454, 8, 13941, 92, 56, 3248, 814, 1021, 3027, 80, 42, 119, 957, 212, 3028, 796, 1021, 26, 2813, 2437, 1, 1283, 10, 896, 271, 4159, 712, 357, 1856, 531, 678, 6672, 197, 6026, 193, 565, 357, 8699, 449, 65, 1776, 1658, 449, 2108, 3249, 455, 2108, 886, 449, 2910, 13942, 4160, 988, 886, 449, 4741, 17, 1037, 207, 2053, 13943, 779, 2610, 5519, 217, 690, 107, 98, 391, 946, 610, 2353, 334, 5100, 566, 774, 1205, 145, 1052, 20, 17, 6027, 13944, 83, 1242, 5101, 567, 567, 203, 62, 7526, 2910, 1395, 203, 2814, 151, 97, 193, 5102, 508, 391, 321, 58, 96, 774, 1205, 6673, 6673, 193, 659, 20, 2109, 302, 1100, 1599, 3731, 5103, 736, 58, 4431, 477, 377, 477, 532, 362, 17, 182, 2287, 13945, 2911, 62, 4432, 13946, 65, 2155, 6674, 62, 5520, 13947, 45, 164, 1114, 3563, 167, 212, 204, 790, 1396, 748, 475, 3389, 119, 13948, 1436, 8700, 690, 325, 1600, 164, 6675, 766, 5521, 21, 15, 1659, 119, 62, 5522, 3029, 509, 157, 1659, 573, 207, 303, 4161, 7, 41, 720, 4, 182, 8701, 652, 971, 660, 4162, 7527, 642, 485, 2611, 8702, 39, 8702, 8702, 2611, 144, 107, 510, 93, 28, 1694, 749, 154, 2156, 321, 180, 24, 642, 1284, 2523, 815, 7528, 7528, 2, 815, 7527, 23, 1308, 250, 13949, 250, 382, 909, 265, 351, 66, 1114, 679, 177, 76, 103, 66, 265, 1, 244, 66, 111, 1694, 749, 154, 2156, 102, 387, 2612, 774, 1090, 2612, 7529, 5102, 21, 176, 22, 137, 1, 2438, 773, 141, 106, 91, 3922, 4161, 11, 195, 4163, 21, 1999, 179, 102, 737, 3132, 226, 184, 2910, 304, 1053, 2053, 207, 11, 3, 4, 7530, 7531, 173, 153, 74, 548, 468, 11, 5, 244, 39, 3732, 1397, 39, 3732, 2912, 5523, 862, 456, 173, 153, 7, 13950, 1732, 17, 2354, 45, 3733, 862, 1070, 36, 863, 17, 862, 10477, 862, 3734, 11, 132, 8703, 887, 10478, 1167, 7532, 5524, 266, 417, 21, 157, 7533, 3923, 58, 305, 933, 74, 17, 2354, 93, 738, 2288, 1242, 10479, 17, 17, 2354, 221, 22, 88, 160, 288, 387, 270, 13951, 897, 8704, 66, 387, 270, 1335, 135, 42, 249, 47, 135, 2608, 244, 42, 494, 337, 288, 387, 212, 3028, 212, 133, 5525, 212, 972, 2221, 3564, 304, 7534, 414, 76, 2222, 102, 265, 246, 119, 525, 5, 554, 8705, 21, 223, 49, 1168, 39, 1218, 3250, 276, 241, 9, 23, 91, 750, 13952, 126, 1336, 1285, 193, 1264, 774, 1205, 193, 1100, 652, 103, 15, 1264, 20, 13953, 3133, 266, 3133, 63, 118, 2157, 2613, 2054, 46, 1167, 193, 4742, 1206, 4433, 19, 12, 1125, 5, 1364, 1733, 1, 221, 1, 4161, 2910, 8, 138, 253, 39, 247, 133, 6676, 423, 6676, 2000, 8706, 2355, 2910, 1, 23, 304, 23, 451, 3390, 4434, 10480, 11, 45, 11, 3, 11, 5, 8707, 2158, 978, 2289, 624, 625, 19, 1, 22, 304, 8, 76, 391, 1219, 2815, 8, 76, 1601, 5104, 99, 3, 74, 548, 554, 1091, 1564, 1627, 77, 624, 149, 76, 193, 3134, 12, 97, 11, 193, 3134, 449, 13954, 2110, 603, 6677, 1419, 2, 193, 1100, 1, 567, 203, 8, 76, 1395, 1419, 3735, 68, 298, 2614, 330, 46, 13955, 603, 331, 68, 3251, 1309, 28, 1264, 22, 1054, 273, 115, 1, 533, 12, 4164, 63, 3565, 58, 3564, 449, 128, 273, 1365, 235, 215, 2816, 62, 13956, 243, 69, 1896, 69, 13957, 212, 697, 2223, 12, 2, 243, 4435, 525, 418, 573, 120, 197, 97, 246, 1191, 67, 418, 137, 2, 2159, 67, 1, 20, 67, 6678, 311, 137, 19, 31, 97, 2817, 3, 76, 516, 739, 568, 8, 25, 22, 113, 517, 477, 1055, 116, 7535, 7, 999, 5105, 225, 26, 5526, 816, 3, 68, 842, 3924, 74, 249, 2290, 10481, 10482, 48, 418, 133, 13958, 47, 50, 138, 8708, 1056, 373, 373, 278, 26, 183, 22, 97, 2817, 113, 12, 8, 2, 517, 8, 285, 817, 32, 1335, 494, 42, 5, 86, 5526, 32, 2, 1335, 494, 42, 32, 10483, 2290, 209, 5106, 20, 457, 680, 2160, 7536, 6028, 2524, 7, 41, 2525, 64, 32, 841, 187, 1337, 13959, 6028, 279, 740, 921, 391, 178, 740, 3925, 13960, 1939, 1540, 6029, 187, 8709, 178, 34, 32, 36, 19, 64, 13, 19, 1169, 10484, 2615, 3135, 77, 833, 3926, 261, 833, 3926, 13961, 2615, 3135, 3, 25, 208, 13962, 3736, 77, 833, 151, 2161, 834, 274, 309, 127, 3734, 910, 472, 32, 346, 1, 1265, 958, 14, 1466, 338, 80, 220, 4436, 80, 313, 269, 7537, 130, 1139, 791, 3, 703, 4165, 123, 81, 482, 307, 8710, 293, 3, 13963, 598, 7538, 6679, 95, 7539, 2439, 224, 6680, 703, 4, 10485, 10486, 123, 81, 2001, 6030, 28, 780, 365, 5, 130, 8711, 2437, 156, 298, 168, 1, 13964, 216, 50, 959, 254, 118, 1812, 39, 156, 781, 332, 130, 49, 8712, 13, 1939, 817, 2, 2111, 25, 258, 8713, 25, 258, 234, 13, 2818, 851, 2440, 780, 365, 130, 10487, 5, 130, 13965, 123, 731, 130, 10488, 137, 32, 989, 841, 298, 198, 3388, 80, 298, 2608, 25, 6669, 841, 83, 81, 8714, 81, 2, 331, 440, 114, 5, 626, 8715, 132, 8716, 3030, 2, 3030, 2, 258, 48, 7540, 445, 550, 671, 2526, 5103, 43, 5, 44, 55, 185, 3252, 10489, 1854, 2055, 1565, 632, 5, 102, 225, 66, 40, 26, 26, 149, 7541, 151, 97, 2526, 632, 632, 80, 89, 179, 588, 179, 405, 1813, 187, 2224, 185, 4743, 179, 8, 2, 121, 3391, 525, 316, 165, 39, 2225, 529, 1628, 3253, 45, 66, 28, 363, 2156, 124, 270, 574, 225, 7, 115, 5103, 3254, 2712, 322, 3927, 5103, 66, 36, 3254, 323, 1541, 1542, 756, 7, 115, 7542, 165, 538, 386, 154, 2156, 3031, 7, 32, 741, 321, 7, 6681, 248, 2616, 7, 6681, 737, 19, 147, 228, 135, 538, 154, 457, 61, 147, 180, 3255, 387, 270, 387, 574, 1777, 48, 2002, 184, 135, 4744, 7, 2526, 11, 11, 147, 13966, 2226, 9, 7, 3737, 2, 25, 478, 2226, 147, 6682, 249, 13967, 228, 6682, 116, 6031, 2226, 5, 626, 5, 8, 121, 268, 3, 184, 32, 3247, 311, 32, 25, 258, 65, 2527, 32, 477, 2003, 346, 477, 401, 726, 76, 97, 196, 86, 290, 23, 2817, 76, 5, 972, 1310, 10490, 7543, 288, 13, 387, 1168, 28, 56, 2817, 99, 99, 126, 1814, 4745, 105, 392, 30, 65, 424, 1, 78, 488, 78, 1, 104, 7, 7543, 8717, 818, 6032, 3, 157, 1021, 2712, 93, 32, 11, 32, 1366, 60, 6032, 39, 22, 13968, 179, 11, 132, 5, 3392, 7, 14, 1266, 7, 39, 176, 13969, 11, 31, 2526, 1140, 6683, 1811, 3, 957, 19, 1, 1815, 34, 216, 4163, 226, 4163, 13970, 89, 1815, 216, 529, 2227, 529, 9, 77, 481, 103, 13971, 11, 843, 611, 32, 2438, 579, 973, 6033, 19, 48, 26, 2606, 103, 4161, 2002, 41, 425, 171, 120, 643, 171, 428, 417, 32, 70, 4, 362, 4, 38, 2, 70, 5, 1, 271, 2713, 185, 190, 3031, 66, 756, 66, 3031, 66, 406, 293, 661, 9, 605, 429, 405, 179, 118, 210, 210, 293, 2000, 74, 245, 98, 142, 26, 50, 66, 86, 1, 3738, 337, 3, 22, 137, 13972, 2617, 4, 1286, 679, 245, 98, 1243, 1, 13973, 314, 16, 60, 108, 61, 329, 3566, 4, 2441, 1022, 60, 3, 319, 110, 43, 1, 22, 48, 26, 50, 138, 5527, 185, 13, 3032, 7544, 197, 874, 1126, 1897, 243, 319, 2162, 1, 874, 130, 8, 124, 1940, 529, 2112, 309, 127, 644, 1287, 3928, 32, 3925, 123, 81, 851, 178, 3928, 56, 65, 2527, 391, 7545, 13974, 516, 181, 2819, 555, 376, 10491, 13975, 309, 127, 947, 7546, 32, 672, 19, 989, 2714, 1000, 1311, 713, 13976, 17, 10492, 30, 65, 424, 2056, 95, 166, 1115, 1115, 44, 68, 3136, 3393, 3136, 2820, 32, 462, 7546, 2356, 64, 130, 8718, 2714, 1000, 3929, 6034, 64, 266, 1437, 269, 108, 1170, 679, 66, 387, 69, 6684, 91, 174, 2291, 99, 10493, 1695, 1857, 4, 1467, 13977, 9, 767, 3250, 2, 8, 767, 4161, 10, 10494, 479, 10494, 1023, 823, 457, 144, 329, 30, 22, 30, 8, 2, 76, 289, 289, 30, 1116, 13, 97, 246, 413, 128, 128, 1465, 4737, 1393, 2, 2711, 2, 823, 413, 133, 128, 1538, 7520, 93, 144, 1775, 7521, 83, 1595, 102, 516, 9, 102, 516, 32, 510, 2821, 9, 20, 1543, 1141, 213, 3739, 2, 286, 287, 606, 1629, 3, 920, 30, 3394, 909, 2820, 103, 814, 110, 605, 1207, 1092, 1171, 1491, 304, 187, 115, 15, 400, 704, 80, 132, 8703, 5528, 737, 283, 1898, 1, 92, 320, 315, 110, 21, 168, 3256, 21, 22, 5529, 44, 9, 21, 2228, 13978, 844, 6035, 321, 2113, 2, 2528, 259, 2, 498, 2, 10495, 362, 2, 157, 76, 284, 1153, 721, 44, 4166, 44, 258, 234, 44, 34, 555, 77, 2715, 284, 14, 3567, 284, 14, 3395, 284, 672, 151, 411, 6685, 2715, 14, 2, 1895, 362, 481, 234, 2913, 293, 422, 13979, 1895, 14, 396, 362, 23, 213, 886, 4167, 5107, 136, 960, 30, 10473, 989, 3033, 371, 886, 338, 80, 439, 2357, 160, 288, 160, 2114, 74, 104, 66, 6681, 737, 337, 329, 177, 74, 13980, 13981, 1858, 270, 574, 359, 140, 41, 797, 92, 74, 103, 446, 132, 6686, 757, 144, 180, 24, 25, 948, 140, 30, 2716, 288, 2442, 120, 270, 574, 77, 2442, 1734, 737, 721, 6036, 3, 1, 20, 525, 5, 3257, 10496, 3034, 46, 166, 3, 974, 6687, 467, 210, 1855, 6670, 832, 7, 41, 564, 166, 1069, 469, 13982, 1069, 6670, 832, 1, 456, 5530, 423, 5530, 423, 8, 314, 5, 215, 13983, 104, 8, 10497, 182, 2057, 1, 816, 317, 633, 4, 3568, 353, 84, 3258, 25, 74, 3569, 17, 14, 25, 317, 2529, 3137, 1, 861, 705, 58, 177, 4168, 13984, 93, 1, 13, 68, 246, 3138, 99, 403, 1660, 1142, 852, 1, 14, 121, 888, 146, 5, 157, 36, 29, 8, 1420, 1001, 2358, 3035, 250, 2443, 9, 864, 3396, 50, 325, 1735, 37, 341, 26, 74, 2163, 402, 22, 43, 24, 13, 1816, 3, 275, 26, 48, 26, 2004, 26, 4738, 1154, 175, 1420, 175, 1420, 1172, 1420, 6688, 13985, 63, 426, 175, 6037, 175, 1420, 128, 5108, 119, 128, 13986, 243, 119, 23, 35, 44, 284, 175, 6037, 18, 31, 3, 2717, 414, 339, 3570, 265, 129, 2, 3570, 265, 129, 13987, 26, 1630, 627, 1695, 26, 1695, 113, 775, 68, 26, 23, 565, 8, 2, 948, 1695, 1816, 26, 23, 175, 6037, 13, 681, 21, 22, 8719, 1602, 256, 449, 3139, 449, 256, 362, 5, 240, 1596, 1155, 157, 121, 13988, 21, 6038, 137, 706, 7547, 529, 2224, 57, 44, 398, 1002, 398, 1002, 449, 2822, 298, 3259, 298, 7548, 3259, 8720, 3259, 2822, 6039, 315, 34, 122, 4746, 5531, 4746, 5531, 149, 188, 598, 95, 178, 714, 95, 14, 1395, 235, 243, 14, 13989, 3930, 258, 234, 49, 9, 178, 774, 360, 7, 61, 922, 7, 147, 1492, 101, 175, 865, 178, 5, 3739, 103, 181, 294, 35, 634, 377, 58, 217, 178, 163, 302, 178, 659, 181, 235, 1267, 44, 740, 234, 502, 30, 65, 424, 2444, 2112, 740, 234, 10498, 30, 756, 30, 242, 1003, 7549, 401, 3260, 10498, 30, 2359, 30, 178, 714, 95, 756, 7550, 3031, 1023, 455, 79, 68, 13990, 44, 714, 95, 178, 1493, 3, 539, 1116, 22, 1493, 8, 99, 2618, 300, 411, 13991, 2914, 452, 6040, 6041, 5532, 4437, 388, 74, 249, 2915, 114, 74, 34, 249, 736, 705, 3, 539, 178, 714, 95, 5, 3739, 41, 1, 335, 7551, 335, 1089, 4437, 335, 2915, 335, 8721, 99, 10499, 335, 8722, 2530, 2530, 458, 377, 1153, 10500, 95, 151, 294, 8723, 740, 234, 714, 95, 178, 10500, 95, 1566, 3, 539, 193, 1696, 740, 234, 3259, 2915, 452, 6040, 1566, 1394, 1597, 50, 3, 23, 8723, 3, 127, 714, 95, 178, 51, 95, 5, 103, 598, 34, 24, 20, 449, 92, 127, 20, 8723, 740, 234, 1696, 46, 756, 1092, 258, 234, 69, 6689, 15, 7550, 1, 252, 3, 436, 13992, 252, 193, 807, 193, 13993, 63, 807, 6042, 212, 8724, 234, 7550, 740, 193, 212, 540, 459, 540, 1941, 540, 411, 756, 1092, 258, 234, 181, 2224, 475, 302, 193, 1143, 2224, 181, 2531, 682, 3261, 357, 316, 41, 357, 682, 3261, 357, 74, 181, 260, 475, 302, 193, 1143, 181, 5, 386, 634, 58, 292, 86, 113, 221, 41, 3, 115, 2532, 143, 9, 1, 1156, 4161, 181, 2, 1603, 7550, 740, 234, 2, 756, 1092, 258, 234, 2616, 1092, 258, 234, 1156, 113, 63, 35, 853, 122, 175, 1420, 175, 6037, 79, 68, 144, 203, 3, 22, 20, 256, 3262, 256, 5109, 797, 17, 5, 3, 773, 831, 6690, 248, 732, 680, 181, 183, 113, 3, 773, 831, 808, 3571, 2718, 612, 8725, 3571, 5533, 1004, 715, 10, 301, 1157, 1220, 1604, 383, 2823, 592, 13994, 256, 7549, 256, 3260, 773, 1859, 947, 289, 10501, 5, 7, 244, 736, 10502, 1939, 92, 237, 3263, 1221, 50, 175, 3570, 5, 244, 831, 10503, 166, 1221, 1899, 1, 4747, 58, 177, 58, 177, 166, 1221, 4, 3, 124, 773, 1004, 715, 797, 3, 124, 773, 7552, 2619, 1, 1697, 58, 177, 19, 989, 481, 5, 1421, 108, 337, 370, 58, 726, 10504, 185, 1, 2445, 3740, 8726, 4438, 933, 256, 256, 10503, 2, 449, 481, 449, 103, 156, 1308, 8, 138, 382, 635, 43, 1, 6691, 51, 6043, 241, 51, 1308, 241, 798, 1308, 457, 4, 321, 679, 130, 10505, 3397, 63, 256, 1308, 43, 449, 4, 254, 1001, 13995, 21, 2824, 377, 2113, 1038, 377, 377, 13996, 1038, 1, 1900, 1900, 3741, 1604, 49, 21, 68, 5, 1173, 5110, 275, 243, 174, 2620, 1220, 58, 177, 6692, 530, 6044, 449, 46, 3572, 315, 2360, 3572, 10506, 3, 8727, 5534, 3931, 13997, 6693, 49, 183, 116, 1127, 482, 1, 8, 2, 8728, 37, 44, 35, 614, 44, 13998, 44, 97, 3, 7, 2361, 3140, 533, 196, 314, 273, 10507, 2533, 13999, 2533, 115, 3264, 5535, 758, 4169, 10507, 2533, 1860, 87, 29, 2534, 1518, 3573, 23, 1698, 118, 1518, 534, 2440, 151, 1492, 539, 2535, 539, 1736, 3739, 220, 4, 842, 1492, 21, 436, 43, 1736, 366, 185, 185, 7553, 262, 575, 185, 4748, 670, 8729, 14000, 3574, 715, 5, 1860, 3, 337, 370, 58, 177, 1494, 1860, 3, 190, 539, 60, 237, 1398, 2440, 534, 2164, 4, 705, 300, 120, 37, 8730, 54, 653, 842, 1492, 54, 4749, 2535, 539, 705, 300, 539, 445, 4170, 220, 10508, 15, 14001, 727, 165, 4170, 727, 1737, 334, 1737, 18, 56, 200, 14002, 5, 165, 727, 4750, 214, 7, 1288, 41, 1288, 185, 41, 1312, 149, 396, 362, 362, 185, 437, 351, 1312, 197, 193, 10509, 5, 114, 185, 1901, 1699, 539, 1902, 6694, 6045, 339, 6695, 3265, 14003, 8731, 34, 1538, 564, 758, 4169, 248, 6045, 193, 332, 422, 14004, 2441, 6694, 119, 4439, 2533, 3266, 564, 21, 23, 293, 311, 3265, 670, 21, 6696, 8732, 1895, 23, 2058, 1024, 10510, 670, 18, 46, 613, 29, 1338, 613, 29, 149, 188, 34, 10511, 2292, 241, 158, 426, 449, 1289, 189, 158, 426, 426, 158, 43, 21, 4171, 370, 300, 158, 426, 449, 5536, 1856, 2228, 4751, 1393, 14005, 3036, 6046, 14006, 2293, 5, 168, 158, 426, 2294, 961, 529, 799, 68, 529, 168, 165, 5, 4, 576, 961, 5537, 1038, 137, 1038, 1244, 1071, 1038, 1244, 1071, 5537, 4, 165, 1244, 1071, 529, 3932, 529, 12, 165, 3932, 5537, 1038, 7, 41, 149, 1144, 449, 165, 57, 44, 1002, 824, 5538, 253, 726, 113, 482, 482, 18, 2719, 10512, 142, 70, 142, 43, 14, 2, 6047, 14, 2, 14, 70, 362, 4752, 87, 29, 18, 312, 5539, 18, 312, 87, 29, 647, 312, 5539, 18, 1903, 1192, 4172, 312, 4753, 258, 1903, 1, 5, 5540, 5, 6048, 809, 285, 5541, 9, 14, 2, 188, 5541, 14, 2, 188, 19, 959, 1174, 2621, 258, 543, 35, 188, 3926, 34, 2159, 13, 237, 268, 3, 10, 256, 15, 31, 34, 8, 25, 250, 10513, 34, 15, 7]\" is converted into [0. 1. 1. ... 0. 0. 0.]\n",
      "For this example we have 207.0 features with a value of 1.\n"
     ]
    }
   ],
   "source": [
    "def one_hot_seq(seqs, nb_features = NB_WORDS):\n",
    "    ohs = np.zeros((len(seqs), nb_features))\n",
    "    for i, s in enumerate(seqs):\n",
    "        ohs[i, s] = 1.\n",
    "    return ohs\n",
    "\n",
    "X_train_oh = one_hot_seq(X_train_seq)\n",
    "X_test_oh = one_hot_seq(X_test_seq)\n",
    "\n",
    "print('\"{}\" is converted into {}'.format(X_train_seq[0], X_train_oh[292]))\n",
    "print('For this example we have {} features with a value of 1.'.format(X_train_oh[292].sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting the target classes to numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"1\" is converted into 1\n",
      "\"1\" is converted into [0. 1.]\n"
     ]
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "y_train_le = le.fit_transform(y_train)\n",
    "y_test_le = le.transform(y_test)\n",
    "y_train_oh = to_categorical(y_train_le)\n",
    "y_test_oh = to_categorical(y_test_le)\n",
    "\n",
    "print('\"{}\" is converted into {}'.format(y_train[292], y_train_le[292]))\n",
    "print('\"{}\" is converted into {}'.format(y_train_le[292], y_train_oh[292]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting of a validation set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of validation set: (33, 20000)\n"
     ]
    }
   ],
   "source": [
    "X_train_rest, X_valid, y_train_rest, y_valid = train_test_split(X_train_oh, y_train_oh, test_size=0.1, random_state=37)\n",
    "\n",
    "assert X_valid.shape[0] == y_valid.shape[0]\n",
    "assert X_train_rest.shape[0] == y_train_rest.shape[0]\n",
    "\n",
    "print('Shape of validation set:',X_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Implementation\n",
    "## Baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with a model with 2 densely connected layers of 64 hidden elements. The input_shape for the first layer is equal to the number of words we allowed in the dictionary and for which we created one-hot-encoded features.\n",
    "\n",
    " The softmax activation function makes sure the three probabilities sum up to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 64)                1280064   \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 1,284,354\n",
      "Trainable params: 1,284,354\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "base_model = models.Sequential()\n",
    "base_model.add(layers.Dense(64, activation='relu', input_shape=(NB_WORDS,)))\n",
    "base_model.add(layers.Dense(64, activation='relu'))\n",
    "base_model.add(layers.Dense(2, activation='softmax'))\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because this project is a multi-class, single-label prediction, we use categorical_crossentropy as the loss function and softmax as the final activation function. We fit the model on the remaining train data and validate on the validation set. We run for a predetermined number of epochs and will see when the model starts to overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_model(model):\n",
    "    model.compile(optimizer='rmsprop'\n",
    "                  , loss='categorical_crossentropy'\n",
    "                  , metrics=['accuracy',tf.keras.metrics.Precision(),tf.keras.metrics.Recall()])\n",
    "    \n",
    "    history = model.fit(X_train_rest\n",
    "                       , y_train_rest\n",
    "                       , epochs=NB_START_EPOCHS\n",
    "                       , batch_size=BATCH_SIZE\n",
    "                       , validation_data=(X_valid, y_valid)\n",
    "                       , verbose=0)\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.6515330672264099,\n",
       "  1.0684441328048706,\n",
       "  0.4800872802734375,\n",
       "  0.30994516611099243,\n",
       "  0.6294538974761963,\n",
       "  0.5422257781028748,\n",
       "  0.15482644736766815,\n",
       "  0.12437337636947632,\n",
       "  0.1073225885629654,\n",
       "  0.09470220655202866,\n",
       "  0.08451832830905914,\n",
       "  0.07696767151355743,\n",
       "  0.07240038365125656,\n",
       "  0.06925303488969803,\n",
       "  0.061528246849775314,\n",
       "  0.057339850813150406,\n",
       "  0.05443510040640831,\n",
       "  0.055011797696352005,\n",
       "  0.04843403398990631,\n",
       "  0.04613620787858963,\n",
       "  0.044980209320783615,\n",
       "  0.049956709146499634,\n",
       "  0.060111355036497116,\n",
       "  0.04544132575392723,\n",
       "  0.04295087978243828,\n",
       "  0.041815679520368576,\n",
       "  0.04123577103018761,\n",
       "  0.03935204818844795,\n",
       "  0.037804607301950455,\n",
       "  0.036207105964422226],\n",
       " 'accuracy': [0.6551724076271057,\n",
       "  0.6517241597175598,\n",
       "  0.9137930870056152,\n",
       "  0.8758620619773865,\n",
       "  0.6517241597175598,\n",
       "  0.7344827651977539,\n",
       "  0.9793103337287903,\n",
       "  0.9793103337287903,\n",
       "  0.9793103337287903,\n",
       "  0.9793103337287903,\n",
       "  0.9793103337287903,\n",
       "  0.982758641242981,\n",
       "  0.9862068891525269,\n",
       "  0.9862068891525269,\n",
       "  0.9862068891525269,\n",
       "  0.9862068891525269,\n",
       "  0.9862068891525269,\n",
       "  0.9862068891525269,\n",
       "  0.9862068891525269,\n",
       "  0.9896551966667175,\n",
       "  0.9862068891525269,\n",
       "  0.9862068891525269,\n",
       "  0.9862068891525269,\n",
       "  0.9862068891525269,\n",
       "  0.9862068891525269,\n",
       "  0.9862068891525269,\n",
       "  0.9862068891525269,\n",
       "  0.9862068891525269,\n",
       "  0.9862068891525269,\n",
       "  0.9862068891525269],\n",
       " 'precision_1': [0.6574394702911377,\n",
       "  0.6517241597175598,\n",
       "  0.9137930870056152,\n",
       "  0.8758620619773865,\n",
       "  0.6517241597175598,\n",
       "  0.7344827651977539,\n",
       "  0.9793103337287903,\n",
       "  0.9793103337287903,\n",
       "  0.9793103337287903,\n",
       "  0.9793103337287903,\n",
       "  0.9793103337287903,\n",
       "  0.982758641242981,\n",
       "  0.9862068891525269,\n",
       "  0.9862068891525269,\n",
       "  0.9862068891525269,\n",
       "  0.9862068891525269,\n",
       "  0.9862068891525269,\n",
       "  0.9862068891525269,\n",
       "  0.9862068891525269,\n",
       "  0.9896551966667175,\n",
       "  0.9862068891525269,\n",
       "  0.9862068891525269,\n",
       "  0.9862068891525269,\n",
       "  0.9862068891525269,\n",
       "  0.9862068891525269,\n",
       "  0.9862068891525269,\n",
       "  0.9862068891525269,\n",
       "  0.9862068891525269,\n",
       "  0.9862068891525269,\n",
       "  0.9862068891525269],\n",
       " 'recall_1': [0.6551724076271057,\n",
       "  0.6517241597175598,\n",
       "  0.9137930870056152,\n",
       "  0.8758620619773865,\n",
       "  0.6517241597175598,\n",
       "  0.7344827651977539,\n",
       "  0.9793103337287903,\n",
       "  0.9793103337287903,\n",
       "  0.9793103337287903,\n",
       "  0.9793103337287903,\n",
       "  0.9793103337287903,\n",
       "  0.982758641242981,\n",
       "  0.9862068891525269,\n",
       "  0.9862068891525269,\n",
       "  0.9862068891525269,\n",
       "  0.9862068891525269,\n",
       "  0.9862068891525269,\n",
       "  0.9862068891525269,\n",
       "  0.9862068891525269,\n",
       "  0.9896551966667175,\n",
       "  0.9862068891525269,\n",
       "  0.9862068891525269,\n",
       "  0.9862068891525269,\n",
       "  0.9862068891525269,\n",
       "  0.9862068891525269,\n",
       "  0.9862068891525269,\n",
       "  0.9862068891525269,\n",
       "  0.9862068891525269,\n",
       "  0.9862068891525269,\n",
       "  0.9862068891525269],\n",
       " 'val_loss': [1.1663739681243896,\n",
       "  0.5814805030822754,\n",
       "  0.47035646438598633,\n",
       "  1.0552762746810913,\n",
       "  0.9172863960266113,\n",
       "  0.3378971517086029,\n",
       "  0.349372923374176,\n",
       "  0.35461854934692383,\n",
       "  0.37268373370170593,\n",
       "  0.37451988458633423,\n",
       "  0.38269391655921936,\n",
       "  0.3767574429512024,\n",
       "  0.42296019196510315,\n",
       "  0.3781675696372986,\n",
       "  0.41762295365333557,\n",
       "  0.39908987283706665,\n",
       "  0.40354013442993164,\n",
       "  0.4110170304775238,\n",
       "  0.4279894232749939,\n",
       "  0.4072283208370209,\n",
       "  0.48490750789642334,\n",
       "  0.3527151048183441,\n",
       "  0.5289850234985352,\n",
       "  0.4266473054885864,\n",
       "  0.525650680065155,\n",
       "  0.4245339035987854,\n",
       "  0.5362420678138733,\n",
       "  0.4491977095603943,\n",
       "  0.5259796977043152,\n",
       "  0.46963825821876526],\n",
       " 'val_accuracy': [0.5454545617103577,\n",
       "  0.7575757503509521,\n",
       "  0.7272727489471436,\n",
       "  0.5454545617103577,\n",
       "  0.6060606241226196,\n",
       "  0.8484848737716675,\n",
       "  0.8484848737716675,\n",
       "  0.8484848737716675,\n",
       "  0.8181818127632141,\n",
       "  0.8484848737716675,\n",
       "  0.8181818127632141,\n",
       "  0.8484848737716675,\n",
       "  0.7878788113594055,\n",
       "  0.8484848737716675,\n",
       "  0.7878788113594055,\n",
       "  0.8181818127632141,\n",
       "  0.8181818127632141,\n",
       "  0.8181818127632141,\n",
       "  0.7878788113594055,\n",
       "  0.8484848737716675,\n",
       "  0.7878788113594055,\n",
       "  0.8484848737716675,\n",
       "  0.7878788113594055,\n",
       "  0.8181818127632141,\n",
       "  0.7878788113594055,\n",
       "  0.8484848737716675,\n",
       "  0.7878788113594055,\n",
       "  0.8181818127632141,\n",
       "  0.7878788113594055,\n",
       "  0.8181818127632141],\n",
       " 'val_precision_1': [0.5454545617103577,\n",
       "  0.7575757503509521,\n",
       "  0.7272727489471436,\n",
       "  0.5454545617103577,\n",
       "  0.6060606241226196,\n",
       "  0.8484848737716675,\n",
       "  0.8484848737716675,\n",
       "  0.8484848737716675,\n",
       "  0.8181818127632141,\n",
       "  0.8484848737716675,\n",
       "  0.8181818127632141,\n",
       "  0.8484848737716675,\n",
       "  0.7878788113594055,\n",
       "  0.8484848737716675,\n",
       "  0.7878788113594055,\n",
       "  0.8181818127632141,\n",
       "  0.8181818127632141,\n",
       "  0.8181818127632141,\n",
       "  0.7878788113594055,\n",
       "  0.8484848737716675,\n",
       "  0.7878788113594055,\n",
       "  0.8484848737716675,\n",
       "  0.7878788113594055,\n",
       "  0.8181818127632141,\n",
       "  0.7878788113594055,\n",
       "  0.8484848737716675,\n",
       "  0.7878788113594055,\n",
       "  0.8181818127632141,\n",
       "  0.7878788113594055,\n",
       "  0.8181818127632141],\n",
       " 'val_recall_1': [0.5454545617103577,\n",
       "  0.7575757503509521,\n",
       "  0.7272727489471436,\n",
       "  0.5454545617103577,\n",
       "  0.6060606241226196,\n",
       "  0.8484848737716675,\n",
       "  0.8484848737716675,\n",
       "  0.8484848737716675,\n",
       "  0.8181818127632141,\n",
       "  0.8484848737716675,\n",
       "  0.8181818127632141,\n",
       "  0.8484848737716675,\n",
       "  0.7878788113594055,\n",
       "  0.8484848737716675,\n",
       "  0.7878788113594055,\n",
       "  0.8181818127632141,\n",
       "  0.8181818127632141,\n",
       "  0.8181818127632141,\n",
       "  0.7878788113594055,\n",
       "  0.8484848737716675,\n",
       "  0.7878788113594055,\n",
       "  0.8484848737716675,\n",
       "  0.7878788113594055,\n",
       "  0.8181818127632141,\n",
       "  0.7878788113594055,\n",
       "  0.8484848737716675,\n",
       "  0.7878788113594055,\n",
       "  0.8181818127632141,\n",
       "  0.7878788113594055,\n",
       "  0.8181818127632141]}"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_history = first_model(base_model)\n",
    "base_history.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the model performance, we will look at the training and validation loss and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_metric(history, metric_name):\n",
    "    metric = history.history[metric_name]\n",
    "    val_metric = history.history['val_' + metric_name]\n",
    "\n",
    "    e = range(1, NB_START_EPOCHS + 1)\n",
    "\n",
    "    plt.plot(e, metric, 'bo', label='Train ' + metric_name)\n",
    "    plt.plot(e, val_metric, 'b', label='Validation ' + metric_name)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'val_loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-207-ac1e3ea0ddc2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0meval_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0meval_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0meval_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'recall'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0meval_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'precision'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-206-e2844daf7ad7>\u001b[0m in \u001b[0;36meval_metric\u001b[0;34m(history, metric_name)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0meval_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mmetric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmetric_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mval_metric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmetric_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNB_START_EPOCHS\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'val_loss'"
     ]
    }
   ],
   "source": [
    "eval_metric(history, 'loss')\n",
    "eval_metric(base_history, 'accuracy')\n",
    "eval_metric(base_history, 'recall')\n",
    "eval_metric(base_history, 'precision')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Validation\n",
    "\n",
    "lets check the model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, epoch_stop):\n",
    "    model.fit(X_train_oh\n",
    "              , y_train_oh\n",
    "              , epochs=epoch_stop\n",
    "              , batch_size=BATCH_SIZE\n",
    "              , verbose=0)\n",
    "    results = model.evaluate(X_test_oh, y_test_oh)\n",
    "    \n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 1ms/step - loss: 0.9636 - accuracy: 0.9167 - precision_1: 0.9167 - recall_1: 0.9167\n",
      "[0.9635969996452332, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816]\n",
      "Test accuracy of baseline model: 91.67%\n"
     ]
    }
   ],
   "source": [
    "test_results = test_model(base_model,30)\n",
    "print(test_results)\n",
    "print('Test accuracy of baseline model: {0:.2f}%'.format(test_results[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 1 0 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "y_score = base_model.predict_classes(X_test_oh)\n",
    "print(y_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.70      0.82        10\n",
      "           1       0.90      1.00      0.95        26\n",
      "\n",
      "    accuracy                           0.92        36\n",
      "   macro avg       0.95      0.85      0.88        36\n",
      "weighted avg       0.93      0.92      0.91        36\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = base_model.predict_classes(X_test_oh)\n",
    "print(y_test.ndim)\n",
    "print(classification_report(y_test_le, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.896551724137931\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "\n",
    "average_precision = average_precision_score(y_test_le, y_pred)\n",
    "print(average_precision)\n",
    "average_recall = recall_score(y_test_le, y_pred)\n",
    "print(average_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,\n",
       "       1, 1, 1])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_arr = np.column_stack((y_test_le, y_pred)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [1, 1],\n",
       "       [1, 1],\n",
       "       [1, 1],\n",
       "       [0, 1],\n",
       "       [0, 0],\n",
       "       [1, 1],\n",
       "       [0, 1],\n",
       "       [1, 1],\n",
       "       [1, 1],\n",
       "       [0, 1],\n",
       "       [1, 1],\n",
       "       [1, 1],\n",
       "       [0, 1],\n",
       "       [1, 1],\n",
       "       [1, 0],\n",
       "       [1, 1],\n",
       "       [1, 1],\n",
       "       [1, 1],\n",
       "       [1, 1],\n",
       "       [0, 1],\n",
       "       [1, 1],\n",
       "       [1, 1],\n",
       "       [1, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [1, 1],\n",
       "       [1, 1],\n",
       "       [1, 1],\n",
       "       [1, 1],\n",
       "       [0, 0],\n",
       "       [1, 1],\n",
       "       [0, 0],\n",
       "       [1, 1],\n",
       "       [1, 1],\n",
       "       [1, 1],\n",
       "       [1, 1],\n",
       "       [1, 1],\n",
       "       [0, 1],\n",
       "       [1, 1],\n",
       "       [0, 0],\n",
       "       [0, 0],\n",
       "       [1, 0],\n",
       "       [1, 1],\n",
       "       [1, 1],\n",
       "       [1, 1],\n",
       "       [1, 1]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
